---
title: Defense against Backdoor Attack on Pre-trained Language Models via Head Pruning
  and Attention Normalization
openreview: 1SiEfsCecd
abstract: Pre-trained language models (PLMs) are commonly used for various downstream
  natural language processing tasks via fine-tuning. However, recent studies have
  demonstrated that PLMs are vulnerable to backdoor attacks, which can mislabel poisoned
  samples to target outputs even after a vanilla fine-tuning process. The key challenge
  for defending against the backdoored PLMs is that end users who adopt the PLMs for
  their downstream tasks usually do not have any knowledge about the attacking strategies,
  such as triggers. To tackle this challenge, in this work, we propose a backdoor
  mitigation approach, PURE, via head pruning and normalization of attention weights.
  The idea is to prune the attention heads that are potentially affected by poisoned
  texts with only clean texts on hand and then further normalize the weights of remaining
  attention heads to mitigate the backdoor impacts. We conduct experiments to defend
  against various backdoor attacks on the classification task. The experimental results
  show the effectiveness of PURE in lowering the attack success rate without sacrificing
  the performance on clean texts.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: zhao24r
month: 0
tex_title: Defense against Backdoor Attack on Pre-trained Language Models via Head
  Pruning and Attention Normalization
firstpage: 61108
lastpage: 61120
page: 61108-61120
order: 61108
cycles: false
bibtex_author: Zhao, Xingyi and Xu, Depeng and Yuan, Shuhan
author:
- given: Xingyi
  family: Zhao
- given: Depeng
  family: Xu
- given: Shuhan
  family: Yuan
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/zhao24r/zhao24r.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
