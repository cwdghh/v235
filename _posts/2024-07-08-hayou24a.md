---
title: 'LoRA+: Efficient Low Rank Adaptation of Large Models'
openreview: NEv8YqBROO
abstract: In this paper, we show that Low Rank Adaptation (LoRA) as originally introduced
  in (Hu et al., 2021) leads to suboptimal finetuning of models with large width.
  This is due to the fact that adapter matrices A and B in LoRA are updated with the
  same learning rate in ADAM. Using scaling arguments for large width networks, we
  demonstrate that the same learning rate does not allow efficient feature learning.
  We then show that this suboptimality of LoRA can be corrected simply by setting
  different learning rates for the LoRA adapter matrices A and B with a well-chosen
  fixed ratio. We call this proposed algorithm LoRA+. In our extensive experiments,
  LoRA+ improves finetuning speed (up to ∼ 2X SpeedUp) and performance (1% − 2% improvements),
  at the same computational cost as LoRA. The code is available at https://github.com/nikhil-ghosh-berkeley/loraplus
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: hayou24a
month: 0
tex_title: "{L}o{RA}+: Efficient Low Rank Adaptation of Large Models"
firstpage: 17783
lastpage: 17806
page: 17783-17806
order: 17783
cycles: false
bibtex_author: Hayou, Soufiane and Ghosh, Nikhil and Yu, Bin
author:
- given: Soufiane
  family: Hayou
- given: Nikhil
  family: Ghosh
- given: Bin
  family: Yu
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://raw.githubusercontent.com/mlresearch/v235/main/assets/hayou24a/hayou24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
