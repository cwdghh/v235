---
title: Can Mamba Learn How To Learn? A Comparative Study on In-Context Learning Tasks
openreview: GbFluKMmtE
abstract: State-space models (SSMs), such as Mamba (Gu & Dao, 2023), have been proposed
  as alternatives to Transformer networks in language modeling, incorporating gating,
  convolutions, and input-dependent token selection to mitigate the quadratic cost
  of multi-head attention. Although SSMs exhibit competitive performance, their in-context
  learning (ICL) capabilities, a remarkable emergent property of modern language models
  that enables task execution without parameter optimization, remain less explored
  compared to Transformers. In this study, we evaluate the ICL performance of SSMs,
  focusing on Mamba, against Transformer models across various tasks. Our results
  show that SSMs perform comparably to Transformers in standard regression ICL tasks,
  while outperforming them in tasks like sparse parity learning. However, SSMs fall
  short in tasks involving non-standard retrieval functionality. To address these
  limitations, we introduce a hybrid model, MambaFormer, that combines Mamba with
  attention blocks, surpassing individual models in tasks where they struggle independently.
  Our findings suggest that hybrid architectures offer promising avenues for enhancing
  ICL in language models.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: park24j
month: 0
tex_title: Can Mamba Learn How To Learn? {A} Comparative Study on In-Context Learning
  Tasks
firstpage: 39793
lastpage: 39812
page: 39793-39812
order: 39793
cycles: false
bibtex_author: Park, Jongho and Park, Jaeseung and Xiong, Zheyang and Lee, Nayoung
  and Cho, Jaewoong and Oymak, Samet and Lee, Kangwook and Papailiopoulos, Dimitris
author:
- given: Jongho
  family: Park
- given: Jaeseung
  family: Park
- given: Zheyang
  family: Xiong
- given: Nayoung
  family: Lee
- given: Jaewoong
  family: Cho
- given: Samet
  family: Oymak
- given: Kangwook
  family: Lee
- given: Dimitris
  family: Papailiopoulos
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/assets/park24j/park24j.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
