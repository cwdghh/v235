---
title: Rethinking the Flat Minima Searching in Federated Learning
openreview: 6TM62kpI5c
abstract: Albeit the success of federated learning (FL) in decentralized training,
  bolstering the generalization of models by overcoming heterogeneity across clients
  still remains a huge challenge. To aim at improved generalization of FL, a group
  of recent works pursues flatter minima of models by employing sharpness-aware minimization
  in the local training at the client side. However, we observe that the global model,
  i.e., the aggregated model, does not lie on flat minima of the global objective,
  even with the effort of flatness searching in local training, which we define as
  flatness discrepancy. By rethinking and theoretically analyzing flatness searching
  in FL through the lens of the discrepancy problem, we propose a method called Federated
  Learning for Global Flatness (FedGF) that explicitly pursues the flatter minima
  of the global models, leading to the relieved flatness discrepancy and remarkable
  performance gains in the heterogeneous FL benchmarks.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: lee24aa
month: 0
tex_title: Rethinking the Flat Minima Searching in Federated Learning
firstpage: 27037
lastpage: 27071
page: 27037-27071
order: 27037
cycles: false
bibtex_author: Lee, Taehwan and Yoon, Sung Whan
author:
- given: Taehwan
  family: Lee
- given: Sung Whan
  family: Yoon
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://raw.githubusercontent.com/mlresearch/v235/main/assets/lee24aa/lee24aa.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
