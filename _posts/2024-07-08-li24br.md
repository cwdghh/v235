---
title: Disentangled Graph Self-supervised Learning for Out-of-Distribution Generalization
openreview: OS0szhkPmF
abstract: Graph out-of-distribution (OOD) generalization, aiming to generalize graph
  neural networks (GNNs) under distribution shifts between training and testing environments,
  has attracted ever-increasing attention recently. However, existing literature heavily
  relies on sufficient task-dependent graph labels, which are often scarce or even
  unavailable, limiting their applications in real-world scenarios. In this paper,
  we study the self-supervised graph OOD generalization problem, i.e., learning GNNs
  capable of achieving relatively stable performances under distribution shifts without
  graph labels. However, the problem remains largely unexplored, with the critical
  challenge that the invariant and variant information are highly entangled in graphs.
  To solve this problem, we propose an OOD generalized disentangled graph contrastive
  learning model (OOD-GCL), which is capable of learning disentangled graph-level
  representations with self-supervision that can handle distribution shifts between
  training and testing graph data. Specifically, we first introduce a disentangled
  graph encoder to map each input graph into the factorized graph representation.
  Then we propose a tailored disentangled invariant self-supervised learning module
  to maximize predictive ability of the representations and make sure the representations
  other than from one specific channel are invariant to the environments partitioned
  by this latent factor for excluding the information corresponding to this latent
  factor for disentanglement. Finally, the disentangled graph representations are
  fed into a linear predictor and finetuned for the downstream tasks. We provide comprehensive
  theoretical analyses to show that our model can learn disentangled graph representations
  and achieve OOD generalization. Extensive experiments on real-world datasets demonstrate
  the superiority of our model against state-of-the-art baselines under distribution
  shifts for graph classification tasks.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: li24br
month: 0
tex_title: Disentangled Graph Self-supervised Learning for Out-of-Distribution Generalization
firstpage: 28890
lastpage: 28904
page: 28890-28904
order: 28890
cycles: false
bibtex_author: Li, Haoyang and Wang, Xin and Zhang, Zeyang and Chen, Haibo and Zhang,
  Ziwei and Zhu, Wenwu
author:
- given: Haoyang
  family: Li
- given: Xin
  family: Wang
- given: Zeyang
  family: Zhang
- given: Haibo
  family: Chen
- given: Ziwei
  family: Zhang
- given: Wenwu
  family: Zhu
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/assets/li24br/li24br.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
