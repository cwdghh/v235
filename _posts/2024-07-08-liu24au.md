---
title: 'KnowFormer: Revisiting Transformers for Knowledge Graph Reasoning'
openreview: EncFNR3hxM
abstract: Knowledge graph reasoning plays a vital role in various applications and
  has garnered considerable attention. Recently, path-based methods have achieved
  impressive performance. However, they may face limitations stemming from constraints
  in message-passing neural networks, such as missing paths and information over-squashing.
  In this paper, we revisit the application of transformers for knowledge graph reasoning
  to address the constraints faced by path-based methods and propose a novel method
  KnowFormer. KnowFormer utilizes a transformer architecture to perform reasoning
  on knowledge graphs from the message-passing perspective, rather than reasoning
  by textual information like previous pretrained language model based methods. Specifically,
  we define the attention computation based on the query prototype of knowledge graph
  reasoning, facilitating convenient construction and efficient optimization. To incorporate
  structural information into the self-attention mechanism, we introduce structure-aware
  modules to calculate query, key, and value respectively. Additionally, we present
  an efficient attention computation method for better scalability. Experimental results
  demonstrate the superior performance of KnowFormer compared to prominent baseline
  methods on both transductive and inductive benchmarks.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: liu24au
month: 0
tex_title: "{K}now{F}ormer: Revisiting Transformers for Knowledge Graph Reasoning"
firstpage: 31669
lastpage: 31690
page: 31669-31690
order: 31669
cycles: false
bibtex_author: Liu, Junnan and Mao, Qianren and Jiang, Weifeng and Li, Jianxin
author:
- given: Junnan
  family: Liu
- given: Qianren
  family: Mao
- given: Weifeng
  family: Jiang
- given: Jianxin
  family: Li
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/liu24au/liu24au.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
