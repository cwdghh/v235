---
title: On the Implicit Bias of Adam
openreview: y8YovS0lOg
abstract: 'In previous literature, backward error analysis was used to find ordinary
  differential equations (ODEs) approximating the gradient descent trajectory. It
  was found that finite step sizes implicitly regularize solutions because terms appearing
  in the ODEs penalize the two-norm of the loss gradients. We prove that the existence
  of similar implicit regularization in RMSProp and Adam depends on their hyperparameters
  and the training stage, but with a different "norm" involved: the corresponding
  ODE terms either penalize the (perturbed) one-norm of the loss gradients or, conversely,
  impede its reduction (the latter case being typical). We also conduct numerical
  experiments and discuss how the proven facts can influence generalization.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: cattaneo24a
month: 0
tex_title: On the Implicit Bias of {A}dam
firstpage: 5862
lastpage: 5906
page: 5862-5906
order: 5862
cycles: false
bibtex_author: Cattaneo, Matias D. and Klusowski, Jason Matthew and Shigida, Boris
author:
- given: Matias D.
  family: Cattaneo
- given: Jason Matthew
  family: Klusowski
- given: Boris
  family: Shigida
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/assets/cattaneo24a/cattaneo24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
