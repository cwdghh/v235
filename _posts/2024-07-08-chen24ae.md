---
title: 'EE-LLM: Large-Scale Training and Inference of Early-Exit Large Language Models
  with 3D Parallelism'
openreview: xFk0w9zoV3
abstract: We present EE-LLM, a framework for large-scale training and inference of
  early-exit large language models (LLMs). While recent works have shown preliminary
  evidence for the efficacy of early exiting in accelerating LLM inference, EE-LLM
  makes a foundational step towards scaling up early-exit LLMs by supporting their
  training and inference with massive 3D parallelism. Built upon Megatron-LM, EE-LLM
  implements a variety of algorithmic innovations and performance optimizations tailored
  to early exiting, including a lightweight method that facilitates backpropagation
  for the early-exit training objective with pipeline parallelism, techniques of leveraging
  idle resources in the original pipeline schedule for computation related to early-exit
  layers, and two approaches of early-exit inference that are compatible with KV caching
  for autoregressive generation. Our analytical and empirical study shows that EE-LLM
  achieves great training efficiency with negligible computational overhead compared
  to standard LLM training, as well as outstanding inference speedup without compromising
  output quality. To facilitate further research and adoption, we release EE-LLM at
  https://github.com/pan-x-c/EE-LLM.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: chen24ae
month: 0
tex_title: "{EE}-{LLM}: Large-Scale Training and Inference of Early-Exit Large Language
  Models with 3{D} Parallelism"
firstpage: 7163
lastpage: 7189
page: 7163-7189
order: 7163
cycles: false
bibtex_author: Chen, Yanxi and Pan, Xuchen and Li, Yaliang and Ding, Bolin and Zhou,
  Jingren
author:
- given: Yanxi
  family: Chen
- given: Xuchen
  family: Pan
- given: Yaliang
  family: Li
- given: Bolin
  family: Ding
- given: Jingren
  family: Zhou
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://raw.githubusercontent.com/mlresearch/v235/main/assets/chen24ae/chen24ae.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
