---
title: Invariant Risk Minimization Is A Total Variation Model
openreview: P7qwBmzwwZ
abstract: Invariant risk minimization (IRM) is an arising approach to generalize invariant
  features to different environments in machine learning. While most related works
  focus on new IRM settings or new application scenarios, the mathematical essence
  of IRM remains to be properly explained. We verify that IRM is essentially a total
  variation based on $L^2$ norm (TV-$\ell_2$) of the learning risk with respect to
  the classifier variable. Moreover, we propose a novel IRM framework based on the
  TV-$\ell_1$ model. It not only expands the classes of functions that can be used
  as the learning risk and the feature extractor, but also has robust performance
  in denoising and invariant feature preservation based on the coarea formula. We
  also illustrate some requirements for IRM-TV-$\ell_1$ to achieve out-of-distribution
  generalization. Experimental results show that the proposed framework achieves competitive
  performance in several benchmark machine learning scenarios.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: lai24c
month: 0
tex_title: Invariant Risk Minimization Is A Total Variation Model
firstpage: 25913
lastpage: 25935
page: 25913-25935
order: 25913
cycles: false
bibtex_author: Lai, Zhao-Rong and Wang, Weiwen
author:
- given: Zhao-Rong
  family: Lai
- given: Weiwen
  family: Wang
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/lai24c/lai24c.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
