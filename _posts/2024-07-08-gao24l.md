---
title: Non-convex Stochastic Composite Optimization with Polyak Momentum
openreview: 1ySQI9LE4w
abstract: The stochastic proximal gradient method is a powerful generalization of
  the widely used stochastic gradient descent (SGD) method and has found numerous
  applications in Machine Learning. However, it is notoriously known that this method
  fails to converge in non-convex settings where the stochastic noise is significant
  (i.e. when only small or bounded batch sizes are used). In this paper, we focus
  on the stochastic proximal gradient method with Polyak momentum. We prove this method
  attains an optimal convergence rate for non-convex composite optimization problems,
  regardless of batch size. Additionally, we rigorously analyze the variance reduction
  effect of the Polyak momentum in the composite optimization setting and we show
  the method also converges when the proximal step can only be solved inexactly. Finally,
  we provide numerical experiments to validate our theoretical results.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: gao24l
month: 0
tex_title: Non-convex Stochastic Composite Optimization with Polyak Momentum
firstpage: 14826
lastpage: 14843
page: 14826-14843
order: 14826
cycles: false
bibtex_author: Gao, Yuan and Rodomanov, Anton and Stich, Sebastian U
author:
- given: Yuan
  family: Gao
- given: Anton
  family: Rodomanov
- given: Sebastian U
  family: Stich
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/assets/gao24l/gao24l.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
