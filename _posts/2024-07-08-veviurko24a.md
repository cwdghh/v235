---
title: 'To the Max: Reinventing Reward in Reinforcement Learning'
openreview: 4KQ0VwqPg8
abstract: In reinforcement learning (RL), different reward functions can define the
  same optimal policy but result in drastically different learning performance. For
  some, the agent gets stuck with a suboptimal behavior, and for others, it solves
  the task efficiently. Choosing a good reward function is hence an extremely important
  yet challenging problem. In this paper, we explore an alternative approach for using
  rewards for learning. We introduce <em>max-reward RL</em>, where an agent optimizes
  the maximum rather than the cumulative reward. Unlike earlier works, our approach
  works for deterministic and stochastic environments and can be easily combined with
  state-of-the-art RL algorithms. In the experiments, we study the performance of
  max-reward RL algorithms in two goal-reaching environments from Gymnasium-Robotics
  and demonstrate its benefits over standard RL. The code is available at https://github.com/veviurko/To-the-Max.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: veviurko24a
month: 0
tex_title: 'To the Max: Reinventing Reward in Reinforcement Learning'
firstpage: 49455
lastpage: 49470
page: 49455-49470
order: 49455
cycles: false
bibtex_author: Veviurko, Grigorii and Boehmer, Wendelin and Weerdt, Mathijs De
author:
- given: Grigorii
  family: Veviurko
- given: Wendelin
  family: Boehmer
- given: Mathijs De
  family: Weerdt
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/veviurko24a/veviurko24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
