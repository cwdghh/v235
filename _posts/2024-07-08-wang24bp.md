---
title: Integrated Hardware Architecture and Device Placement Search
openreview: ucl3B05EsX
abstract: Distributed execution of deep learning training involves a dynamic interplay
  between hardware accelerator architecture and device placement strategy. This is
  the first work to explore the co-optimization of determining the optimal architecture
  and device placement strategy through novel algorithms, improving the balance of
  computational resources, memory usage, and data distribution. Our architecture search
  leverages tensor and vector units, determining their quantity and dimensionality,
  and on-chip and off-chip memory configurations. It also determines the microbatch
  size and decides whether to recompute or stash activations, balancing the memory
  footprint of training and storage size. For each explored architecture configuration,
  we use an Integer Linear Program (ILP) to find the optimal schedule for executing
  operators on the accelerator. The ILP results then integrate with a dynamic programming
  solution to identify the most effective device placement strategy, combining data,
  pipeline, and tensor model parallelism across multiple accelerators. Our approach
  achieves higher throughput on large language models compared to the state-of-the-art
  TPUv4 and the Spotlight accelerator search framework. The entire source code of
  PHAZE is available at https://github.com/msr-fiddle/phaze.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: wang24bp
month: 0
tex_title: Integrated Hardware Architecture and Device Placement Search
firstpage: 51523
lastpage: 51545
page: 51523-51545
order: 51523
cycles: false
bibtex_author: Wang, Irene and Tarnawski, Jakub and Phanishayee, Amar and Mahajan,
  Divya
author:
- given: Irene
  family: Wang
- given: Jakub
  family: Tarnawski
- given: Amar
  family: Phanishayee
- given: Divya
  family: Mahajan
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/assets/wang24bp/wang24bp.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
