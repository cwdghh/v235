---
title: Hierarchical Neural Operator Transformer with Learnable Frequency-aware Loss
  Prior for Arbitrary-scale Super-resolution
openreview: LhAuVPWq6q
abstract: In this work, we present an arbitrary-scale super-resolution (SR) method
  to enhance the resolution of scientific data, which often involves complex challenges
  such as continuity, multi-scale physics, and the intricacies of high-frequency signals.
  Grounded in operator learning, the proposed method is resolution-invariant. The
  core of our model is a hierarchical neural operator that leverages a Galerkin-type
  self-attention mechanism, enabling efficient learning of mappings between function
  spaces. Sinc filters are used to facilitate the information transfer across different
  levels in the hierarchy, thereby ensuring representation equivalence in the proposed
  neural operator. Additionally, we introduce a learnable prior structure that is
  derived from the spectral resizing of the input data. This loss prior is model-agnostic
  and is designed to dynamically adjust the weighting of pixel contributions, thereby
  balancing gradients effectively across the model. We conduct extensive experiments
  on diverse datasets from different domains and demonstrate consistent improvements
  compared to strong baselines, which consist of various state-of-the-art SR methods.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: luo24g
month: 0
tex_title: Hierarchical Neural Operator Transformer with Learnable Frequency-aware
  Loss Prior for Arbitrary-scale Super-resolution
firstpage: 33466
lastpage: 33485
page: 33466-33485
order: 33466
cycles: false
bibtex_author: Luo, Xihaier and Qian, Xiaoning and Yoon, Byung-Jun
author:
- given: Xihaier
  family: Luo
- given: Xiaoning
  family: Qian
- given: Byung-Jun
  family: Yoon
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://raw.githubusercontent.com/mlresearch/v235/main/assets/luo24g/luo24g.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
