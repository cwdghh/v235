---
title: Do Topological Characteristics Help in Knowledge Distillation?
openreview: 2dEH0u8w0b
abstract: Knowledge distillation (KD) aims to transfer knowledge from larger (teacher)
  to smaller (student) networks. Previous studies focus on point-to-point or pairwise
  relationships in embedding features as knowledge and struggle to efficiently transfer
  relationships of complex latent spaces. To tackle this issue, we propose a novel
  KD method called TopKD, which considers the global topology of the latent spaces.
  We define <em>global topology knowledge</em> using the persistence diagram (PD)
  that captures comprehensive geometric structures such as shape of distribution,
  multiscale structure and connectivity, and the <em>topology distillation loss</em>
  for teaching this knowledge. To make the PD transferable within reasonable computational
  time, we employ approximated persistence images of PDs. Through experiments, we
  support the benefits of using global topology as knowledge and demonstrate the potential
  of TopKD. Code is available at https://github.com/jekim5418/TopKD
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: kim24aj
month: 0
tex_title: Do Topological Characteristics Help in Knowledge Distillation?
firstpage: 24674
lastpage: 24693
page: 24674-24693
order: 24674
cycles: false
bibtex_author: Kim, Jungeun and You, Junwon and Lee, Dongjin and Kim, Ha Young and
  Jung, Jae-Hun
author:
- given: Jungeun
  family: Kim
- given: Junwon
  family: You
- given: Dongjin
  family: Lee
- given: Ha Young
  family: Kim
- given: Jae-Hun
  family: Jung
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/kim24aj/kim24aj.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
