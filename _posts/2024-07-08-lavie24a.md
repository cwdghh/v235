---
title: 'Towards Understanding Inductive Bias in Transformers: A View From Infinity'
openreview: HOMXUneCTR
abstract: We study inductive bias in Transformers in the infinitely over-parameterized
  Gaussian process limit and argue transformers tend to be biased towards more permutation
  symmetric functions in sequence space. We show that the representation theory of
  the symmetric group can be used to give quantitative analytical predictions when
  the dataset is symmetric to permutations between tokens. We present a simplified
  transformer block and solve the model at the limit, including accurate predictions
  for the learning curves and network outputs. We show that in common setups, one
  can derive tight bounds in the form of a scaling law for the learnability as a function
  of the context length. Finally, we argue WikiText dataset, does indeed possess a
  degree of permutation symmetry.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: lavie24a
month: 0
tex_title: 'Towards Understanding Inductive Bias in Transformers: A View From Infinity'
firstpage: 26043
lastpage: 26069
page: 26043-26069
order: 26043
cycles: false
bibtex_author: Lavie, Itay and Gur-Ari, Guy and Ringel, Zohar
author:
- given: Itay
  family: Lavie
- given: Guy
  family: Gur-Ari
- given: Zohar
  family: Ringel
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/assets/lavie24a/lavie24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
