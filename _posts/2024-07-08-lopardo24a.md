---
title: 'Attention Meets Post-hoc Interpretability: A Mathematical Perspective'
openreview: wnkC5T11Z9
abstract: Attention-based architectures, in particular transformers, are at the heart
  of a technological revolution. Interestingly, in addition to helping obtain state-of-the-art
  results on a wide range of applications, the attention mechanism intrinsically provides
  meaningful insights on the internal behavior of the model. Can these insights be
  used as explanations? Debate rages on. In this paper, we mathematically study a
  simple attention-based architecture and pinpoint the differences between post-hoc
  and attention-based explanations. We show that they provide quite different results,
  and that, despite their limitations, post-hoc methods are capable of capturing more
  useful insights than merely examining the attention weights.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: lopardo24a
month: 0
tex_title: 'Attention Meets Post-hoc Interpretability: A Mathematical Perspective'
firstpage: 32781
lastpage: 32800
page: 32781-32800
order: 32781
cycles: false
bibtex_author: Lopardo, Gianluigi and Precioso, Frederic and Garreau, Damien
author:
- given: Gianluigi
  family: Lopardo
- given: Frederic
  family: Precioso
- given: Damien
  family: Garreau
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/lopardo24a/lopardo24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
