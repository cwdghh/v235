---
title: 'When Linear Attention Meets Autoregressive Decoding: Towards More Effective
  and Efficient Linearized Large Language Models'
openreview: 7mFSaP6IiN
abstract: 'Autoregressive Large Language Models (LLMs) have achieved impressive performance
  in language tasks but face two significant bottlenecks: (1) quadratic complexity
  in the attention module as the number of tokens increases, and (2) limited efficiency
  due to the sequential processing nature of autoregressive LLMs during generation.
  While linear attention and speculative decoding offer potential solutions, their
  applicability and synergistic potential for enhancing autoregressive LLMs remain
  uncertain. We conduct the first comprehensive study on the efficacy of existing
  linear attention methods for autoregressive LLMs, integrating them with speculative
  decoding. We introduce an augmentation technique for linear attention that ensures
  compatibility with speculative decoding, enabling more efficient training and serving
  of LLMs. Extensive experiments and ablation studies involving seven existing linear
  attention models and five encoder/decoder-based LLMs consistently validate the effectiveness
  of our augmented linearized LLMs. Notably, our approach achieves up to a 6.67 reduction
  in perplexity on the LLaMA model and up to a 2$\times$ speedup during generation
  compared to prior linear attention methods. Codes and models are available at https://github.com/GATECH-EIC/Linearized-LLM.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: you24a
month: 0
tex_title: 'When Linear Attention Meets Autoregressive Decoding: Towards More Effective
  and Efficient Linearized Large Language Models'
firstpage: 57350
lastpage: 57366
page: 57350-57366
order: 57350
cycles: false
bibtex_author: You, Haoran and Fu, Yichao and Wang, Zheng and Yazdanbakhsh, Amir and
  Lin, Yingyan Celine
author:
- given: Haoran
  family: You
- given: Yichao
  family: Fu
- given: Zheng
  family: Wang
- given: Amir
  family: Yazdanbakhsh
- given: Yingyan Celine
  family: Lin
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/you24a/you24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
