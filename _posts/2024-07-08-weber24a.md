---
title: Reinforcement Learning and Regret Bounds for Admission Control
openreview: Vdr87ZUfnl
abstract: The expected regret of any reinforcement learning algorithm is lower bounded
  by $\Omega\left(\sqrt{DXAT}\right)$ for undiscounted returns, where $D$ is the diameter
  of the Markov decision process, $X$ the size of the state space, $A$ the size of
  the action space and $T$ the number of time steps. However, this lower bound is
  general. A smaller regret can be obtained by taking into account some specific knowledge
  of the problem structure. In this article, we consider an admission control problem
  to an $M/M/c/S$ queue with $m$ job classes and class-dependent rewards and holding
  costs. Queuing systems often have a diameter that is exponential in the buffer size
  $S$, making the previous lower bound prohibitive for any practical use. We propose
  an algorithm inspired by UCRL2, and use the structure of the problem to upper bound
  the expected total regret by $O(S\log T + \sqrt{mT \log T})$ in the finite server
  case. In the infinite server case, we prove that the dependence of the regret on
  $S$ disappears.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: weber24a
month: 0
tex_title: Reinforcement Learning and Regret Bounds for Admission Control
firstpage: 52403
lastpage: 52427
page: 52403-52427
order: 52403
cycles: false
bibtex_author: Weber, Lucas and Busic, Ana and Zhu, Jiamin
author:
- given: Lucas
  family: Weber
- given: Ana
  family: Busic
- given: Jiamin
  family: Zhu
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/assets/weber24a/weber24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
