---
title: 'Dynamic Memory Compression: Retrofitting LLMs for Accelerated Inference'
openreview: tDRYrAkOB7
abstract: Transformers have emerged as the backbone of large language models (LLMs).
  However, generation remains inefficient due to the need to store in memory a cache
  of key–value representations for past tokens, whose size scales linearly with the
  input sequence length and batch size. As a solution, we propose Dynamic Memory Compression
  (DMC), a method for on-line key–value cache compression at inference time. Most
  importantly, the model learns to apply different compression ratios in different
  heads and layers. We retrofit pre-trained LLMs such as Llama 2 (7B, 13B and 70B)
  into DMC Transformers, achieving up to $\sim 3.7 \times$ throughput increase during
  auto-regressive inference on an NVIDIA H100 GPU. DMC is applied via continued pre-training
  on a negligible percentage of the original data without adding any extra parameters.
  We find that DMC preserves the original downstream performance with up to 4$\times$
  cache compression, outperforming up-trained grouped-query attention (GQA) and key–value
  eviction policies (H$_2$O, TOVA). GQA and DMC can be even combined to obtain compounded
  gains. As a result DMC fits longer contexts and larger batches within any given
  memory budget. We release the DMC code and models at https://github.com/NVIDIA/Megatron-LM/tree/DMC.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: nawrot24a
month: 0
tex_title: 'Dynamic Memory Compression: Retrofitting {LLM}s for Accelerated Inference'
firstpage: 37396
lastpage: 37412
page: 37396-37412
order: 37396
cycles: false
bibtex_author: Nawrot, Piotr and {\L}a\'{n}cucki, Adrian and Chochowski, Marcin and
  Tarjan, David and Ponti, Edoardo
author:
- given: Piotr
  family: Nawrot
- given: Adrian
  family: Łańcucki
- given: Marcin
  family: Chochowski
- given: David
  family: Tarjan
- given: Edoardo
  family: Ponti
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://raw.githubusercontent.com/mlresearch/v235/main/assets/nawrot24a/nawrot24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
