---
title: Random Latent Exploration for Deep Reinforcement Learning
openreview: Y9qzwNlKVU
abstract: The ability to efficiently explore high-dimensional state spaces is essential
  for the practical success of deep Reinforcement Learning (RL). This paper introduces
  a new exploration technique called Random Latent Exploration (RLE), that combines
  the strengths of exploration bonuses and randomized value functions (two popular
  approaches for effective exploration in deep RL). RLE leverages the idea of perturbing
  rewards by adding structured random rewards to the original task rewards in certain
  (random) states of the environment, to encourage the agent to explore the environment
  during training. RLE is straightforward to implement and performs well in practice.
  To demonstrate the practical effectiveness of RLE, we evaluate it on the challenging
  Atari and IsaacGym benchmarks and show that RLE exhibits higher overall scores across
  all the tasks than other approaches, including action-noise and randomized value
  function exploration.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: mahankali24a
month: 0
tex_title: Random Latent Exploration for Deep Reinforcement Learning
firstpage: 34219
lastpage: 34252
page: 34219-34252
order: 34219
cycles: false
bibtex_author: Mahankali, Srinath V. and Hong, Zhang-Wei and Sekhari, Ayush and Rakhlin,
  Alexander and Agrawal, Pulkit
author:
- given: Srinath V.
  family: Mahankali
- given: Zhang-Wei
  family: Hong
- given: Ayush
  family: Sekhari
- given: Alexander
  family: Rakhlin
- given: Pulkit
  family: Agrawal
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/mahankali24a/mahankali24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
