---
title: 'SiT: Symmetry-invariant Transformers for Generalisation in Reinforcement Learning'
openreview: SWrwurHAeq
abstract: An open challenge in reinforcement learning (RL) is the effective deployment
  of a trained policy to new or slightly different situations as well as semantically-similar
  environments. We introduce <b>S</b>ymmetry-<b>I</b>nvariant <b>T</b>ransformer (<b>SiT</b>),
  a scalable vision transformer (ViT) that leverages both local and global data patterns
  in a self-supervised manner to improve generalisation. Central to our approach is
  Graph Symmetric Attention, which refines the traditional self-attention mechanism
  to preserve graph symmetries, resulting in invariant and equivariant latent representations.
  We showcase SiTâ€™s superior generalization over ViTs on MiniGrid and Procgen RL benchmarks,
  and its sample efficiency on Atari 100k and CIFAR10.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: weissenbacher24a
month: 0
tex_title: "{S}i{T}: Symmetry-invariant Transformers for Generalisation in Reinforcement
  Learning"
firstpage: 52695
lastpage: 52719
page: 52695-52719
order: 52695
cycles: false
bibtex_author: Weissenbacher, Matthias and Agarwal, Rishabh and Kawahara, Yoshinobu
author:
- given: Matthias
  family: Weissenbacher
- given: Rishabh
  family: Agarwal
- given: Yoshinobu
  family: Kawahara
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/weissenbacher24a/weissenbacher24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
