---
title: Unsupervised Evaluation of Code LLMs with Round-Trip Correctness
openreview: YnFuUX08CE
abstract: To evaluate code large language models (LLMs), research has relied on a
  few small manually curated benchmarks, such as HumanEval and MBPP, which represent
  a narrow part of the real-world software domains. In this work, we introduce round-trip
  correctness (RTC) as an alternative evaluation method. RTC allows Code LLM evaluation
  on a broader spectrum of real-world software domains without the need for costly
  human curation. RTC rests on the idea that we can ask a model to make a prediction
  (e.g., describe some code using natural language), feed that prediction back (e.g.,
  synthesize code from the predicted description), and check if this round-trip leads
  to code that is semantically equivalent to the original input. We show how to employ
  RTC to evaluate code synthesis and editing. We find that RTC strongly correlates
  with model performance on existing narrow-domain code synthesis benchmarks while
  allowing us to expand to a much broader set of domains and tasks which was not previously
  possible without costly human annotations.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: allamanis24a
month: 0
tex_title: Unsupervised Evaluation of Code {LLM}s with Round-Trip Correctness
firstpage: 1050
lastpage: 1066
page: 1050-1066
order: 1050
cycles: false
bibtex_author: Allamanis, Miltiadis and Panthaplackel, Sheena and Yin, Pengcheng
author:
- given: Miltiadis
  family: Allamanis
- given: Sheena
  family: Panthaplackel
- given: Pengcheng
  family: Yin
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/allamanis24a/allamanis24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
