---
title: Causality Based Front-door Defense Against Backdoor Attack on Language Models
openreview: dmHHVcHFdM
abstract: 'We have developed a new framework based on the theory of causal inference
  to protect language models against backdoor attacks. Backdoor attackers can poison
  language models with different types of triggers, such as words, sentences, grammar,
  and style, enabling them to selectively modify the decision-making of the victim
  model. However, existing defense approaches are only effective when the backdoor
  attack form meets specific assumptions, making it difficult to counter diverse backdoor
  attacks. We propose a new defense framework <b>F</b>ront-door <b>A</b>djustment
  for <b>B</b>ackdoor <b>E</b>limination (FABE) based on causal reasoning that does
  not rely on assumptions about the form of triggers. This method effectively differentiates
  between spurious and legitimate associations by creating a ’front door’ that maps
  out the actual causal relationships. The term ’front door’ refers to a text that
  retains the semantic equivalence of the initial input, which is generated by an
  additional, fine-tuned language model, denoted as the defense model. Our defense
  experiments against various attack methods at the token, sentence, and syntactic
  levels reduced the attack success rate from 93.63% to 15.12%, improving the defense
  effect by 2.91 times compared to the best baseline result of 66.61%, achieving state-of-the-art
  results. Through ablation study analysis, we analyzed the effect of each module
  in FABE, demonstrating the importance of complying with the front-door criterion
  and front-door adjustment formula, which also explains why previous methods failed.
  Our code to reproduce the experiments is available at: https://github.com/lyr17/Frontdoor-Adjustment-Backdoor-Elimination.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: liu24bu
month: 0
tex_title: Causality Based Front-door Defense Against Backdoor Attack on Language
  Models
firstpage: 32239
lastpage: 32252
page: 32239-32252
order: 32239
cycles: false
bibtex_author: Liu, Yiran and Xu, Xiaoang and Hou, Zhiyi and Yu, Yang
author:
- given: Yiran
  family: Liu
- given: Xiaoang
  family: Xu
- given: Zhiyi
  family: Hou
- given: Yang
  family: Yu
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/assets/liu24bu/liu24bu.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
