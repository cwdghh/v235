---
title: Quasi-Monte Carlo Features for Kernel Approximation
openreview: gSMUjrkRRk
abstract: Random features (Rahimi & Recht, 2007), based on Monte Carlo (MC) method,
  is one of the most popular approximation techniques to accelerate kernel methods.
  We show for a class of kernels, including Gaussian kernels, quasi-Monte Carlo (QMC)
  methods can be used in place of MC to improve the approximation error from $O_P(1/\sqrt{M})$
  to $O(1/M)$ (up to logarithmic factors), for estimating both the kernel function
  itself and the associated integral operator, where $M$ is the number of features
  being used. Furthermore, we demonstrate the advantage of QMC features in the case
  of kernel ridge regression, where theoretically, fewer random features suffice to
  guarantee the same convergence rate of the excess risk. In practice, the QMC kernel
  approximation approach is easily implementable and shows superior performance, as
  supported by the empirical evidence provided in the paper.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: huang24w
month: 0
tex_title: Quasi-{M}onte {C}arlo Features for Kernel Approximation
firstpage: 20134
lastpage: 20165
page: 20134-20165
order: 20134
cycles: false
bibtex_author: Huang, Zhen and Sun, Jiajin and Huang, Yian
author:
- given: Zhen
  family: Huang
- given: Jiajin
  family: Sun
- given: Yian
  family: Huang
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://raw.githubusercontent.com/mlresearch/v235/main/assets/huang24w/huang24w.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
