---
title: How to Escape Sharp Minima with Random Perturbations
openreview: tpYHbEl7P1
abstract: Modern machine learning applications have witnessed the remarkable success
  of optimization algorithms that are designed to find flat minima. Motivated by this
  design choice, we undertake a formal study that (i) formulates the notion of flat
  minima, and (ii) studies the complexity of finding them. Specifically, we adopt
  the trace of the Hessian of the cost function as a measure of flatness, and use
  it to formally define the notion of approximate flat minima. Under this notion,
  we then analyze algorithms that find approximate flat minima efficiently. For general
  cost functions, we discuss a gradient-based algorithm that finds an approximate
  flat local minimum efficiently. The main component of the algorithm is to use gradients
  computed from randomly perturbed iterates to estimate a direction that leads to
  flatter minima. For the setting where the cost function is an empirical risk over
  training data, we present a faster algorithm that is inspired by a recently proposed
  practical algorithm called sharpness-aware minimization, supporting its success
  in practice.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: ahn24a
month: 0
tex_title: How to Escape Sharp Minima with Random Perturbations
firstpage: 597
lastpage: 618
page: 597-618
order: 597
cycles: false
bibtex_author: Ahn, Kwangjun and Jadbabaie, Ali and Sra, Suvrit
author:
- given: Kwangjun
  family: Ahn
- given: Ali
  family: Jadbabaie
- given: Suvrit
  family: Sra
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/ahn24a/ahn24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
