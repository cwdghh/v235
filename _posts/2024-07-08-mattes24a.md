---
title: 'Hieros: Hierarchical Imagination on Structured State Space Sequence World
  Models'
openreview: IUBhvyJ9Sr
abstract: One of the biggest challenges to modern deep reinforcement learning (DRL)
  algorithms is sample efficiency. Many approaches learn a world model in order to
  train an agent entirely in imagination, eliminating the need for direct environment
  interaction during training. However, these methods often suffer from either a lack
  of imagination accuracy, exploration capabilities, or runtime efficiency. We propose
  HIEROS, a hierarchical policy that learns time abstracted world representations
  and imagines trajectories at multiple time scales in latent space. HIEROS uses an
  S5 layer-based world model, which predicts next world states in parallel during
  training and iteratively during environment interaction. Due to the special properties
  of S5 layers, our method can train in parallel and predict next world states iteratively
  during imagination. This allows for more efficient training than RNN-based world
  models and more efficient imagination than Transformer-based world models. We show
  that our approach outperforms the state of the art in terms of mean and median normalized
  human score on the Atari 100k benchmark, and that our proposed world model is able
  to predict complex dynamics very accurately. We also show that HIEROS displays superior
  exploration capabilities compared to existing approaches.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: mattes24a
month: 0
tex_title: 'Hieros: Hierarchical Imagination on Structured State Space Sequence World
  Models'
firstpage: 35079
lastpage: 35103
page: 35079-35103
order: 35079
cycles: false
bibtex_author: Mattes, Paul and Schlosser, Rainer and Herbrich, Ralf
author:
- given: Paul
  family: Mattes
- given: Rainer
  family: Schlosser
- given: Ralf
  family: Herbrich
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://raw.githubusercontent.com/mlresearch/v235/main/assets/mattes24a/mattes24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
