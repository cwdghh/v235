---
title: Improving Transformers with Dynamically Composable Multi-Head Attention
openreview: RbiBKPtuHp
abstract: Multi-Head Attention (MHA) is a key component of Transformer. In MHA, attention
  heads work independently, causing problems such as low-rank bottleneck of attention
  score matrices and head redundancy. We propose Dynamically Composable Multi-Head
  Attention (DCMHA), a parameter and computation efficient attention architecture
  that tackles the shortcomings of MHA and increases the expressive power of the model
  by dynamically composing attention heads. At the core of DCMHA is a Compose function
  that transforms the attention score and weight matrices in an input-dependent way.
  DCMHA can be used as a drop-in replacement of MHA in any transformer architecture
  to obtain the corresponding DCFormer. DCFormer significantly outperforms Transformer
  on different architectures and model scales in language modeling, matching the performance
  of models with 1.7x-2.0x compute. For example, DCPythia-6.9B outperforms open source
  Pythia-12B on both pretraining perplexity and downstream task evaluation.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: xiao24d
month: 0
tex_title: Improving Transformers with Dynamically Composable Multi-Head Attention
firstpage: 54300
lastpage: 54318
page: 54300-54318
order: 54300
cycles: false
bibtex_author: Xiao, Da and Meng, Qingye and Li, Shengping and Yuan, Xingyuan
author:
- given: Da
  family: Xiao
- given: Qingye
  family: Meng
- given: Shengping
  family: Li
- given: Xingyuan
  family: Yuan
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/assets/xiao24d/xiao24d.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
