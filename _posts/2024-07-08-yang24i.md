---
title: 'Better Safe than Sorry: Pre-training CLIP against Targeted Data Poisoning
  and Backdoor Attacks'
openreview: ycLHJuLYuD
abstract: Contrastive Language-Image Pre-training (CLIP) on large image-caption datasets
  has achieved remarkable success in zero-shot classification and enabled transferability
  to new domains. However, CLIP is extremely more vulnerable to targeted data poisoning
  and backdoor attacks compared to supervised learning. Perhaps surprisingly, poisoning
  0.0001% of CLIP pre-training data is enough to make targeted data poisoning attacks
  successful. This is four orders of magnitude smaller than what is required to poison
  supervised models. Despite this vulnerability, existing methods are very limited
  in defending CLIP models during pre-training. In this work, we propose a strong
  defense, SAFECLIP, to safely pre-train CLIP against targeted data poisoning and
  backdoor attacks. SAFECLIP warms up the model by applying unimodal contrastive learning
  (CL) on image and text modalities separately. Then, it divides the data into safe
  and risky sets by applying a Gaussian Mixture Model to the cosine similarity of
  image-caption pair representations. SAFECLIP pre-trains the model by applying the
  CLIP loss to the safe set and applying unimodal CL to image and text modalities
  of the risky set separately. By gradually increasing the size of the safe set during
  pre-training, SAFECLIP effectively breaks targeted data poisoning and backdoor attacks
  without harming the CLIP performance. Our extensive experiments on CC3M, Visual
  Genome, and MSCOCO demonstrate that SAFECLIP significantly reduces the success rate
  of targeted data poisoning attacks from 93.75% to 0% and that of various backdoor
  attacks from up to 100% to 0%, without harming CLIPâ€™s performance.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: yang24i
month: 0
tex_title: 'Better Safe than Sorry: Pre-training {CLIP} against Targeted Data Poisoning
  and Backdoor Attacks'
firstpage: 56096
lastpage: 56108
page: 56096-56108
order: 56096
cycles: false
bibtex_author: Yang, Wenhan and Gao, Jingdong and Mirzasoleiman, Baharan
author:
- given: Wenhan
  family: Yang
- given: Jingdong
  family: Gao
- given: Baharan
  family: Mirzasoleiman
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/assets/yang24i/yang24i.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
