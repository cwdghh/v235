---
title: Aligning Transformers with Weisfeiler-Leman
openreview: 4FJJfYjUQR
abstract: Graph neural network architectures aligned with the $k$-dimensional Weisfeiler–Leman
  ($k$-WL) hierarchy offer theoretically well-understood expressive power. However,
  these architectures often fail to deliver state-of-the-art predictive performance
  on real-world graphs, limiting their practical utility. While recent works aligning
  graph transformer architectures with the $k$-WL hierarchy have shown promising empirical
  results, employing transformers for higher orders of $k$ remains challenging due
  to a prohibitive runtime and memory complexity of self-attention as well as impractical
  architectural assumptions, such as an infeasible number of attention heads. Here,
  we advance the alignment of transformers with the $k$-WL hierarchy, showing stronger
  expressivity results for each $k$, making them more feasible in practice. In addition,
  we develop a theoretical framework that allows the study of established positional
  encodings such as Laplacian PEs and SPE. We evaluate our transformers on the large-scale
  PCQM4Mv2 dataset, showing competitive predictive performance with the state-of-the-art
  and demonstrating strong downstream performance when fine-tuning them on small-scale
  molecular datasets.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: muller24c
month: 0
tex_title: Aligning Transformers with Weisfeiler-Leman
firstpage: 36654
lastpage: 36704
page: 36654-36704
order: 36654
cycles: false
bibtex_author: M\"{u}ller, Luis and Morris, Christopher
author:
- given: Luis
  family: Müller
- given: Christopher
  family: Morris
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/muller24c/muller24c.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
