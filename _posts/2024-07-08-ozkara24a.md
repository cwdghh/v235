---
title: 'MADA: Meta-Adaptive Optimizers Through Hyper-Gradient Descent'
openreview: tASXcrMekp
abstract: Following the introduction of Adam, several novel adaptive optimizers for
  deep learning have been proposed. These optimizers typically excel in some tasks
  but may not outperform Adam uniformly across all tasks. In this work, we introduce
  Meta-Adaptive Optimizers (MADA), a unified optimizer framework that can generalize
  several known optimizers and dynamically learn the most suitable one during training.
  The key idea in MADA is to parameterize the space of optimizers and dynamically
  search through it using hyper-gradient descent during training. We empirically compare
  MADA to other popular optimizers on vision and language tasks, and find that MADA
  consistently outperforms Adam and other popular optimizers, and is robust against
  sub-optimally tuned hyper-parameters. MADA achieves a greater validation performance
  improvement over Adam compared to other popular optimizers during GPT-2 training
  and fine-tuning. We also propose AVGrad, a modification of AMSGrad that replaces
  the maximum operator with averaging, which is more suitable for hyper-gradient optimization.
  Finally, we provide a convergence analysis to show that parameterized interpolations
  of optimizers can improve their error bounds (up to constants), hinting at an advantage
  for meta-optimizers.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: ozkara24a
month: 0
tex_title: "{MADA}: Meta-Adaptive Optimizers Through Hyper-Gradient Descent"
firstpage: 38983
lastpage: 39008
page: 38983-39008
order: 38983
cycles: false
bibtex_author: Ozkara, Kaan and Karakus, Can and Raman, Parameswaran and Hong, Mingyi
  and Sabach, Shoham and Kveton, Branislav and Cevher, Volkan
author:
- given: Kaan
  family: Ozkara
- given: Can
  family: Karakus
- given: Parameswaran
  family: Raman
- given: Mingyi
  family: Hong
- given: Shoham
  family: Sabach
- given: Branislav
  family: Kveton
- given: Volkan
  family: Cevher
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://raw.githubusercontent.com/mlresearch/v235/main/assets/ozkara24a/ozkara24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
