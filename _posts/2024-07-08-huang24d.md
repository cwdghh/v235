---
title: In-context Convergence of Transformers
openreview: 9GLvXGkUE2
abstract: Transformers have recently revolutionized many domains in modern machine
  learning and one salient discovery is their remarkable in-context learning capability,
  where models can solve an unseen task by utilizing task-specific prompts without
  further parameters fine-tuning. This also inspired recent theoretical studies aiming
  to understand the in-context learning mechanism of transformers, which however focused
  only on <em>linear</em> transformers. In this work, we take the first step toward
  studying the learning dynamics of a one-layer transformer with <em>softmax</em>
  attention trained via gradient descent in order to in-context learn linear function
  classes. We consider a structured data model, where each token is randomly sampled
  from a set of feature vectors in either balanced or imbalanced fashion. For data
  with balanced features, we establish the finite-time convergence guarantee with
  near-zero prediction error by navigating our analysis over two phases of the training
  dynamics of the attention map. More notably, for data with imbalanced features,
  we show that the learning dynamics take a stage-wise convergence process, where
  the transformer first converges to a near-zero prediction error for the query tokens
  of dominant features, and then converges later to a near-zero error for query tokens
  of under-represented features, via one and four training phases. Our proof features
  new techniques for analyzing the competing strengths of two types of attention weights,
  the change of which determines different training phases.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: huang24d
month: 0
tex_title: In-context Convergence of Transformers
firstpage: 19660
lastpage: 19722
page: 19660-19722
order: 19660
cycles: false
bibtex_author: Huang, Yu and Cheng, Yuan and Liang, Yingbin
author:
- given: Yu
  family: Huang
- given: Yuan
  family: Cheng
- given: Yingbin
  family: Liang
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://raw.githubusercontent.com/mlresearch/v235/main/assets/huang24d/huang24d.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
