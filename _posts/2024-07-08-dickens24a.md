---
title: Convex and Bilevel Optimization for Neural-Symbolic Inference and Learning
openreview: 6NQ77Vj3DT
abstract: We leverage convex and bilevel optimization techniques to develop a general
  gradient-based parameter learning framework for neural-symbolic (NeSy) systems.
  We demonstrate our framework with NeuPSL, a state-of-the-art NeSy architecture.
  To achieve this, we propose a smooth primal and dual formulation of NeuPSL inference
  and show learning gradients are functions of the optimal dual variables. Additionally,
  we develop a dual block coordinate descent algorithm for the new formulation that
  naturally exploits warm-starts. This leads to over $100 \times$ learning runtime
  improvements over the current best NeuPSL inference method. Finally, we provide
  extensive empirical evaluations across $8$ datasets covering a range of tasks and
  demonstrate our learning framework achieves up to a $16$% point prediction performance
  improvement over alternative learning methods.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: dickens24a
month: 0
tex_title: Convex and Bilevel Optimization for Neural-Symbolic Inference and Learning
firstpage: 10865
lastpage: 10896
page: 10865-10896
order: 10865
cycles: false
bibtex_author: Dickens, Charles Andrew and Gao, Changyu and Pryor, Connor and Wright,
  Stephen and Getoor, Lise
author:
- given: Charles Andrew
  family: Dickens
- given: Changyu
  family: Gao
- given: Connor
  family: Pryor
- given: Stephen
  family: Wright
- given: Lise
  family: Getoor
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://raw.githubusercontent.com/mlresearch/v235/main/assets/dickens24a/dickens24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
