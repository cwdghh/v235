---
title: Harnessing Neural Unit Dynamics for Effective and Scalable Class-Incremental
  Learning
openreview: eDtty9ZCvt
abstract: Class-incremental learning (CIL) aims to train a model to learn new classes
  from non-stationary data streams without forgetting old ones. In this paper, we
  propose a new kind of connectionist model by tailoring neural unit dynamics that
  adapt the behavior of neural networks for CIL. In each training session, it introduces
  a supervisory mechanism to guide network expansion whose growth size is compactly
  commensurate with the intrinsic complexity of a newly arriving task. This constructs
  a near-minimal network while allowing the model to expand its capacity when cannot
  sufficiently hold new classes. At inference time, it automatically reactivates the
  required neural units to retrieve knowledge and leaves the remaining inactivated
  to prevent interference. We name our model AutoActivator, which is effective and
  scalable. To gain insights into the neural unit dynamics, we theoretically analyze
  the modelâ€™s convergence property via a universal approximation theorem on learning
  sequential mappings, which is under-explored in the CIL community. Experiments show
  that our method achieves strong CIL performance in rehearsal-free and minimal-expansion
  settings with different backbones.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: li24bk
month: 0
tex_title: Harnessing Neural Unit Dynamics for Effective and Scalable Class-Incremental
  Learning
firstpage: 28688
lastpage: 28705
page: 28688-28705
order: 28688
cycles: false
bibtex_author: Li, Depeng and Wang, Tianqi and Chen, Junwei and Dai, Wei and Zeng,
  Zhigang
author:
- given: Depeng
  family: Li
- given: Tianqi
  family: Wang
- given: Junwei
  family: Chen
- given: Wei
  family: Dai
- given: Zhigang
  family: Zeng
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/li24bk/li24bk.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
