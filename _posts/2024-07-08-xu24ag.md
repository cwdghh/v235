---
title: Random Masking Finds Winning Tickets for Parameter Efficient Fine-tuning
openreview: VrwIrAa1Lc
abstract: 'Fine-tuning large language models (LLM) can be costly. Parameter-efficient
  fine-tuning (PEFT) addresses the problems by training a fraction of the parameters,
  whose success reveals the expressiveness and flexibility of pretrained models. This
  paper studies the limit of PEFT, by further simplifying its design and reducing
  the number of trainable parameters beyond standard setups. To this end, we use Random
  Masking to fine-tune the pretrained model. Despite its simplicity, we show that
  Random Masking is surprisingly effective: with a larger-than-expected learning rate,
  Random Masking can match the performance of standard PEFT algorithms such as LoRA
  on various tasks, using fewer trainable parameters. We provide both empirical and
  theoretical explorations into the success of Random Masking. We show that masking
  induces a flatter loss landscape and more distant solutions, which allows for and
  necessitates large learning rates.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: xu24ag
month: 0
tex_title: Random Masking Finds Winning Tickets for Parameter Efficient Fine-tuning
firstpage: 55501
lastpage: 55524
page: 55501-55524
order: 55501
cycles: false
bibtex_author: Xu, Jing and Zhang, Jingzhao
author:
- given: Jing
  family: Xu
- given: Jingzhao
  family: Zhang
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/xu24ag/xu24ag.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
