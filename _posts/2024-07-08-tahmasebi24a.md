---
title: Sample Complexity Bounds for Estimating Probability Divergences under Invariances
openreview: sKjcrAC4eZ
abstract: 'Group-invariant probability distributions appear in many data-generative
  models in machine learning, such as graphs, point clouds, and images. In practice,
  one often needs to estimate divergences between such distributions. In this work,
  we study how the inherent invariances, with respect to any smooth action of a Lie
  group on a manifold, improve sample complexity when estimating the 1-Wasserstein
  distance, the Sobolev Integral Probability Metrics (Sobolev IPMs), the Maximum Mean
  Discrepancy (MMD), and also the complexity of the density estimation problem (in
  the $L^2$ and $L^\infty$ distance). Our results indicate a two-fold gain: (1) reducing
  the sample complexity by a multiplicative factor corresponding to the group size
  (for finite groups) or the normalized volume of the quotient space (for groups of
  positive dimension); (2) improving the exponent in the convergence rate (for groups
  of positive dimension). These results are completely new for groups of positive
  dimension and extend recent bounds for finite group actions.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: tahmasebi24a
month: 0
tex_title: Sample Complexity Bounds for Estimating Probability Divergences under Invariances
firstpage: 47396
lastpage: 47417
page: 47396-47417
order: 47396
cycles: false
bibtex_author: Tahmasebi, Behrooz and Jegelka, Stefanie
author:
- given: Behrooz
  family: Tahmasebi
- given: Stefanie
  family: Jegelka
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/assets/tahmasebi24a/tahmasebi24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
