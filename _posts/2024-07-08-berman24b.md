---
title: 'CoLoRA: Continuous low-rank adaptation for reduced implicit neural modeling
  of parameterized partial differential equations'
openreview: iHSgfGob9j
abstract: This work introduces reduced models based on Continuous Low Rank Adaptation
  (CoLoRA) that pre-train neural networks for a given partial differential equation
  and then continuously adapt low-rank weights in time to rapidly predict the evolution
  of solution fields at new physics parameters and new initial conditions. The adaptation
  can be either purely data-driven or via an equation-driven variational approach
  that provides Galerkin-optimal approximations. Because CoLoRA approximates solution
  fields locally in time, the rank of the weights can be kept small, which means that
  only few training trajectories are required offline so that CoLoRA is well suited
  for data-scarce regimes. Predictions with CoLoRA are orders of magnitude faster
  than with classical methods and their accuracy and parameter efficiency is higher
  compared to other neural network approaches.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: berman24b
month: 0
tex_title: "{C}o{L}o{RA}: Continuous low-rank adaptation for reduced implicit neural
  modeling of parameterized partial differential equations"
firstpage: 3565
lastpage: 3583
page: 3565-3583
order: 3565
cycles: false
bibtex_author: Berman, Jules and Peherstorfer, Benjamin
author:
- given: Jules
  family: Berman
- given: Benjamin
  family: Peherstorfer
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://raw.githubusercontent.com/mlresearch/v235/main/assets/berman24b/berman24b.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
