---
title: ReLU Network with Width $d+\mathcalO(1)$ Can Achieve Optimal Approximation
  Rate
openreview: o4HF3N6CZR
abstract: The prevalent employment of narrow neural networks, characterized by their
  minimal parameter count per layer, has led to a surge in research exploring their
  potential as universal function approximators. A notable result in this field states
  that networks with just a width of $d+1$ can approximate any continuous function
  for input dimension $d$ arbitrarily well. However, the optimal approximation rate
  for these narrowest networks, i.e., the optimal relation between the count of tunable
  parameters and the approximation error, remained unclear. In this paper, we address
  this gap by proving that ReLU networks with width $d+1$ can achieve the optimal
  approximation rate for continuous functions over the domain $[0,1]^d$ under $L^p$
  norm for $p\in[1,\infty)$. We further show that for the uniform norm, a width of
  $d+11$ is sufficient. We also extend the results to narrow feed-forward networks
  with various activations, confirming their capability to approximate at the optimal
  rate. This work adds to the understanding of universal approximation of narrow networks.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: liu24g
month: 0
tex_title: "{R}e{LU} Network with Width $d+\\mathcal{O}(1)$ Can Achieve Optimal Approximation
  Rate"
firstpage: 30755
lastpage: 30788
page: 30755-30788
order: 30755
cycles: false
bibtex_author: Liu, Chenghao and Chen, Minghua
author:
- given: Chenghao
  family: Liu
- given: Minghua
  family: Chen
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/assets/liu24g/liu24g.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
