---
title: 'Beyond Sole Strength: Customized Ensembles for Generalized Vision-Language
  Models'
openreview: Lc1HlMo77m
abstract: Fine-tuning pre-trained vision-language models (VLMs), e.g., CLIP, for the
  open-world generalization has gained increasing popularity due to its practical
  value. However, performance advancements are limited when relying solely on intricate
  algorithmic designs for a single model, even one exhibiting strong performance,
  e.g., CLIP-ViT-B/16. This paper, for the first time, explores the collaborative
  potential of leveraging much weaker VLMs to enhance the generalization of a robust
  single model. The affirmative findings motivate us to address the generalization
  problem from a novel perspective, i.e., ensemble of pre-trained VLMs. We introduce
  three customized ensemble strategies, each tailored to one specific scenario. Firstly,
  we introduce the zero-shot ensemble, automatically adjusting the logits of different
  models based on their confidence when only pre-trained VLMs are available. Furthermore,
  for scenarios with extra few-shot samples, we propose the training-free and tuning
  ensemble, offering flexibility based on the availability of computing resources.
  The code is available at https://github.com/zhiheLu/Ensemble_VLM.git.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: lu24a
month: 0
tex_title: 'Beyond Sole Strength: Customized Ensembles for Generalized Vision-Language
  Models'
firstpage: 32924
lastpage: 32938
page: 32924-32938
order: 32924
cycles: false
bibtex_author: Lu, Zhihe and Bai, Jiawang and Li, Xin and Xiao, Zeyu and Wang, Xinchao
author:
- given: Zhihe
  family: Lu
- given: Jiawang
  family: Bai
- given: Xin
  family: Li
- given: Zeyu
  family: Xiao
- given: Xinchao
  family: Wang
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://raw.githubusercontent.com/mlresearch/v235/main/assets/lu24a/lu24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
