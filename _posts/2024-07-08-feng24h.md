---
title: 'Privacy Backdoors: Stealing Data with Corrupted Pretrained Models'
openreview: 7yixJXmzb8
abstract: Practitioners commonly download pretrained machine learning models from
  open repositories and finetune them to fit specific applications. We show that this
  practice introduces a new risk of privacy backdoors. By tampering with a pretrained
  model’s weights, an attacker can fully compromise the privacy of the finetuning
  data. We show how to build privacy backdoors for a variety of models, including
  transformers, which enable an attacker to reconstruct individual finetuning samples,
  with a guaranteed success! We further show that backdoored models allow for tight
  privacy attacks on models trained with differential privacy (DP). The common optimistic
  practice of training DP models with loose privacy guarantees is thus insecure if
  the model is not trusted. Overall, our work highlights a crucial and overlooked
  supply chain attack on machine learning privacy.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: feng24h
month: 0
tex_title: 'Privacy Backdoors: Stealing Data with Corrupted Pretrained Models'
firstpage: 13326
lastpage: 13364
page: 13326-13364
order: 13326
cycles: false
bibtex_author: Feng, Shanglun and Tram\`{e}r, Florian
author:
- given: Shanglun
  family: Feng
- given: Florian
  family: Tramèr
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/assets/feng24h/feng24h.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
