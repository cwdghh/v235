---
title: Error Feedback Can Accurately Compress Preconditioners
openreview: OJTKlubFk1
abstract: Leveraging second-order information about the loss at the scale of deep
  networks is one of the main lines of approach for improving the performance of current
  optimizers for deep learning. Yet, existing approaches for accurate full-matrix
  preconditioning, such as Full-Matrix Adagrad (GGT) or Matrix-Free Approximate Curvature
  (M-FAC) suffer from massive storage costs when applied even to small-scale models,
  as they must store a sliding window of gradients, whose memory requirements are
  multiplicative in the model dimension. In this paper, we address this issue via
  a novel and efficient error-feedback technique that can be applied to compress preconditioners
  by up to two orders of magnitude in practice, without loss of convergence. Specifically,
  our approach compresses the gradient information via sparsification or low-rank
  compression before it is fed into the preconditioner, feeding the compression error
  back into future iterations. Extensive experiments on deep neural networks show
  that this approach can compress full-matrix preconditioners to up to 99% sparsity
  without accuracy loss, effectively removing the memory overhead of fullmatrix preconditioners
  such as GGT and M-FAC.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: modoranu24a
month: 0
tex_title: Error Feedback Can Accurately Compress Preconditioners
firstpage: 35910
lastpage: 35933
page: 35910-35933
order: 35910
cycles: false
bibtex_author: Modoranu, Ionut-Vlad and Kalinov, Aleksei and Kurtic, Eldar and Frantar,
  Elias and Alistarh, Dan
author:
- given: Ionut-Vlad
  family: Modoranu
- given: Aleksei
  family: Kalinov
- given: Eldar
  family: Kurtic
- given: Elias
  family: Frantar
- given: Dan
  family: Alistarh
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://raw.githubusercontent.com/mlresearch/v235/main/assets/modoranu24a/modoranu24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
