---
title: Federated Full-Parameter Tuning of Billion-Sized Language Models with Communication
  Cost under 18 Kilobytes
openreview: cit0hg4sEz
abstract: Pre-trained large language models (LLMs) need fine-tuning to improve their
  responsiveness to natural language instructions. Federated learning offers a way
  to fine-tune LLMs using the abundant data on end devices without compromising data
  privacy. Most existing federated fine-tuning methods for LLMs rely on parameter-efficient
  fine-tuning techniques, which may not reach the performance height possible with
  full-parameter tuning. However, federated full-parameter tuning of LLMs is a non-trivial
  problem due to the immense communication cost. This work introduces FedKSeed that
  employs zeroth-order optimization with a finite set of random seeds. It significantly
  reduces transmission requirements between the server and clients to just a few random
  seeds and scalar gradients, amounting to only a few thousand bytes, making federated
  full-parameter tuning of billion-sized LLMs possible on devices. Building on it,
  we develop a strategy enabling probability-differentiated seed sampling, prioritizing
  perturbations with greater impact on model accuracy. Experiments across six scenarios
  with various LLMs, datasets and data partitions demonstrate that our approach outperforms
  existing federated LLM fine-tuning methods in both communication efficiency and
  zero-shot generalization.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: qin24a
month: 0
tex_title: Federated Full-Parameter Tuning of Billion-Sized Language Models with Communication
  Cost under 18 Kilobytes
firstpage: 41473
lastpage: 41497
page: 41473-41497
order: 41473
cycles: false
bibtex_author: Qin, Zhen and Chen, Daoyuan and Qian, Bingchen and Ding, Bolin and
  Li, Yaliang and Deng, Shuiguang
author:
- given: Zhen
  family: Qin
- given: Daoyuan
  family: Chen
- given: Bingchen
  family: Qian
- given: Bolin
  family: Ding
- given: Yaliang
  family: Li
- given: Shuiguang
  family: Deng
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://raw.githubusercontent.com/mlresearch/v235/main/assets/qin24a/qin24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
