---
title: LoRA Training in the NTK Regime has No Spurious Local Minima
openreview: s1sdx6vNsU
abstract: 'Low-rank adaptation (LoRA) has become the standard approach for parameter-efficient
  fine-tuning of large language models (LLM), but our theoretical understanding of
  LoRA has been limited. In this work, we theoretically analyze LoRA fine-tuning in
  the neural tangent kernel (NTK) regime with $N$ data points, showing: (i) full fine-tuning
  (without LoRA) admits a low-rank solution of rank $r\lesssim \sqrt{N}$; (ii) using
  LoRA with rank $r\gtrsim \sqrt{N}$ eliminates spurious local minima, allowing gradient
  descent to find the low-rank solutions; (iii) the low-rank solution found using
  LoRA generalizes well.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: jang24d
month: 0
tex_title: "{L}o{RA} Training in the {NTK} Regime has No Spurious Local Minima"
firstpage: 21306
lastpage: 21328
page: 21306-21328
order: 21306
cycles: false
bibtex_author: Jang, Uijeong and Lee, Jason D. and Ryu, Ernest K.
author:
- given: Uijeong
  family: Jang
- given: Jason D.
  family: Lee
- given: Ernest K.
  family: Ryu
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://raw.githubusercontent.com/mlresearch/v235/main/assets/jang24d/jang24d.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
