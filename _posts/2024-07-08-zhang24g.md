---
title: Provably Efficient Partially Observable Risk-sensitive Reinforcement Learning
  with Hindsight Observation
openreview: 5S8ukkEQr2
abstract: This work pioneers regret analysis of risk-sensitive reinforcement learning
  in partially observable environments with hindsight observation, addressing a gap
  in theoretical exploration. We introduce a novel formulation that integrates hindsight
  observations into a Partially Observable Markov Decision Process (POMDP) framework,
  where the goal is to optimize accumulated reward under the entropic risk measure.
  We develop the first provably efficient RL algorithm tailored for this setting.
  We also prove by rigorous analysis that our algorithm achieves polynomial regret
  $\tilde{O}\left(\frac{e^{|{\gamma}|H}-1}{|{\gamma}|H}H^2\sqrt{KHS^2OA}\right)$,
  which outperforms or matches existing upper bounds when the model degenerates to
  risk-neutral or fully observable settings. We adopt the method of change-of-measure
  and develop a novel analytical tool of beta vectors to streamline mathematical derivations.
  These techniques are of particular interest to the theoretical study of reinforcement
  learning.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: zhang24g
month: 0
tex_title: Provably Efficient Partially Observable Risk-sensitive Reinforcement Learning
  with Hindsight Observation
firstpage: 58680
lastpage: 58716
page: 58680-58716
order: 58680
cycles: false
bibtex_author: Zhang, Tonghe and Chen, Yu and Huang, Longbo
author:
- given: Tonghe
  family: Zhang
- given: Yu
  family: Chen
- given: Longbo
  family: Huang
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/assets/zhang24g/zhang24g.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
