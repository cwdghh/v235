---
title: 'Generalized Smooth Variational Inequalities: Methods with Adaptive Stepsizes'
openreview: 4iBJyJeBX5
abstract: Variational Inequality (VI) problems have attracted great interest in the
  machine learning (ML) community due to their application in adversarial and multi-agent
  training. Despite its relevance in ML, the oft-used strong-monotonicity and Lipschitz
  continuity assumptions on VI problems are restrictive and do not hold in many machine
  learning problems. To address this, we relax smoothness and monotonicity assumptions
  and study structured non-monotone generalized smoothness. The key idea of our results
  is in adaptive stepsizes. We prove the first-known convergence results for solving
  generalized smooth VIs for the three popular methods, namely, projection, Korpelevich,
  and Popov methods. Our convergence rate results for generalized smooth VIs match
  or improve existing results on smooth VIs. We present numerical experiments that
  support our theoretical guarantees and highlight the efficiency of proposed adaptive
  stepsizes.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: vankov24a
month: 0
tex_title: 'Generalized Smooth Variational Inequalities: Methods with Adaptive Stepsizes'
firstpage: 49137
lastpage: 49170
page: 49137-49170
order: 49137
cycles: false
bibtex_author: Vankov, Daniil and Nedich, Angelia and Sankar, Lalitha
author:
- given: Daniil
  family: Vankov
- given: Angelia
  family: Nedich
- given: Lalitha
  family: Sankar
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/vankov24a/vankov24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
