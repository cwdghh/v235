---
title: A Primal-Dual Algorithm for Offline Constrained Reinforcement Learning with
  Linear MDPs
openreview: S80a4hJtuE
abstract: We study offline reinforcement learning (RL) with linear MDPs under the
  infinite-horizon discounted setting which aims to learn a policy that maximizes
  the expected discounted cumulative reward using a pre-collected dataset. Existing
  algorithms for this setting either require a uniform data coverage assumptions or
  are computationally inefficient for finding an $\epsilon$-optimal policy with $\mathcal{O}(\epsilon^{-2})$
  sample complexity. In this paper, we propose a primal dual algorithm for offline
  RL with linear MDPs in the infinite-horizon discounted setting. Our algorithm is
  the first computationally efficient algorithm in this setting that achieves sample
  complexity of $\mathcal{O}(\epsilon^{-2})$ with partial data coverage assumption.
  Our work is an improvement upon a recent work that requires $\mathcal{O}(\epsilon^{-4})$
  samples. Moreover, we extend our algorithm to work in the offline constrained RL
  setting that enforces constraints on additional reward signals.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: hong24e
month: 0
tex_title: A Primal-Dual Algorithm for Offline Constrained Reinforcement Learning
  with Linear {MDP}s
firstpage: 18711
lastpage: 18737
page: 18711-18737
order: 18711
cycles: false
bibtex_author: Hong, Kihyuk and Tewari, Ambuj
author:
- given: Kihyuk
  family: Hong
- given: Ambuj
  family: Tewari
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/assets/hong24e/hong24e.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
