---
title: Vision Transformers as Probabilistic Expansion from Learngene
openreview: 5ExWEazod5
abstract: Deep learning has advanced through the combination of large datasets and
  computational power, leading to the development of extensive pre-trained models
  like Vision Transformers (ViTs). However, these models often assume a one-size-fits-all
  utility, lacking the ability to initialize models with elastic scales tailored to
  the resource constraints of specific downstream tasks. To address these issues,
  we propose Probabilistic Expansion from LearnGene (PEG) for mixture sampling and
  elastic initialization of Vision Transformers. Specifically, PEG utilizes a probabilistic
  mixture approach to sample Multi-Head Self-Attention layers and Feed-Forward Networks
  from a large ancestry model into a more compact part termed as learngene. Theoretically,
  we demonstrate that these learngene can approximate the parameter distribution of
  the original ancestry model, thereby preserving its significant knowledge. Next,
  PEG expands the sampled learngene through non-linear mapping, enabling the initialization
  of descendant models with elastic scales to suit various resource constraints. Our
  extensive experiments demonstrate the effectiveness of PEG and outperforming traditional
  initialization strategies.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: wang24cf
month: 0
tex_title: Vision Transformers as Probabilistic Expansion from Learngene
firstpage: 52019
lastpage: 52032
page: 52019-52032
order: 52019
cycles: false
bibtex_author: Wang, Qiufeng and Yang, Xu and Chen, Haokun and Geng, Xin
author:
- given: Qiufeng
  family: Wang
- given: Xu
  family: Yang
- given: Haokun
  family: Chen
- given: Xin
  family: Geng
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://raw.githubusercontent.com/mlresearch/v235/main/assets/wang24cf/wang24cf.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
