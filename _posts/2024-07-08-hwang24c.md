---
title: Adapting Pretrained ViTs with Convolution Injector for Visuo-Motor Control
openreview: CuiRGtVI55
abstract: Vision Transformers (ViT), when paired with large-scale pretraining, have
  shown remarkable performance across various computer vision tasks, primarily due
  to their weak inductive bias. However, while such weak inductive bias aids in pretraining
  scalability, this may hinder the effective adaptation of ViTs for visuo-motor control
  tasks as a result of the absence of control-centric inductive biases. Such absent
  inductive biases include spatial locality and translation equivariance bias which
  convolutions naturally offer. To this end, we introduce Convolution Injector (CoIn),
  an add-on module that injects convolutions which are rich in locality and equivariance
  biases into a pretrained ViT for effective adaptation in visuo-motor control. We
  evaluate CoIn with three distinct types of pretrained ViTs (CLIP, MVP, VC-1) across
  12 varied control tasks within three separate domains (Adroit, MetaWorld, DMC),
  and demonstrate that CoIn consistently enhances control task performance across
  all experimented environments and models, validating the effectiveness of providing
  pretrained ViTs with control-centric biases.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: hwang24c
month: 0
tex_title: Adapting Pretrained {V}i{T}s with Convolution Injector for Visuo-Motor
  Control
firstpage: 20871
lastpage: 20888
page: 20871-20888
order: 20871
cycles: false
bibtex_author: Hwang, Dongyoon and Lee, Byungkun and Lee, Hojoon and Kim, Hyunseung
  and Choo, Jaegul
author:
- given: Dongyoon
  family: Hwang
- given: Byungkun
  family: Lee
- given: Hojoon
  family: Lee
- given: Hyunseung
  family: Kim
- given: Jaegul
  family: Choo
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://raw.githubusercontent.com/mlresearch/v235/main/assets/hwang24c/hwang24c.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
