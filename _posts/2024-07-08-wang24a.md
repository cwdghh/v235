---
title: Understanding Reasoning Ability of Language Models From the Perspective of
  Reasoning Paths Aggregation
openreview: dZsEOFUDew
abstract: 'Pre-trained language models (LMs) are able to perform complex reasoning
  without explicit fine-tuning. To understand how pre-training with a next-token prediction
  objective contributes to the emergence of such reasoning capability, we propose
  that we can view an LM as deriving new conclusions by aggregating indirect reasoning
  paths seen at pre-training time. We found this perspective effective in two important
  cases of reasoning: logic reasoning with knowledge graphs (KGs) and chain-of-thought
  (CoT) reasoning. More specifically, we formalize the reasoning paths as random walk
  paths on the knowledge/reasoning graphs. Analyses of learned LM distributions suggest
  that a weighted sum of relevant random walk path probabilities is a reasonable way
  to explain how LMs reason. Experiments and analysis on multiple KG and CoT datasets
  reveal the effect of training on random walk paths and suggest that augmenting unlabeled
  random walk reasoning paths can improve real-world multi-step reasoning performance.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: wang24a
month: 0
tex_title: Understanding Reasoning Ability of Language Models From the Perspective
  of Reasoning Paths Aggregation
firstpage: 50026
lastpage: 50042
page: 50026-50042
order: 50026
cycles: false
bibtex_author: Wang, Xinyi and Amayuelas, Alfonso and Zhang, Kexun and Pan, Liangming
  and Chen, Wenhu and Wang, William Yang
author:
- given: Xinyi
  family: Wang
- given: Alfonso
  family: Amayuelas
- given: Kexun
  family: Zhang
- given: Liangming
  family: Pan
- given: Wenhu
  family: Chen
- given: William Yang
  family: Wang
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/wang24a/wang24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
