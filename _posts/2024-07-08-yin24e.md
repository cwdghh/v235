---
title: 'Outlier Weighed Layerwise Sparsity (OWL): A Missing Secret Sauce for Pruning
  LLMs to High Sparsity'
openreview: ahEm3l2P6w
abstract: Large Language Models (LLMs), renowned for their remarkable performance
  across diverse domains, present a challenge due to their colossal model size when
  it comes to practical deployment. In response to this challenge, efforts have been
  directed toward the application of traditional network pruning techniques to LLMs,
  uncovering a massive number of parameters can be pruned in one-shot without hurting
  performance. Building upon insights gained from pre-LLM models, particularly BERT-level
  language models, prevailing LLM pruning strategies have consistently adhered to
  the practice of uniformly pruning all layers at equivalent sparsity levels, resulting
  in robust performance. However, this observation stands in contrast to the prevailing
  trends observed in the field of vision models, where non-uniform layerwise sparsity
  typically yields substantially improved results. To elucidate the underlying reasons
  for this disparity, we conduct a comprehensive analysis of the distribution of token
  features within LLMs. In doing so, we discover a strong correlation with the emergence
  of outliers, defined as features exhibiting significantly greater magnitudes compared
  to their counterparts in feature dimensions. Inspired by this finding, we introduce
  a novel LLM pruning methodology that incorporates a tailored set of <b>non-uniform
  layerwise sparsity ratios</b> specifically designed for LLM pruning, termed as <b>O</b>utlier
  <b>W</b>eighed <b>L</b>ayerwise sparsity (<b>OWL</b>). The sparsity ratio of OWL
  is directly proportional to the outlier ratio observed within each layer, facilitating
  a more effective alignment between layerwise weight sparsity and outlier ratios.
  Our empirical evaluation, conducted across the LLaMA-V1/V2, Vicuna, OPT, and Mistral,
  spanning various benchmarks, demonstrates the distinct advantages offered by OWL
  over previous methods. For instance, OWL exhibits a remarkable performance gain,
  surpassing the state-of-the-art Wanda and SparseGPT by <b>61.22</b> and <b>6.80</b>
  perplexity at a high sparsity level of 70%, respectively, while delivering <b>2.6$\times$</b>
  end-to-end inference speed-up in the DeepSparse inference engine. Code is available
  at https://github.com/luuyin/OWL.git.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: yin24e
month: 0
tex_title: 'Outlier Weighed Layerwise Sparsity ({OWL}): A Missing Secret Sauce for
  Pruning {LLM}s to High Sparsity'
firstpage: 57101
lastpage: 57115
page: 57101-57115
order: 57101
cycles: false
bibtex_author: Yin, Lu and Wu, You and Zhang, Zhenyu and Hsieh, Cheng-Yu and Wang,
  Yaqing and Jia, Yiling and Li, Gen and Jaiswal, Ajay Kumar and Pechenizkiy, Mykola
  and Liang, Yi and Bendersky, Michael and Wang, Zhangyang and Liu, Shiwei
author:
- given: Lu
  family: Yin
- given: You
  family: Wu
- given: Zhenyu
  family: Zhang
- given: Cheng-Yu
  family: Hsieh
- given: Yaqing
  family: Wang
- given: Yiling
  family: Jia
- given: Gen
  family: Li
- given: Ajay Kumar
  family: Jaiswal
- given: Mykola
  family: Pechenizkiy
- given: Yi
  family: Liang
- given: Michael
  family: Bendersky
- given: Zhangyang
  family: Wang
- given: Shiwei
  family: Liu
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://raw.githubusercontent.com/mlresearch/v235/main/assets/yin24e/yin24e.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
