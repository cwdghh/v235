---
title: Scaling Beyond the GPU Memory Limit for Large Mixture-of-Experts Model Training
openreview: uLpyWQPyF9
abstract: Mixture-of-Experts (MoE) is a powerful technique for enhancing the performance
  of neural networks while decoupling computational complexity from the number of
  parameters. However, despite this, scaling the number of experts requires adding
  more GPUs. In addition, the load imbalance in token load across experts causes unnecessary
  computation or straggler problems. We present ES-MoE, a novel method for efficient
  scaling MoE training. It offloads expert parameters to host memory and leverages
  pipelined expert processing to overlap GPU-CPU communication with GPU computation.
  It dynamically balances token loads across GPUs, improving computational efficiency.
  ES-MoE accelerates MoE training on a limited number of GPUs without degradation
  in model performance. We validate our approach on GPT-based MoE models, demonstrating
  67$\times$ better scalability and up to 17.5$\times$ better throughput over existing
  frameworks.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: kim24w
month: 0
tex_title: Scaling Beyond the {GPU} Memory Limit for Large Mixture-of-Experts Model
  Training
firstpage: 24342
lastpage: 24353
page: 24342-24353
order: 24342
cycles: false
bibtex_author: Kim, Yechan and Lim, Hwijoon and Han, Dongsu
author:
- given: Yechan
  family: Kim
- given: Hwijoon
  family: Lim
- given: Dongsu
  family: Han
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/assets/kim24w/kim24w.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
