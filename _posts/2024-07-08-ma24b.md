---
title: Rethinking Decision Transformer via Hierarchical Reinforcement Learning
openreview: WsM4TVsZpJ
abstract: Decision Transformer (DT) is an innovative algorithm leveraging recent advances
  of the transformer architecture in reinforcement learning (RL). However, a notable
  limitation of DT is its reliance on recalling trajectories from datasets, losing
  the capability to seamlessly stitch sub-optimal trajectories together. In this work
  we introduce a general sequence modeling framework for studying sequential decision
  making through the lens of <em>Hierarchical RL</em>. At the time of making decisions,
  a <em>high-level</em> policy first proposes an ideal <em>prompt</em> for the current
  state, a <em>low-level</em> policy subsequently generates an action conditioned
  on the given prompt. We show DT emerges as a special case of this framework with
  certain choices of high-level and low-level policies, and discuss the potential
  failure of these choices. Inspired by these observations, we study how to jointly
  optimize the high-level and low-level policies to enable the stitching ability,
  which further leads to the development of new offline RL algorithms. Our empirical
  results clearly show that the proposed algorithms significantly surpass DT on several
  control and navigation benchmarks. We hope our contributions can inspire the integration
  of transformer architectures within the field of RL.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: ma24b
month: 0
tex_title: Rethinking Decision Transformer via Hierarchical Reinforcement Learning
firstpage: 33730
lastpage: 33745
page: 33730-33745
order: 33730
cycles: false
bibtex_author: Ma, Yi and Hao, Jianye and Liang, Hebin and Xiao, Chenjun
author:
- given: Yi
  family: Ma
- given: Jianye
  family: Hao
- given: Hebin
  family: Liang
- given: Chenjun
  family: Xiao
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/assets/ma24b/ma24b.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
