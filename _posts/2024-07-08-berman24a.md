---
title: Sequential Disentanglement by Extracting Static Information From A Single Sequence
  Element
openreview: AocOA4h3bu
abstract: One of the fundamental representation learning tasks is unsupervised sequential
  disentanglement, where latent codes of inputs are decomposed to a single static
  factor and a sequence of dynamic factors. To extract this latent information, existing
  methods condition the static and dynamic codes on the entire input sequence. Unfortunately,
  these models often suffer from information leakage, i.e., the dynamic vectors encode
  both static and dynamic information, or vice versa, leading to a non-disentangled
  representation. Attempts to alleviate this problem via reducing the dynamic dimension
  and auxiliary loss terms gain only partial success. Instead, we propose a novel
  and simple architecture that mitigates information leakage by offering a simple
  and effective subtraction inductive bias while conditioning on a single sample.
  Remarkably, the resulting variational framework is simpler in terms of required
  loss terms, hyper-parameters, and data augmentation. We evaluate our method on multiple
  data-modality benchmarks including general time series, video, and audio, and we
  show beyond state-of-the-art results on generation and prediction tasks in comparison
  to several strong baselines.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: berman24a
month: 0
tex_title: Sequential Disentanglement by Extracting Static Information From A Single
  Sequence Element
firstpage: 3539
lastpage: 3564
page: 3539-3564
order: 3539
cycles: false
bibtex_author: Berman, Nimrod and Naiman, Ilan and Arbiv, Idan and Fadlon, Gal and
  Azencot, Omri
author:
- given: Nimrod
  family: Berman
- given: Ilan
  family: Naiman
- given: Idan
  family: Arbiv
- given: Gal
  family: Fadlon
- given: Omri
  family: Azencot
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://raw.githubusercontent.com/mlresearch/v235/main/assets/berman24a/berman24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
