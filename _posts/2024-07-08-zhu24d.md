---
title: Improving Open-Ended Text Generation via Adaptive Decoding
openreview: aXD94eATtT
abstract: Current language models decode text token by token according to probabilistic
  distribution, and determining the appropriate candidates for the next token is crucial
  to ensure generation quality. This study introduces adaptive decoding, a mechanism
  that dynamically empowers language models to ascertain a sensible candidate set
  during generation. Specifically, we introduce an entropy-based metric called confidence
  and conceptualize determining the optimal candidate set as a confidence-increasing
  process. The rationality of including a token in the candidate set is assessed by
  leveraging the increment of confidence. Experimental results reveal that our method
  balances diversity and coherence well. The human evaluation shows that our method
  can generate human-preferred text. Additionally, our method can potentially improve
  the reasoning ability of language models.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: zhu24d
month: 0
tex_title: Improving Open-Ended Text Generation via Adaptive Decoding
firstpage: 62386
lastpage: 62404
page: 62386-62404
order: 62386
cycles: false
bibtex_author: Zhu, Wenhong and Hao, Hongkun and He, Zhiwei and Ai, Yiming and Wang,
  Rui
author:
- given: Wenhong
  family: Zhu
- given: Hongkun
  family: Hao
- given: Zhiwei
  family: He
- given: Yiming
  family: Ai
- given: Rui
  family: Wang
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://raw.githubusercontent.com/mlresearch/v235/main/assets/zhu24d/zhu24d.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
