---
title: Improving Neural Additive Models with Bayesian Principles
openreview: 0pSTzCnEmi
abstract: Neural additive models (NAMs) enhance the transparency of deep neural networks
  by handling input features in separate additive sub-networks. However, they lack
  inherent mechanisms that provide calibrated uncertainties and enable selection of
  relevant features and interactions. Approaching NAMs from a Bayesian perspective,
  we augment them in three primary ways, namely by a) providing credible intervals
  for the individual additive sub-networks; b) estimating the marginal likelihood
  to perform an implicit selection of features via an empirical Bayes procedure; and
  c) facilitating the ranking of feature pairs as candidates for second-order interaction
  in fine-tuned models. In particular, we develop Laplace-approximated NAMs (LA-NAMs),
  which show improved empirical performance on tabular datasets and challenging real-world
  medical tasks.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: bouchiat24a
month: 0
tex_title: Improving Neural Additive Models with {B}ayesian Principles
firstpage: 4416
lastpage: 4443
page: 4416-4443
order: 4416
cycles: false
bibtex_author: Bouchiat, Kouroche and Immer, Alexander and Y\`{e}che, Hugo and Ratsch,
  Gunnar and Fortuin, Vincent
author:
- given: Kouroche
  family: Bouchiat
- given: Alexander
  family: Immer
- given: Hugo
  family: YÃ¨che
- given: Gunnar
  family: Ratsch
- given: Vincent
  family: Fortuin
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://raw.githubusercontent.com/mlresearch/v235/main/assets/bouchiat24a/bouchiat24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
