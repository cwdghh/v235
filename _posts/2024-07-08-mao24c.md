---
title: "$H$-Consistency Guarantees for Regression"
openreview: nvHlHfjJPe
abstract: We present a detailed study of $H$-consistency bounds for regression. We
  first present new theorems that generalize the tools previously given to establish
  $H$-consistency bounds. This generalization proves essential for analyzing $H$-consistency
  bounds specific to regression. Next, we prove a series of novel $H$-consistency
  bounds for surrogate loss functions of the squared loss, under the assumption of
  a symmetric distribution and a bounded hypothesis set. This includes positive results
  for the Huber loss, all $\ell_p$ losses, $p \geq 1$, the squared $\epsilon$-insensitive
  loss, as well as a negative result for the $\epsilon$-insensitive loss used in Support
  Vector Regression (SVR). We further leverage our analysis of $H$-consistency for
  regression and derive principled surrogate losses for adversarial regression (Section
  5). This readily establishes novel algorithms for adversarial regression, for which
  we report favorable experimental results in Section 6.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: mao24c
month: 0
tex_title: "$H$-Consistency Guarantees for Regression"
firstpage: 34712
lastpage: 34737
page: 34712-34737
order: 34712
cycles: false
bibtex_author: Mao, Anqi and Mohri, Mehryar and Zhong, Yutao
author:
- given: Anqi
  family: Mao
- given: Mehryar
  family: Mohri
- given: Yutao
  family: Zhong
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://raw.githubusercontent.com/mlresearch/v235/main/assets/mao24c/mao24c.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
