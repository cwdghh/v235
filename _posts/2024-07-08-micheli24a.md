---
title: Efficient World Models with Context-Aware Tokenization
openreview: BiWIERWBFX
abstract: Scaling up deep Reinforcement Learning (RL) methods presents a significant
  challenge. Following developments in generative modelling, model-based RL positions
  itself as a strong contender. Recent advances in sequence modelling have led to
  effective transformer-based world models, albeit at the price of heavy computations
  due to the long sequences of tokens required to accurately simulate environments.
  In this work, we propose $\Delta$-IRIS, a new agent with a world model architecture
  composed of a discrete autoencoder that encodes stochastic deltas between time steps
  and an autoregressive transformer that predicts future deltas by summarizing the
  current state of the world with continuous tokens. In the Crafter benchmark, $\Delta$-IRIS
  sets a new state of the art at multiple frame budgets, while being an order of magnitude
  faster to train than previous attention-based approaches. We release our code and
  models at https://github.com/vmicheli/delta-iris.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: micheli24a
month: 0
tex_title: Efficient World Models with Context-Aware Tokenization
firstpage: 35623
lastpage: 35638
page: 35623-35638
order: 35623
cycles: false
bibtex_author: Micheli, Vincent and Alonso, Eloi and Fleuret, Fran\c{c}ois
author:
- given: Vincent
  family: Micheli
- given: Eloi
  family: Alonso
- given: Fran√ßois
  family: Fleuret
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://raw.githubusercontent.com/mlresearch/v235/main/assets/micheli24a/micheli24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
