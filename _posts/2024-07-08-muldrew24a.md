---
title: Active Preference Learning for Large Language Models
openreview: CTgEV6qgUy
abstract: As large language models (LLMs) become more capable, fine-tuning techniques
  for aligning with human intent are increasingly important. A key consideration for
  aligning these models is how to most effectively use human resources, or model resources
  in the case where LLMs themselves are used as oracles. Reinforcement learning from
  Human or AI preferences (RLHF/RLAIF) is the most prominent example of such a technique,
  but is complex and often unstable. Direct Preference Optimization (DPO) has recently
  been proposed as a simpler and more stable alternative. In this work, we develop
  an active learning strategy for DPO to make better use of preference labels. We
  propose a practical acquisition function for prompt/completion pairs based on the
  predictive entropy of the language model and a measure of certainty of the implicit
  preference model optimized by DPO. We demonstrate how our approach improves both
  the rate of learning and final performance of fine-tuning on pairwise preference
  data.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: muldrew24a
month: 0
tex_title: Active Preference Learning for Large Language Models
firstpage: 36577
lastpage: 36590
page: 36577-36590
order: 36577
cycles: false
bibtex_author: Muldrew, William and Hayes, Peter and Zhang, Mingtian and Barber, David
author:
- given: William
  family: Muldrew
- given: Peter
  family: Hayes
- given: Mingtian
  family: Zhang
- given: David
  family: Barber
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/muldrew24a/muldrew24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
