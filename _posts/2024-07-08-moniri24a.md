---
title: A Theory of Non-Linear Feature Learning with One Gradient Step in Two-Layer
  Neural Networks
openreview: TWu1fzFJm0
abstract: Feature learning is thought to be one of the fundamental reasons for the
  success of deep neural networks. It is rigorously known that in two-layer fully-connected
  neural networks under certain conditions, one step of gradient descent on the first
  layer can lead to feature learning; characterized by the appearance of a separated
  rank-one component—spike—in the spectrum of the feature matrix. However, with a
  constant gradient descent step size, this spike only carries information from the
  linear component of the target function and therefore learning non-linear components
  is impossible. We show that with a learning rate that grows with the sample size,
  such training in fact introduces multiple rank-one components, each corresponding
  to a specific polynomial feature. We further prove that the limiting large-dimensional
  and large sample training and test errors of the updated neural networks are fully
  characterized by these spikes. By precisely analyzing the improvement in the training
  and test errors, we demonstrate that these non-linear features can enhance learning.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: moniri24a
month: 0
tex_title: A Theory of Non-Linear Feature Learning with One Gradient Step in Two-Layer
  Neural Networks
firstpage: 36106
lastpage: 36159
page: 36106-36159
order: 36106
cycles: false
bibtex_author: Moniri, Behrad and Lee, Donghwan and Hassani, Hamed and Dobriban, Edgar
author:
- given: Behrad
  family: Moniri
- given: Donghwan
  family: Lee
- given: Hamed
  family: Hassani
- given: Edgar
  family: Dobriban
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/moniri24a/moniri24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
