---
title: 'Junk DNA Hypothesis: Pruning Small Pre-Trained Weights $\textitIrreversibly$
  and $\textitMonotonically$ Impairs “Difficult" Downstream Tasks in LLMs'
openreview: EfUrTeuUfy
abstract: 'We present <em>Junk DNA Hypothesis</em> by adopting a novel <em>task-centric</em>
  angle for the pre-trained weights of large language models (LLMs). It has been believed
  that weights in LLMs contain significant redundancy, leading to the conception that
  a considerable chunk of the parameters can be removed by <em>pruning</em> without
  compromising performance. Contrary to this belief, this paper presents a <em>counter-argument</em>:
  small-magnitude weights of pre-trained model weights encode vital knowledge essential
  for tackling difficult downstream tasks - manifested as the <b>monotonic relationship</b>
  between the performance drop of downstream tasks across the difficulty spectrum,
  as we prune more pre-trained weights by magnitude. Moreover, we reveal that these
  seemingly inconsequential weights can result in <b>irreparable loss</b> of knowledge
  and performance degradation in difficult tasks, even when downstream continual training
  is allowed. Interestingly, our evaluations show that the other popular compression,
  namely <em>quantization</em> <b>fail</b> to exhibit similar “monotonic" effect and
  does not as convincingly disentangle this task-difficulty information. To study
  formally, we introduce several quantifiable metrics to <em>gauge the downstream
  task difficulty</em>: (a) within the same task category, and (b) across different
  task categories. Our extensive experiments substantiate the Junk DNA Hypothesis
  across a diverse range of model sizes, tasks, datasets, and even pruning methods.
  Codes are available at https://github.com/VITA-Group/Junk_DNA_Hypothesis.git.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: yin24b
month: 0
tex_title: 'Junk {DNA} Hypothesis: Pruning Small Pre-Trained Weights $\textit{Irreversibly}$
  and $\textit{Monotonically}$ Impairs “Difficult" Downstream Tasks in {LLM}s'
firstpage: 57053
lastpage: 57068
page: 57053-57068
order: 57053
cycles: false
bibtex_author: Yin, Lu and Jaiswal, Ajay Kumar and Liu, Shiwei and Kundu, Souvik and
  Wang, Zhangyang
author:
- given: Lu
  family: Yin
- given: Ajay Kumar
  family: Jaiswal
- given: Shiwei
  family: Liu
- given: Souvik
  family: Kundu
- given: Zhangyang
  family: Wang
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/assets/yin24b/yin24b.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
