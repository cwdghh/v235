---
title: Data Engineering for Scaling Language Models to 128K Context
openreview: TaAqeo7lUh
abstract: 'We study continual pretraining recipe for scaling language models’ context
  lengths to 128K, with a focus on data engineering. We hypothesize that long context
  modeling, in particular <em>the ability to utilize information at arbitrary input
  locations</em>, is a capability that is mostly already acquired through large-scale
  pretraining, and that this capability can be readily extended to contexts substantially
  longer than seen during training (e.g., 4K to 128K) through lightweight continual
  pretraining on appropriate data mixture. We investigate the <em>quantity</em> and
  <em>quality</em> of the data for continual pretraining: (1) for quantity, we show
  that 500 million to 5 billion tokens are enough to enable the model to retrieve
  information anywhere within the 128K context; (2) for quality, our results equally
  emphasize <em>domain balance</em> and <em>length upsampling</em>. Concretely, naïvely
  upsampling longer data on certain domains like books, a common practice of existing
  work, gives suboptimal performance; a balanced domain mixture is equally important.
  We demonstrate that continual pretraining of the full model on 1B-5B tokens of such
  data is an effective and affordable strategy for scaling the context length of language
  models to 128K. Our recipe outperforms strong open-source long-context models and
  closes the gap to frontier models like GPT-4 128K.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: fu24d
month: 0
tex_title: Data Engineering for Scaling Language Models to 128{K} Context
firstpage: 14125
lastpage: 14134
page: 14125-14134
order: 14125
cycles: false
bibtex_author: Fu, Yao and Panda, Rameswar and Niu, Xinyao and Yue, Xiang and Hajishirzi,
  Hannaneh and Kim, Yoon and Peng, Hao
author:
- given: Yao
  family: Fu
- given: Rameswar
  family: Panda
- given: Xinyao
  family: Niu
- given: Xiang
  family: Yue
- given: Hannaneh
  family: Hajishirzi
- given: Yoon
  family: Kim
- given: Hao
  family: Peng
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/assets/fu24d/fu24d.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
