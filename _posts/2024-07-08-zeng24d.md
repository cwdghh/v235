---
title: Learning Reward for Robot Skills Using Large Language Models via Self-Alignment
openreview: Z19JQ6WFtJ
abstract: 'Learning reward functions remains the bottleneck to equip a robot with
  a broad repertoire of skills. Large Language Models (LLM) contain valuable task-related
  knowledge that can potentially aid in the learning of reward functions. However,
  the proposed reward function can be imprecise, thus ineffective which requires to
  be further grounded with environment information. We proposed a method to learn
  rewards more efficiently in the absence of humans. Our approach consists of two
  components: We first use the LLM to propose features and parameterization of the
  reward, then update the parameters through an iterative self-alignment process.
  In particular, the process minimizes the ranking inconsistency between the LLM and
  the learnt reward functions based on the execution feedback. The method was validated
  on 9 tasks across 2 simulation environments. It demonstrates a consistent improvement
  in training efficacy and efficiency, meanwhile consuming significantly fewer GPT
  tokens compared to the alternative mutation-based method.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: zeng24d
month: 0
tex_title: Learning Reward for Robot Skills Using Large Language Models via Self-Alignment
firstpage: 58366
lastpage: 58386
page: 58366-58386
order: 58366
cycles: false
bibtex_author: Zeng, Yuwei and Mu, Yao and Shao, Lin
author:
- given: Yuwei
  family: Zeng
- given: Yao
  family: Mu
- given: Lin
  family: Shao
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/zeng24d/zeng24d.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
