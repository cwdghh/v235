---
title: A Closer Look at the Limitations of Instruction Tuning
openreview: XkHJo8iXGQ
abstract: Instruction Tuning (IT), the process of training large language models (LLMs)
  using instruction-response pairs, has emerged as the predominant method for transforming
  base pre-trained LLMs into open-domain conversational agents. While IT has achieved
  notable success and widespread adoption, its limitations and shortcomings remain
  underexplored. In this paper, through rigorous experiments and an in-depth analysis
  of the changes LLMs undergo through IT, we reveal various limitations of IT. In
  particular, we show that (1) IT fails to enhance knowledge or skills in LLMs. LoRA
  fine-tuning is limited to learning response initiation and style tokens, and full-parameter
  fine-tuning leads to knowledge degradation. (2) Copying response patterns from IT
  datasets derived from knowledgeable sources leads to a decline in response quality.
  (3) Full-parameter fine-tuning increases hallucination by inaccurately borrowing
  tokens from conceptually similar instances in the IT dataset for generating responses.
  (4) Popular methods to improve IT do not lead to performance improvements over a
  simple LoRA fine-tuned model. Our findings reveal that responses generated solely
  from pre-trained knowledge consistently outperform responses by models that learn
  any form of new knowledge from IT on open-source datasets. We hope the insights
  and challenges revealed in this paper inspire future work in related directions.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: ghosh24a
month: 0
tex_title: A Closer Look at the Limitations of Instruction Tuning
firstpage: 15559
lastpage: 15589
page: 15559-15589
order: 15559
cycles: false
bibtex_author: Ghosh, Sreyan and Evuru, Chandra Kiran Reddy and Kumar, Sonal and S,
  Ramaneswaran and Aneja, Deepali and Jin, Zeyu and Duraiswami, Ramani and Manocha,
  Dinesh
author:
- given: Sreyan
  family: Ghosh
- given: Chandra Kiran Reddy
  family: Evuru
- given: Sonal
  family: Kumar
- given: Ramaneswaran
  family: S
- given: Deepali
  family: Aneja
- given: Zeyu
  family: Jin
- given: Ramani
  family: Duraiswami
- given: Dinesh
  family: Manocha
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/assets/ghosh24a/ghosh24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
