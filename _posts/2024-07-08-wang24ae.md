---
title: In-context Learning on Function Classes Unveiled for Transformers
openreview: rJkGOARXns
abstract: 'Transformer-based neural sequence models exhibit a remarkable ability to
  perform in-context learning. Given some training examples, a pre-trained model can
  make accurate predictions on an unseen input. This paper studies why transformers
  can learn different types of function classes in-context. We first show by construction
  that there exists a family of transformers (with different activation functions)
  that implement approximate gradient descent on the parameters of neural networks,
  and we provide an upper bound for the number of heads, hidden dimensions, and layers
  of the transformer. We also show that a transformer can learn linear functions,
  the indicator function of a unit ball, and smooth functions in-context by learning
  neural networks that approximate them. The above instances mainly focus on a transformer
  pre-trained on single tasks. We also prove that when pre-trained on two tasks: linear
  regression and classification, a transformer can make accurate predictions on both
  tasks simultaneously. Our results move beyond linearity in terms of in-context learning
  instances and provide a comprehensive understanding of why transformers can learn
  many types of function classes through the bridge of neural networks.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: wang24ae
month: 0
tex_title: In-context Learning on Function Classes Unveiled for Transformers
firstpage: 50726
lastpage: 50745
page: 50726-50745
order: 50726
cycles: false
bibtex_author: Wang, Zhijie and Jiang, Bo and Li, Shuai
author:
- given: Zhijie
  family: Wang
- given: Bo
  family: Jiang
- given: Shuai
  family: Li
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://raw.githubusercontent.com/mlresearch/v235/main/assets/wang24ae/wang24ae.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
