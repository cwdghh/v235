---
title: GNNs Also Deserve Editing, and They Need It More Than Once
openreview: rIc9adYbH2
abstract: Suppose a self-driving car is crashing into pedestrians, or a chatbot is
  instructing its users to conduct criminal wrongdoing; the stakeholders of such products
  will undoubtedly want to patch these catastrophic errors as soon as possible. To
  address such concerns, <em>Model Editing:</em> the study of efficiently patching
  model behaviors without significantly altering their general performance, has seen
  considerable activity, with hundreds of editing techniques developed in various
  domains such as CV and NLP. However, <b>the graph learning community has objectively
  fallen behind with only a few Graph Neural Network-compatible — and just one GNN-specific
  — model editing methods available</b>, where all of which are limited in their practical
  scope. We argue that the impracticality of these methods lies in their lack of <em>Sequential
  Editing Robustness:</em> the ability to edit multiple errors sequentially, and therefore
  fall short in effectiveness, as this approach mirrors how errors are discovered
  and addressed in the real world. In this paper, we delve into the specific reasons
  behind the difficulty of editing GNNs in succession and observe the root cause to
  be model overfitting. We subsequently propose a simple yet effective solution —
  SEED-GNN — by leveraging overfit-prevention techniques in a GNN-specific context
  to derive the first and only GNN model editing method that scales practically. Additionally,
  we formally frame the task paradigm of GNN editing and hope to inspire future research
  in this crucial but currently overlooked field. Please refer to our GitHub repository
  for code and checkpoints.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: zhong24d
month: 0
tex_title: "{GNN}s Also Deserve Editing, and They Need It More Than Once"
firstpage: 61727
lastpage: 61746
page: 61727-61746
order: 61727
cycles: false
bibtex_author: Zhong, Shaochen and Le, Duy and Liu, Zirui and Jiang, Zhimeng and Ye,
  Andrew and Zhang, Jiamu and Yuan, Jiayi and Zhou, Kaixiong and Xu, Zhaozhuo and
  Ma, Jing and Xu, Shuai and Chaudhary, Vipin and Hu, Xia
author:
- given: Shaochen
  family: Zhong
- given: Duy
  family: Le
- given: Zirui
  family: Liu
- given: Zhimeng
  family: Jiang
- given: Andrew
  family: Ye
- given: Jiamu
  family: Zhang
- given: Jiayi
  family: Yuan
- given: Kaixiong
  family: Zhou
- given: Zhaozhuo
  family: Xu
- given: Jing
  family: Ma
- given: Shuai
  family: Xu
- given: Vipin
  family: Chaudhary
- given: Xia
  family: Hu
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://raw.githubusercontent.com/mlresearch/v235/main/assets/zhong24d/zhong24d.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
