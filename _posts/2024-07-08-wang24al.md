---
title: An Iterative Min-Min Optimization Method for Sparse Bayesian Learning
openreview: 69RewQwWA9
abstract: As a well-known machine learning algorithm, sparse Bayesian learning (SBL)
  can find sparse representations in linearly probabilistic models by imposing a sparsity-promoting
  prior on model coefficients. However, classical SBL algorithms lack the essential
  theoretical guarantees of global convergence. To address this issue, we propose
  an iterative Min-Min optimization method to solve the marginal likelihood function
  (MLF) of SBL based on the concave-convex procedure. The method can optimize the
  hyperparameters related to both the prior and noise level analytically at each iteration
  by re-expressing MLF using auxiliary functions. Particularly, we demonstrate that
  the method globally converges to a local minimum or saddle point of MLF. With rigorous
  theoretical guarantees, the proposed novel SBL algorithm outperforms classical ones
  in finding sparse representations on simulation and real-world examples, ranging
  from sparse signal recovery to system identification and kernel regression.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: wang24al
month: 0
tex_title: An Iterative Min-Min Optimization Method for Sparse {B}ayesian Learning
firstpage: 50859
lastpage: 50873
page: 50859-50873
order: 50859
cycles: false
bibtex_author: Wang, Yasen and Li, Junlin and Yue, Zuogong and Yuan, Ye
author:
- given: Yasen
  family: Wang
- given: Junlin
  family: Li
- given: Zuogong
  family: Yue
- given: Ye
  family: Yuan
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/assets/wang24al/wang24al.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
