---
title: 'Position: Do pretrained Transformers Learn In-Context by Gradient Descent?'
openreview: WsawczEqO6
abstract: The emergence of In-Context Learning (ICL) in LLMs remains a remarkable
  phenomenon that is partially understood. To explain ICL, recent studies have created
  theoretical connections to Gradient Descent (GD). We ask, do such connections hold
  up in actual pre-trained language models? We highlight the limiting assumptions
  in prior works that make their setup considerably different from the practical setup
  in which language models are trained. For example, their experimental verification
  uses <em>ICL objective</em> (training models explicitly for ICL), which differs
  from the emergent ICL in the wild. Furthermore, the theoretical hand-constructed
  weights used in these studies have properties that donâ€™t match those of real LLMs.
  We also look for evidence in real models. We observe that ICL and GD have different
  sensitivity to the order in which they observe demonstrations. Finally, we probe
  and compare the ICL vs. GD hypothesis in a natural setting. We conduct comprehensive
  empirical analyses on language models pre-trained on natural data (LLaMa-7B). Our
  comparisons of three performance metrics highlight the inconsistent behavior of
  ICL and GD as a function of various factors such as datasets, models, and the number
  of demonstrations. We observe that ICL and GD modify the output distribution of
  language models differently. These results indicate that <em>the equivalence between
  ICL and GD remains an open hypothesis</em> and calls for further studies.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: shen24d
month: 0
tex_title: 'Position: Do pretrained Transformers Learn In-Context by Gradient Descent?'
firstpage: 44712
lastpage: 44740
page: 44712-44740
order: 44712
cycles: false
bibtex_author: Shen, Lingfeng and Mishra, Aayush and Khashabi, Daniel
author:
- given: Lingfeng
  family: Shen
- given: Aayush
  family: Mishra
- given: Daniel
  family: Khashabi
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/shen24d/shen24d.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
