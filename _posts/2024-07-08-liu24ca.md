---
title: Efficient Policy Evaluation with Offline Data Informed Behavior Policy Design
openreview: kpDd2HCBka
abstract: Most reinforcement learning practitioners evaluate their policies with online
  Monte Carlo estimators for either hyperparameter tuning or testing different algorithmic
  design choices, where the policy is repeatedly executed in the environment to get
  the average outcome. Such massive interactions with the environment are prohibitive
  in many scenarios. In this paper, we propose novel methods that improve the data
  efficiency of online Monte Carlo estimators while maintaining their unbiasedness.
  We first propose a tailored closed-form behavior policy that provably reduces the
  variance of an online Monte Carlo estimator. We then design efficient algorithms
  to learn this closed-form behavior policy from previously collected offline data.
  Theoretical analysis is provided to characterize how the behavior policy learning
  error affects the amount of reduced variance. Compared with previous works, our
  method achieves better empirical performance in a broader set of environments, with
  fewer requirements for offline data.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: liu24ca
month: 0
tex_title: Efficient Policy Evaluation with Offline Data Informed Behavior Policy
  Design
firstpage: 32345
lastpage: 32368
page: 32345-32368
order: 32345
cycles: false
bibtex_author: Liu, Shuze and Zhang, Shangtong
author:
- given: Shuze
  family: Liu
- given: Shangtong
  family: Zhang
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/liu24ca/liu24ca.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
