---
title: Dense Reward for Free in Reinforcement Learning from Human Feedback
openreview: eyxVRMrZ4m
abstract: Reinforcement Learning from Human Feedback (RLHF) has been credited as the
  key advance that has allowed Large Language Models (LLMs) to effectively follow
  instructions and produce useful assistance. Classically, this involves generating
  completions from the LLM in response to a query before using a separate reward model
  to assign a score to the full completion. As an auto-regressive process, the LLM
  has to take many “actions” (selecting individual tokens) and only receives a single,
  sparse reward at the end of an episode, a setup that is known to be difficult to
  optimise in traditional reinforcement learning. In this work we leverage the fact
  that the reward model contains more information than just its scalar output, in
  particular, it calculates an attention map over tokens as part of the transformer
  architecture. We use these attention weights to redistribute the reward along the
  whole completion, effectively densifying the signal and highlighting the most important
  tokens, all without incurring extra computational cost or requiring any additional
  modelling. We demonstrate that, theoretically, this approach is equivalent to potential-based
  reward shaping, ensuring that the optimal policy remains unchanged. Empirically,
  we show that it stabilises training, accelerates the rate of learning, and, in practical
  cases, may lead to better local optima.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: chan24a
month: 0
tex_title: Dense Reward for Free in Reinforcement Learning from Human Feedback
firstpage: 6136
lastpage: 6154
page: 6136-6154
order: 6136
cycles: false
bibtex_author: Chan, Alex James and Sun, Hao and Holt, Samuel and Van Der Schaar,
  Mihaela
author:
- given: Alex James
  family: Chan
- given: Hao
  family: Sun
- given: Samuel
  family: Holt
- given: Mihaela
  family: Van Der Schaar
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/chan24a/chan24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
