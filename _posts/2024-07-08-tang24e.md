---
title: Merging Multi-Task Models via Weight-Ensembling Mixture of Experts
openreview: nLRKnO74RB
abstract: Merging various task-specific Transformer-based vision models trained on
  different tasks into a single unified model can execute all the tasks concurrently.
  Previous methods, exemplified by task arithmetic, have been proven to be both effective
  and scalable. Existing methods have primarily focused on seeking a static optimal
  solution within the original model parameter space. A notable challenge is mitigating
  the interference between parameters of different models, which can substantially
  deteriorate performance. In this paper, we propose to merge most of the parameters
  while upscaling the MLP of the Transformer layers to a weight-ensembling mixture
  of experts (MoE) module, which can dynamically integrate shared and task-specific
  knowledge based on the input, thereby providing a more flexible solution that can
  adapt to the specific needs of each instance. Our key insight is that by identifying
  and separating shared knowledge and task-specific knowledge, and then dynamically
  integrating them, we can mitigate the parameter interference problem to a great
  extent. We conduct the conventional multi-task model merging experiments and evaluate
  the generalization and robustness of our method. The results demonstrate the effectiveness
  of our method and provide a comprehensive understanding of our method. The code
  is available at https://github.com/tanganke/weight-ensembling_MoE
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: tang24e
month: 0
tex_title: Merging Multi-Task Models via Weight-Ensembling Mixture of Experts
firstpage: 47778
lastpage: 47799
page: 47778-47799
order: 47778
cycles: false
bibtex_author: Tang, Anke and Shen, Li and Luo, Yong and Yin, Nan and Zhang, Lefei
  and Tao, Dacheng
author:
- given: Anke
  family: Tang
- given: Li
  family: Shen
- given: Yong
  family: Luo
- given: Nan
  family: Yin
- given: Lefei
  family: Zhang
- given: Dacheng
  family: Tao
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/tang24e/tang24e.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
