---
title: Understanding the Training Speedup from Sampling with Approximate Losses
openreview: uun4fzaiat
abstract: It is well known that selecting samples with large losses/gradients can
  significantly reduce the number of training steps. However, the selection overhead
  is often too high to yield any meaningful gains in terms of overall training time.
  In this work, we focus on the greedy approach of selecting samples with large <em>approximate
  losses</em> instead of exact losses in order to reduce the selection overhead. For
  smooth convex losses, we show that such a greedy strategy can converge to a constant
  factor of the minimum value of the average loss in fewer iterations than the standard
  approach of random selection. We also theoretically quantify the effect of the approximation
  level. We then develop SIFT which uses early exiting to obtain approximate losses
  with an intermediate layerâ€™s representations for sample selection. We evaluate SIFT
  on the task of training a 110M parameter 12 layer BERT base model, and show significant
  gains (in terms of training hours and number of backpropagation steps) without any
  optimized implementation over vanilla training. For e.g., to reach 64% validation
  accuracy, SIFT with exit at the first layer takes $\sim$ 43 hours compared to $\sim$
  57 hours of vanilla training.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: das24b
month: 0
tex_title: Understanding the Training Speedup from Sampling with Approximate Losses
firstpage: 10127
lastpage: 10147
page: 10127-10147
order: 10127
cycles: false
bibtex_author: Das, Rudrajit and Chen, Xi and Ieong, Bertram and Bansal, Parikshit
  and Sanghavi, Sujay
author:
- given: Rudrajit
  family: Das
- given: Xi
  family: Chen
- given: Bertram
  family: Ieong
- given: Parikshit
  family: Bansal
- given: Sujay
  family: Sanghavi
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/assets/das24b/das24b.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
