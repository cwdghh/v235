---
title: Learning Graph Representation via Graph Entropy Maximization
openreview: xwOENWCo46
abstract: Graph representation learning aims to represent graphs as vectors that can
  be utilized in downstream tasks such as graph classification. In this work, we focus
  on learning diverse representations that can capture the graph information as much
  as possible. We propose quantifying graph information using graph entropy, where
  we define a probability distribution of a graph based on its nodesâ€™ representations
  and global-graph representation. However, the computation of graph entropy is NP-hard
  due to the complex vertex-packing polytope involved in its definition. To address
  this challenge, we provide an approximation method leveraging orthonormal representations
  for graph entropy maximization. The proposed method is implemented via graph neural
  networks, resulting in informative node-level and graph-level representations. Experimental
  results demonstrate the effectiveness of our method in comparison to many baselines
  in unsupervised learning and semi-supervised learning tasks. The code of our method
  is available at https://github.com/MathAdventurer/GeMax.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: sun24i
month: 0
tex_title: Learning Graph Representation via Graph Entropy Maximization
firstpage: 47133
lastpage: 47158
page: 47133-47158
order: 47133
cycles: false
bibtex_author: Sun, Ziheng and Wang, Xudong and Ding, Chris and Fan, Jicong
author:
- given: Ziheng
  family: Sun
- given: Xudong
  family: Wang
- given: Chris
  family: Ding
- given: Jicong
  family: Fan
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/sun24i/sun24i.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
