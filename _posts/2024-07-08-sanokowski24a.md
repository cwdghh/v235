---
title: A Diffusion Model Framework for Unsupervised Neural Combinatorial Optimization
openreview: AFfXlKFHXJ
abstract: Learning to sample from intractable distributions over discrete sets without
  relying on corresponding training data is a central problem in a wide range of fields,
  including Combinatorial Optimization. Currently, popular deep learning-based approaches
  rely primarily on generative models that yield exact sample likelihoods. This work
  introduces a method that lifts this restriction and opens the possibility to employ
  highly expressive latent variable models like diffusion models. Our approach is
  conceptually based on a loss that upper bounds the reverse Kullback-Leibler divergence
  and evades the requirement of exact sample likelihoods. We experimentally validate
  our approach in data-free Combinatorial Optimization and demonstrate that our method
  achieves a new state-of-the-art on a wide range of benchmark problems.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: sanokowski24a
month: 0
tex_title: A Diffusion Model Framework for Unsupervised Neural Combinatorial Optimization
firstpage: 43346
lastpage: 43367
page: 43346-43367
order: 43346
cycles: false
bibtex_author: Sanokowski, Sebastian and Hochreiter, Sepp and Lehner, Sebastian
author:
- given: Sebastian
  family: Sanokowski
- given: Sepp
  family: Hochreiter
- given: Sebastian
  family: Lehner
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://raw.githubusercontent.com/mlresearch/v235/main/assets/sanokowski24a/sanokowski24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
