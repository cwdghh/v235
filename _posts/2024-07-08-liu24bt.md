---
title: Correlation-Induced Label Prior for Semi-Supervised Multi-Label Learning
openreview: IuvpVcGUOB
abstract: Semi-supervised multi-label learning (SSMLL) aims to address the challenge
  of limited labeled data availability in multi-label learning (MLL) by leveraging
  unlabeled data to improve the modelâ€™s performance. Due to the difficulty of estimating
  the reliable label correlation on minimal multi-labeled data, previous SSMLL methods
  fail to unlash the power of the correlation among multiple labels to improve the
  performance of the predictive model in SSMLL. To deal with this problem, we propose
  a novel SSMLL method named PCLP where the correlation-induced label prior is inferred
  to enhance the pseudo-labeling instead of dirtily estimating the correlation among
  labels. Specifically, we construct the correlated label prior probability distribution
  using structural causal model (SCM), constraining the correlations of generated
  pseudo-labels to conform to the prior, which can be integrated into a variational
  label enhancement framework optimized by both labeled and unlabeled instances in
  a unified manner. Theoretically, we demonstrate the accuracy of the generated pseudo-labels
  and guarantee the learning consistency of the proposed method. Comprehensive experiments
  on several benchmark datasets have validated the superiority of the proposed method.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: liu24bt
month: 0
tex_title: Correlation-Induced Label Prior for Semi-Supervised Multi-Label Learning
firstpage: 32224
lastpage: 32238
page: 32224-32238
order: 32224
cycles: false
bibtex_author: Liu, Biao and Xu, Ning and Fang, Xiangyu and Geng, Xin
author:
- given: Biao
  family: Liu
- given: Ning
  family: Xu
- given: Xiangyu
  family: Fang
- given: Xin
  family: Geng
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/liu24bt/liu24bt.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
