---
title: 'Towards Efficient Spiking Transformer: a Token Sparsification Framework for
  Training and Inference Acceleration'
openreview: yL6hljtjW4
abstract: Nowadays Spiking Transformers have exhibited remarkable performance close
  to Artificial Neural Networks (ANNs), while enjoying the inherent energy-efficiency
  of Spiking Neural Networks (SNNs). However, training Spiking Transformers on GPUs
  is considerably more time-consuming compared to the ANN counterparts, despite the
  energy-efficient inference through neuromorphic computation. In this paper, we investigate
  the token sparsification technique for efficient training of Spiking Transformer
  and find conventional methods suffer from noticeable performance degradation. We
  analyze the issue and propose our Sparsification with Timestep-wise Anchor Token
  and dual Alignments (STATA). Timestep-wise Anchor Token enables precise identification
  of important tokens across timesteps based on standardized criteria. Additionally,
  dual Alignments incorporate both Intra and Inter Alignment of the attention maps,
  fostering the learning of inferior attention. Extensive experiments show the effectiveness
  of STATA thoroughly, which demonstrates up to $\sim$1.53$\times$ training speedup
  and $\sim$48% energy reduction with comparable performance on various datasets and
  architectures.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: zhuge24b
month: 0
tex_title: 'Towards Efficient Spiking Transformer: a Token Sparsification Framework
  for Training and Inference Acceleration'
firstpage: 62768
lastpage: 62778
page: 62768-62778
order: 62768
cycles: false
bibtex_author: Zhuge, Zhengyang and Wang, Peisong and Yao, Xingting and Cheng, Jian
author:
- given: Zhengyang
  family: Zhuge
- given: Peisong
  family: Wang
- given: Xingting
  family: Yao
- given: Jian
  family: Cheng
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://raw.githubusercontent.com/mlresearch/v235/main/assets/zhuge24b/zhuge24b.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
