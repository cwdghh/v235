---
title: 'Et Tu Certifications: Robustness Certificates Yield Better Adversarial Examples'
openreview: RKlmOBFwAh
abstract: In guaranteeing the absence of adversarial examples in an instanceâ€™s neighbourhood,
  certification mechanisms play an important role in demonstrating neural net robustness.
  In this paper, we ask if these certifications can compromise the very models they
  help to protect? Our new <em>Certification Aware Attack</em> exploits certifications
  to produce computationally efficient norm-minimising adversarial examples $74$%
  more often than comparable attacks, while reducing the median perturbation norm
  by more than $10$%. While these attacks can be used to assess the tightness of certification
  bounds, they also highlight that releasing certifications can paradoxically reduce
  security.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: cullen24a
month: 0
tex_title: 'Et Tu Certifications: Robustness Certificates Yield Better Adversarial
  Examples'
firstpage: 9745
lastpage: 9761
page: 9745-9761
order: 9745
cycles: false
bibtex_author: Cullen, Andrew Craig and Liu, Shijie and Montague, Paul and Erfani,
  Sarah Monazam and Rubinstein, Benjamin I. P.
author:
- given: Andrew Craig
  family: Cullen
- given: Shijie
  family: Liu
- given: Paul
  family: Montague
- given: Sarah Monazam
  family: Erfani
- given: Benjamin I. P.
  family: Rubinstein
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/assets/cullen24a/cullen24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
