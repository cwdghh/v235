---
title: Convex Relaxations of ReLU Neural Networks Approximate Global Optima in Polynomial
  Time
openreview: Zn44XGFGam
abstract: In this paper, we study the optimality gap between two-layer ReLU networks
  regularized with weight decay and their convex relaxations. We show that when the
  training data is random, the relative optimality gap between the original problem
  and its relaxation can be bounded by a factor of O(âˆšlog n), where n is the number
  of training samples. A simple application leads to a tractable polynomial-time algorithm
  that is guaranteed to solve the original non-convex problem up to a logarithmic
  factor. Moreover, under mild assumptions, we show that local gradient methods converge
  to a point with low training loss with high probability. Our result is an exponential
  improvement compared to existing results and sheds new light on understanding why
  local gradient methods work well.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: kim24ac
month: 0
tex_title: Convex Relaxations of {R}e{LU} Neural Networks Approximate Global Optima
  in Polynomial Time
firstpage: 24458
lastpage: 24485
page: 24458-24485
order: 24458
cycles: false
bibtex_author: Kim, Sungyoon and Pilanci, Mert
author:
- given: Sungyoon
  family: Kim
- given: Mert
  family: Pilanci
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://raw.githubusercontent.com/mlresearch/v235/main/assets/kim24ac/kim24ac.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
