---
title: An Empirical Study Into What Matters for Calibrating Vision-Language Models
openreview: qoxuPshrZb
abstract: Vision-Language Models (VLMs) have emerged as the dominant approach for
  zero-shot recognition, adept at handling diverse scenarios and significant distribution
  changes. However, their deployment in risk-sensitive areas requires a deeper understanding
  of their uncertainty estimation capabilities, a relatively uncharted area. In this
  study, we explore the calibration properties of VLMs across different architectures,
  datasets, and training strategies. In particular, we analyze the uncertainty estimation
  performance of VLMs when calibrated in one domain, label set or hierarchy level,
  and tested in a different one. Our findings reveal that while VLMs are not inherently
  calibrated for uncertainty, temperature scaling significantly and consistently improves
  calibration, even across shifts in distribution and changes in label set. Moreover,
  VLMs can be calibrated with a very small set of examples. Through detailed experimentation,
  we highlight the potential applications and importance of our insights, aiming for
  more reliable and effective use of VLMs in critical, real-world scenarios.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: tu24a
month: 0
tex_title: An Empirical Study Into What Matters for Calibrating Vision-Language Models
firstpage: 48791
lastpage: 48808
page: 48791-48808
order: 48791
cycles: false
bibtex_author: Tu, Weijie and Deng, Weijian and Campbell, Dylan and Gould, Stephen
  and Gedeon, Tom
author:
- given: Weijie
  family: Tu
- given: Weijian
  family: Deng
- given: Dylan
  family: Campbell
- given: Stephen
  family: Gould
- given: Tom
  family: Gedeon
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://raw.githubusercontent.com/mlresearch/v235/main/assets/tu24a/tu24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
