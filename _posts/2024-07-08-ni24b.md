---
title: On the Nonlinearity of Layer Normalization
openreview: 18f6iPn0zq
abstract: Layer normalization (LN) is a ubiquitous technique in deep learning but
  our theoretical understanding to it remains elusive. This paper investigates a new
  theoretical direction for LN, regarding to its nonlinearity and representation capacity.
  We investigate the representation capacity of a network with layerwise composition
  of linear and LN transformations, referred to as LN-Net. We theoretically show that,
  given $m$ samples with any label assignment, an LN-Net with only 3 neurons in each
  layer and $O(m)$ LN layers can correctly classify them. We further show the lower
  bound of the VC dimension of an LN-Net. The nonlinearity of LN can be amplified
  by group partition, which is also theoretically demonstrated with mild assumption
  and empirically supported by our experiments. Based on our analyses, we consider
  to design neural architecture by exploiting and amplifying the nonlinearity of LN,
  and the effectiveness is supported by our experiments.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: ni24b
month: 0
tex_title: On the Nonlinearity of Layer Normalization
firstpage: 37957
lastpage: 37998
page: 37957-37998
order: 37957
cycles: false
bibtex_author: Ni, Yunhao and Guo, Yuxin and Jia, Junlong and Huang, Lei
author:
- given: Yunhao
  family: Ni
- given: Yuxin
  family: Guo
- given: Junlong
  family: Jia
- given: Lei
  family: Huang
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/ni24b/ni24b.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
