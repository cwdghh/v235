---
title: 'Transformers Get Stable: An End-to-End Signal Propagation Theory for Language
  Models'
openreview: 30waYPIZUA
abstract: In spite of their huge success, transformer models remain difficult to scale
  in depth. In this work, we develop a unified signal propagation theory and provide
  formulae that govern the moments of the forward and backward signal through the
  transformer model. Our framework can be used to understand and mitigate vanishing/exploding
  gradients, rank collapse, and instability associated with high attention scores.
  We also propose DeepScaleLM, an initialization and scaling scheme that conserves
  unit output/gradient moments throughout the model, enabling the training of very
  deep models with 1000 layers. We find that transformer models could be much deeper
  - our deep models with fewer parameters outperform shallow models in Language Modeling,
  Speech Translation, and Image Classification, across encoder-only, decoder-only
  and encoder-decoder variants, for both Pre-LN and Post-LN transformers, for multiple
  datasets and model sizes. These improvements also translate into improved performance
  on downstream Question Answering tasks and improved robustness for Image Classification.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: kedia24a
month: 0
tex_title: 'Transformers Get Stable: An End-to-End Signal Propagation Theory for Language
  Models'
firstpage: 23449
lastpage: 23531
page: 23449-23531
order: 23449
cycles: false
bibtex_author: Kedia, Akhil and Zaidi, Mohd Abbas and Khyalia, Sushil and Jung, Jungho
  and Goka, Harshith and Lee, Haejun
author:
- given: Akhil
  family: Kedia
- given: Mohd Abbas
  family: Zaidi
- given: Sushil
  family: Khyalia
- given: Jungho
  family: Jung
- given: Harshith
  family: Goka
- given: Haejun
  family: Lee
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/kedia24a/kedia24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
