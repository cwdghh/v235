---
title: Data-efficient Large Vision Models through Sequential Autoregression
openreview: KmCoS6WkgG
abstract: Training general-purpose vision models on purely sequential visual data,
  eschewing linguistic inputs, has heralded a new frontier in visual understanding.
  These models are intended to not only comprehend but also seamlessly transit to
  out-of-domain tasks. However, current endeavors are hamstrung by an over-reliance
  on colossal models, exemplified by models with upwards of 3B parameters, and the
  necessity for an extensive corpus of visual data, often comprising a staggering
  400B tokens. In this paper, we delve into the development of an efficient, autoregression-based
  vision model, innovatively architected to operate on a limited dataset. We meticulously
  demonstrate how this model achieves proficiency in a spectrum of visual tasks spanning
  both high-level and low-level semantic understanding during the testing phase. Our
  empirical evaluations underscore the modelâ€™s agility in adapting to various tasks,
  heralding a significant reduction in the parameter footprint, and a marked decrease
  in training data requirements, thereby paving the way for more sustainable and accessible
  advancements in the field of generalist vision models. The code is available at
  https://github.com/ggjy/DeLVM.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: hao24b
month: 0
tex_title: Data-efficient Large Vision Models through Sequential Autoregression
firstpage: 17572
lastpage: 17596
page: 17572-17596
order: 17572
cycles: false
bibtex_author: Hao, Zhiwei and Guo, Jianyuan and Wang, Chengcheng and Tang, Yehui
  and Wu, Han and Hu, Han and Han, Kai and Xu, Chang
author:
- given: Zhiwei
  family: Hao
- given: Jianyuan
  family: Guo
- given: Chengcheng
  family: Wang
- given: Yehui
  family: Tang
- given: Han
  family: Wu
- given: Han
  family: Hu
- given: Kai
  family: Han
- given: Chang
  family: Xu
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://raw.githubusercontent.com/mlresearch/v235/main/assets/hao24b/hao24b.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
