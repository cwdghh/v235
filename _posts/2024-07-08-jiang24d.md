---
title: On the Origins of Linear Representations in Large Language Models
openreview: otuTw4Mghk
abstract: An array of recent works have argued that high-level semantic concepts are
  encoded "linearly" in the representation space of large language models. In this
  work, we study the origins of such linear representations. To that end, we introduce
  a latent variable model to abstract and formalize the concept dynamics of the next
  token prediction. We use this formalism to prove that linearity arises as a consequence
  of the loss function and the implicit bias of gradient descent. The theory is further
  substantiated empirically via experiments.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: jiang24d
month: 0
tex_title: On the Origins of Linear Representations in Large Language Models
firstpage: 21879
lastpage: 21911
page: 21879-21911
order: 21879
cycles: false
bibtex_author: Jiang, Yibo and Rajendran, Goutham and Ravikumar, Pradeep Kumar and
  Aragam, Bryon and Veitch, Victor
author:
- given: Yibo
  family: Jiang
- given: Goutham
  family: Rajendran
- given: Pradeep Kumar
  family: Ravikumar
- given: Bryon
  family: Aragam
- given: Victor
  family: Veitch
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/assets/jiang24d/jiang24d.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
