---
title: Simplicity Bias of Two-Layer Networks beyond Linearly Separable Data
openreview: eGZH3HCuGm
abstract: Simplicity bias, the propensity of deep models to over-rely on simple features,
  has been identified as a potential reason for limited out-of-distribution generalization
  of neural networks (Shah et al., 2020). Despite the important implications, this
  phenomenon has been theoretically confirmed and characterized only under strong
  dataset assumptions, such as linear separability (Lyu et al., 2021). In this work,
  we characterize simplicity bias for general datasets in the context of two-layer
  neural networks initialized with small weights and trained with gradient flow. Specifically,
  we prove that in the early training phases, network features cluster around a few
  directions that do not depend on the size of the hidden layer. Furthermore, for
  datasets with an XOR-like pattern, we precisely identify the learned features and
  demonstrate that simplicity bias intensifies during later training stages. These
  results indicate that features learned in the middle stages of training may be more
  useful for OOD transfer. We support this hypothesis with experiments on image data.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: tsoy24a
month: 0
tex_title: Simplicity Bias of Two-Layer Networks beyond Linearly Separable Data
firstpage: 48728
lastpage: 48767
page: 48728-48767
order: 48728
cycles: false
bibtex_author: Tsoy, Nikita and Konstantinov, Nikola
author:
- given: Nikita
  family: Tsoy
- given: Nikola
  family: Konstantinov
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/tsoy24a/tsoy24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
