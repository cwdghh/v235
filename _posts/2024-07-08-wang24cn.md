---
title: Benign Overfitting in Adversarial Training of Neural Networks
openreview: DyvhD8J3Wl
abstract: Benign overfitting is the phenomenon wherein none of the predictors in the
  hypothesis class can achieve perfect accuracy (i.e., non-realizable or noisy setting),
  but a model that interpolates the training data still achieves good generalization.
  A series of recent works aim to understand this phenomenon for regression and classification
  tasks using linear predictors as well as two-layer neural networks. In this paper,
  we study such a benign overfitting phenomenon in an adversarial setting. We show
  that under a distributional assumption, interpolating neural networks found using
  adversarial training generalize well despite inference-time attacks. Specifically,
  we provide convergence and generalization guarantees for adversarial training of
  two-layer networks (with smooth as well as non-smooth activation functions) showing
  that under moderate $\ell_2$ norm perturbation budget, the trained model has near-zero
  robust training loss and near-optimal robust generalization error. We support our
  theoretical findings with an empirical study on synthetic and real-world data.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: wang24cn
month: 0
tex_title: Benign Overfitting in Adversarial Training of Neural Networks
firstpage: 52171
lastpage: 52232
page: 52171-52232
order: 52171
cycles: false
bibtex_author: Wang, Yunjuan and Zhang, Kaibo and Arora, Raman
author:
- given: Yunjuan
  family: Wang
- given: Kaibo
  family: Zhang
- given: Raman
  family: Arora
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://raw.githubusercontent.com/mlresearch/v235/main/assets/wang24cn/wang24cn.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
