---
title: Denoising Autoregressive Representation Learning
openreview: dW29JZj0G5
abstract: In this paper, we explore a new generative approach for learning visual
  representations. Our method, DARL, employs a decoder-only Transformer to predict
  image patches autoregressively. We find that training with Mean Squared Error (MSE)
  alone leads to strong representations. To enhance the image generation ability,
  we replace the MSE loss with the diffusion objective by using a denoising patch
  decoder. We show that the learned representation can be improved by using tailored
  noise schedules and longer training in larger models. Notably, the optimal schedule
  differs significantly from the typical ones used in standard image diffusion models.
  Overall, despite its simple architecture, DARL delivers performance remarkably close
  to state-of-the-art masked prediction models under the fine-tuning protocol. This
  marks an important step towards a unified model capable of both visual perception
  and generation, effectively combining the strengths of autoregressive and denoising
  diffusion models.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: li24d
month: 0
tex_title: Denoising Autoregressive Representation Learning
firstpage: 27417
lastpage: 27438
page: 27417-27438
order: 27417
cycles: false
bibtex_author: Li, Yazhe and Bornschein, Jorg and Chen, Ting
author:
- given: Yazhe
  family: Li
- given: Jorg
  family: Bornschein
- given: Ting
  family: Chen
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/assets/li24d/li24d.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
