---
title: Interpreting and Improving Large Language Models in Arithmetic Calculation
openreview: CfOtiepP8s
abstract: Large language models (LLMs) have demonstrated remarkable potential across
  numerous applications and have shown an emergent ability to tackle complex reasoning
  tasks, such as mathematical computations. However, even for the simplest arithmetic
  calculations, the intrinsic mechanisms behind LLMs remains mysterious, making it
  challenging to ensure reliability. In this work, we delve into uncovering a specific
  mechanism by which LLMs execute calculations. Through comprehensive experiments,
  we find that LLMs frequently involve a small fraction ($<$5%) of attention heads,
  which play a pivotal role in focusing on operands and operators during calculation
  processes. Subsequently, the information from these operands is processed through
  multi-layer perceptrons (MLPs), progressively leading to the final solution. These
  pivotal heads/MLPs, though identified on a specific dataset, exhibit transferability
  across different datasets and even distinct tasks. This insight prompted us to investigate
  the potential benefits of selectively fine-tuning these essential heads/MLPs to
  boost the LLMsâ€™ computational performance. We empirically find that such precise
  tuning can yield notable enhancements on mathematical prowess, without compromising
  the performance on non-mathematical tasks. Our work serves as a preliminary exploration
  into the arithmetic calculation abilities inherent in LLMs, laying a solid foundation
  to reveal more intricate mathematical tasks.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: zhang24bk
month: 0
tex_title: Interpreting and Improving Large Language Models in Arithmetic Calculation
firstpage: 59932
lastpage: 59950
page: 59932-59950
order: 59932
cycles: false
bibtex_author: Zhang, Wei and Wan, Chaoqun and Zhang, Yonggang and Cheung, Yiu-Ming
  and Tian, Xinmei and Shen, Xu and Ye, Jieping
author:
- given: Wei
  family: Zhang
- given: Chaoqun
  family: Wan
- given: Yonggang
  family: Zhang
- given: Yiu-Ming
  family: Cheung
- given: Xinmei
  family: Tian
- given: Xu
  family: Shen
- given: Jieping
  family: Ye
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://raw.githubusercontent.com/mlresearch/v235/main/assets/zhang24bk/zhang24bk.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
