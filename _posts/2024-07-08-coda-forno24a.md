---
title: 'CogBench: a large language model walks into a psychology lab'
openreview: Q3104y8djk
abstract: Large language models (LLMs) have significantly advanced the field of artificial
  intelligence. Yet, evaluating them comprehensively remains challenging. We argue
  that this is partly due to the predominant focus on performance metrics in most
  benchmarks. This paper introduces <em>CogBench</em>, a benchmark that includes ten
  behavioral metrics derived from seven cognitive psychology experiments. This novel
  approach offers a toolkit for phenotyping LLMs’ behavior. We apply <em>CogBench</em>
  to 40 LLMs, yielding a rich and diverse dataset. We analyze this data using statistical
  multilevel modeling techniques, accounting for the nested dependencies among fine-tuned
  versions of specific LLMs. Our study highlights the crucial role of model size and
  reinforcement learning from human feedback (RLHF) in improving performance and aligning
  with human behavior. Interestingly, we find that open-source models are less risk-prone
  than proprietary models and that fine-tuning on code does not necessarily enhance
  LLMs’ behavior. Finally, we explore the effects of prompt-engineering techniques.
  We discover that chain-of-thought prompting improves probabilistic reasoning, while
  take-a-step-back prompting fosters model-based behaviors.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: coda-forno24a
month: 0
tex_title: "{C}og{B}ench: a large language model walks into a psychology lab"
firstpage: 9076
lastpage: 9108
page: 9076-9108
order: 9076
cycles: false
bibtex_author: Coda-Forno, Julian and Binz, Marcel and Wang, Jane X and Schulz, Eric
author:
- given: Julian
  family: Coda-Forno
- given: Marcel
  family: Binz
- given: Jane X
  family: Wang
- given: Eric
  family: Schulz
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/coda-forno24a/coda-forno24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
