---
title: Symmetry Induces Structure and Constraint of Learning
openreview: 7AF0AMI4AE
abstract: 'Due to common architecture designs, symmetries exist extensively in contemporary
  neural networks. In this work, we unveil the importance of the loss function symmetries
  in affecting, if not deciding, the learning behavior of machine learning models.
  We prove that every mirror-reflection symmetry, with reflection surface $O$, in
  the loss function leads to the emergence of a constraint on the model parameters
  $\theta$: $O^T\theta =0$. This constrained solution becomes satisfied when either
  the weight decay or gradient noise is large. Common instances of mirror symmetries
  in deep learning include rescaling, rotation, and permutation symmetry. As direct
  corollaries, we show that rescaling symmetry leads to sparsity, rotation symmetry
  leads to low rankness, and permutation symmetry leads to homogeneous ensembling.
  Then, we show that the theoretical framework can explain intriguing phenomena, such
  as the loss of plasticity and various collapse phenomena in neural networks, and
  suggest how symmetries can be used to design an elegant algorithm to enforce hard
  constraints in a differentiable way.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: ziyin24a
month: 0
tex_title: Symmetry Induces Structure and Constraint of Learning
firstpage: 62847
lastpage: 62866
page: 62847-62866
order: 62847
cycles: false
bibtex_author: Ziyin, Liu
author:
- given: Liu
  family: Ziyin
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://raw.githubusercontent.com/mlresearch/v235/main/assets/ziyin24a/ziyin24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
