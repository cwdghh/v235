---
title: Representation Surgery for Multi-Task Model Merging
openreview: Sbl2keQEML
abstract: Multi-task learning (MTL) compresses the information from multiple tasks
  into a unified backbone to improve computational efficiency and generalization.
  Recent work directly merges multiple independently trained models to perform MTL
  instead of collecting their raw data for joint training, greatly expanding the application
  scenarios of MTL. However, by visualizing the representation distribution of existing
  model merging schemes, we find that the merged model often suffers from the dilemma
  of representation bias. That is, there is a significant discrepancy in the representation
  distribution between the merged and individual models, resulting in poor performance
  of merged MTL. In this paper, we propose a representation surgery solution called
  “Surgery" to reduce representation bias in the merged model. Specifically, Surgery
  is a lightweight task-specific plugin that takes the representation of the merged
  model as input and attempts to output the biases contained in the representation
  from the merged model. We then designed an unsupervised optimization objective that
  updates the Surgery plugin by minimizing the distance between the merged model’s
  representation and the individual model’s representation. Extensive experiments
  demonstrate significant MTL performance improvements when our Surgery plugin is
  applied to state-of-the-art (SOTA) model merging schemes.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: yang24t
month: 0
tex_title: Representation Surgery for Multi-Task Model Merging
firstpage: 56332
lastpage: 56356
page: 56332-56356
order: 56332
cycles: false
bibtex_author: Yang, Enneng and Shen, Li and Wang, Zhenyi and Guo, Guibing and Chen,
  Xiaojun and Wang, Xingwei and Tao, Dacheng
author:
- given: Enneng
  family: Yang
- given: Li
  family: Shen
- given: Zhenyi
  family: Wang
- given: Guibing
  family: Guo
- given: Xiaojun
  family: Chen
- given: Xingwei
  family: Wang
- given: Dacheng
  family: Tao
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/assets/yang24t/yang24t.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
