---
title: 'PIDformer: Transformer Meets Control Theory'
openreview: SRzb3QDjdV
abstract: 'In this work, we address two main shortcomings of transformer architectures:
  input corruption and rank collapse in their output representation. We unveil self-attention
  as an autonomous state-space model that inherently promotes smoothness in its solutions,
  leading to lower-rank outputs and diminished representation capacity. Moreover,
  the steady-state solution of the model is sensitive to input perturbations. We incorporate
  a Proportional-Integral-Derivative (PID) closed-loop feedback control system with
  a reference point into the model to improve robustness and representation capacity.
  This integration aims to preserve high-frequency details while bolstering model
  stability, rendering it more noise-resilient. The resulting controlled state-space
  model is theoretically proven robust and adept at addressing the rank collapse.
  Motivated by this control framework, we derive a novel class of transformers, PID-controlled
  Transformer (PIDformer), aimed at improving robustness and mitigating the rank-collapse
  issue inherent in softmax transformers. We empirically evaluate the model for advantages
  and robustness against baseline transformers across various practical tasks, including
  object classification, image segmentation, and language modeling.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: nguyen24i
month: 0
tex_title: "{PID}former: Transformer Meets Control Theory"
firstpage: 37776
lastpage: 37797
page: 37776-37797
order: 37776
cycles: false
bibtex_author: Nguyen, Tam Minh and Uribe, Cesar A and Nguyen, Tan Minh and Baraniuk,
  Richard
author:
- given: Tam Minh
  family: Nguyen
- given: Cesar A
  family: Uribe
- given: Tan Minh
  family: Nguyen
- given: Richard
  family: Baraniuk
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/assets/nguyen24i/nguyen24i.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
