---
title: Improved Operator Learning by Orthogonal Attention
openreview: 6w7zkf9FBR
abstract: This work presents orthogonal attention for constructing neural operators
  to serve as surrogates to model the solutions of a family of Partial Differential
  Equations (PDEs). The motivation is that the kernel integral operator, which is
  usually at the core of neural operators, can be reformulated with orthonormal eigenfunctions.
  Inspired by the success of the neural approximation of eigenfunctions (Deng et al.,
  2022), we opt to directly parameterize the involved eigenfunctions with flexible
  neural networks (NNs), based on which the input function is then transformed by
  the rule of kernel integral. Surprisingly, the resulting NN module bears a striking
  resemblance to regular attention mechanisms, albeit without softmax. Instead, it
  incorporates an orthogonalization operation that provides regularization during
  model training and helps mitigate overfitting, particularly in scenarios with limited
  data availability. In practice, the orthogonalization operation can be implemented
  with minimal additional overheads. Experiments on six standard neural operator benchmark
  datasets comprising both regular and irregular geometries show that our method can
  outperform competing baselines with decent margins.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: xiao24c
month: 0
tex_title: Improved Operator Learning by Orthogonal Attention
firstpage: 54288
lastpage: 54299
page: 54288-54299
order: 54288
cycles: false
bibtex_author: Xiao, Zipeng and Hao, Zhongkai and Lin, Bokai and Deng, Zhijie and
  Su, Hang
author:
- given: Zipeng
  family: Xiao
- given: Zhongkai
  family: Hao
- given: Bokai
  family: Lin
- given: Zhijie
  family: Deng
- given: Hang
  family: Su
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/assets/xiao24c/xiao24c.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
