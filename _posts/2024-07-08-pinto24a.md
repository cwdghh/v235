---
title: Extracting Training Data From Document-Based VQA Models
openreview: qTX1vxzs8b
abstract: Vision-Language Models (VLMs) have made remarkable progress in document-based
  Visual Question Answering (i.e., responding to queries about the contents of an
  input document provided as an image). In this work, we show these models can memorize
  responses for training samples and regurgitate them even when the relevant visual
  information has been removed. This includes Personal Identifiable Information (PII)
  repeated once in the training set, indicating these models could divulge memorised
  sensitive information and therefore pose a privacy risk. We quantitatively measure
  the extractability of information in controlled experiments and differentiate between
  cases where it arises from generalization capabilities or from memorization. We
  further investigate the factors that influence memorization across multiple state-of-the-art
  models and propose an effective heuristic countermeasure that empirically prevents
  the extractability of PII.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: pinto24a
month: 0
tex_title: Extracting Training Data From Document-Based {VQA} Models
firstpage: 40813
lastpage: 40826
page: 40813-40826
order: 40813
cycles: false
bibtex_author: Pinto, Francesco and Rauschmayr, Nathalie and Tram\`{e}r, Florian and
  Torr, Philip and Tombari, Federico
author:
- given: Francesco
  family: Pinto
- given: Nathalie
  family: Rauschmayr
- given: Florian
  family: Tram√®r
- given: Philip
  family: Torr
- given: Federico
  family: Tombari
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/assets/pinto24a/pinto24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
