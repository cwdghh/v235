---
title: Exact Conversion of In-Context Learning to Model Weights in Linearized-Attention
  Transformers
openreview: LVF4P1NNwO
abstract: In-Context Learning (ICL) has been a powerful emergent property of large
  language models that has attracted increasing attention in recent years. In contrast
  to regular gradient-based learning, ICL is highly interpretable and does not require
  parameter updates. In this paper, we show that, for linearized transformer networks,
  ICL can be made explicit and permanent through the inclusion of bias terms. We mathematically
  demonstrate the equivalence between a model with ICL demonstration prompts and the
  same model with the additional bias terms. Our algorithm (ICLCA) allows for exact
  conversion in an inexpensive manner. Existing methods are not exact and require
  expensive parameter updates. We demonstrate the efficacy of our approach through
  experiments that show the exact incorporation of ICL tokens into a linear transformer.
  We further suggest how our method can be adapted to achieve cheap approximate conversion
  of ICL tokens, even in regular transformer networks that are not linearized. Our
  experiments on GPT-2 show that, even though the conversion is only approximate,
  the model still gains valuable context from the included bias terms.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: chen24r
month: 0
tex_title: Exact Conversion of In-Context Learning to Model Weights in Linearized-Attention
  Transformers
firstpage: 6833
lastpage: 6846
page: 6833-6846
order: 6833
cycles: false
bibtex_author: Chen, Brian K and Hu, Tianyang and Jin, Hui and Lee, Hwee Kuan and
  Kawaguchi, Kenji
author:
- given: Brian K
  family: Chen
- given: Tianyang
  family: Hu
- given: Hui
  family: Jin
- given: Hwee Kuan
  family: Lee
- given: Kenji
  family: Kawaguchi
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://raw.githubusercontent.com/mlresearch/v235/main/assets/chen24r/chen24r.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
