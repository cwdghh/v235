---
title: A Single-Loop Robust Policy Gradient Method for Robust Markov Decision Processes
openreview: VaZVZQSgTP
abstract: Robust Markov Decision Processes (RMDPs) have recently been recognized as
  a valuable and promising approach to discovering a policy with creditable performance,
  particularly in the presence of a dynamic environment and estimation errors in the
  transition matrix due to limited data. Despite extensive exploration of dynamic
  programming algorithms for solving RMDPs, there has been a notable upswing in interest
  in developing efficient algorithms using the policy gradient method. In this paper,
  we propose the first single-loop robust policy gradient (SRPG) method with the global
  optimality guarantee for solving RMDPs through its minimax formulation. Moreover,
  we complement the convergence analysis of the nonconvex-nonconcave min-max optimization
  problem with the objective functionâ€™s gradient dominance property, which is not
  explored in the prior literature. Numerical experiments validate the efficacy of
  SRPG, demonstrating its faster and more robust convergence behavior compared to
  its nested-loop counterpart.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: lin24u
month: 0
tex_title: A Single-Loop Robust Policy Gradient Method for Robust {M}arkov Decision
  Processes
firstpage: 30392
lastpage: 30426
page: 30392-30426
order: 30392
cycles: false
bibtex_author: Lin, Zhenwei and Xue, Chenyu and Deng, Qi and Ye, Yinyu
author:
- given: Zhenwei
  family: Lin
- given: Chenyu
  family: Xue
- given: Qi
  family: Deng
- given: Yinyu
  family: Ye
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://raw.githubusercontent.com/mlresearch/v235/main/assets/lin24u/lin24u.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
