---
title: 'Iterative Preference Learning from Human Feedback: Bridging Theory and Practice
  for RLHF under KL-constraint'
openreview: c1AKcA6ry1
abstract: This paper studies the theoretical framework of the alignment process of
  generative models with Reinforcement Learning from Human Feedback (RLHF). We consider
  a standard mathematical formulation, the reverse-KL regularized contextual bandit
  for RLHF. Despite its widespread practical application, a rigorous theoretical analysis
  of this formulation remains open. We investigate its behavior in three distinct
  settings—offline, online, and hybrid—and propose efficient algorithms with finite-sample
  theoretical guarantees. Moving towards practical applications, our framework, with
  a robust approximation of the information-theoretical policy improvement oracle,
  naturally gives rise to several novel RLHF algorithms. This includes an iterative
  version of the Direct Preference Optimization (DPO) algorithm for online settings,
  and a multi-step rejection sampling strategy for offline scenarios. Our empirical
  evaluations on real-world alignment experiment of large language model demonstrate
  that these proposed methods significantly surpass existing strong baselines, such
  as DPO and Rejection Sampling Optimization (RSO), showcasing the connections between
  solid theoretical foundations and their potent practical implementations.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: xiong24a
month: 0
tex_title: 'Iterative Preference Learning from Human Feedback: Bridging Theory and
  Practice for {RLHF} under {KL}-constraint'
firstpage: 54715
lastpage: 54754
page: 54715-54754
order: 54715
cycles: false
bibtex_author: Xiong, Wei and Dong, Hanze and Ye, Chenlu and Wang, Ziqi and Zhong,
  Han and Ji, Heng and Jiang, Nan and Zhang, Tong
author:
- given: Wei
  family: Xiong
- given: Hanze
  family: Dong
- given: Chenlu
  family: Ye
- given: Ziqi
  family: Wang
- given: Han
  family: Zhong
- given: Heng
  family: Ji
- given: Nan
  family: Jiang
- given: Tong
  family: Zhang
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/xiong24a/xiong24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
