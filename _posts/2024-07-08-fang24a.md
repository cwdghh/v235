---
title: 'Bayesian Knowledge Distillation: A Bayesian Perspective of Distillation with
  Uncertainty Quantification'
openreview: knZ4NYzGUd
abstract: Knowledge distillation (KD) has been widely used for model compression and
  deployment acceleration. Nonetheless, the statistical insight of the remarkable
  performance of KD remains elusive, and methods for evaluating the uncertainty of
  the distilled model/student model are lacking. To address these issues, we establish
  a close connection between KD and a Bayesian model. In particular, we develop an
  innovative method named Bayesian Knowledge Distillation (BKD) to provide a transparent
  interpretation of the working mechanism of KD, and a suite of Bayesian inference
  tools for the uncertainty quantification of the student model. In BKD, the regularization
  imposed by the teacher model in KD is formulated as a teacher-informed prior for
  the student modelâ€™s parameters. Consequently, we establish the equivalence between
  minimizing the KD loss and estimating the posterior mode in BKD. Efficient Bayesian
  inference algorithms are developed based on the stochastic gradient Langevin Monte
  Carlo and examined with extensive experiments on uncertainty ranking and credible
  intervals construction for predicted class probabilities.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: fang24a
month: 0
tex_title: "{B}ayesian Knowledge Distillation: A {B}ayesian Perspective of Distillation
  with Uncertainty Quantification"
firstpage: 12935
lastpage: 12956
page: 12935-12956
order: 12935
cycles: false
bibtex_author: Fang, Luyang and Chen, Yongkai and Zhong, Wenxuan and Ma, Ping
author:
- given: Luyang
  family: Fang
- given: Yongkai
  family: Chen
- given: Wenxuan
  family: Zhong
- given: Ping
  family: Ma
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/fang24a/fang24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
