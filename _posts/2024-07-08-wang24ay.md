---
title: Transforming and Combining Rewards for Aligning Large Language Models
openreview: cAWbm9KRZO
abstract: 'A common approach for aligning language models to human preferences is
  to first learn a reward model from preference data, and then use this reward model
  to update the language model. We study two closely related problems that arise in
  this approach. First, any monotone transformation of the reward model preserves
  preference ranking; is there a choice that is "better" than others? Second, we often
  wish to align language models to multiple properties: how should we combine multiple
  reward models? Using a probabilistic interpretation of the alignment procedure,
  we identify a natural choice for transformation for (the common case of) rewards
  learned from Bradley-Terry preference models. The derived transformation is straightforward:
  we apply a log-sigmoid function to the centered rewards, a method we term "LSC-transformation"
  (log-sigmoid-centered transformation). This transformation has two important properties.
  First, it emphasizes improving poorly-performing outputs, rather than outputs that
  already score well. This mitigates both underfitting (where some prompts are not
  improved) and reward hacking (where the model learns to exploit misspecification
  of the reward model). Second, it enables principled aggregation of rewards by linking
  summation to logical conjunction: the sum of transformed rewards corresponds to
  the probability that the output is "good" in all measured properties, in a sense
  we make precise. Experiments aligning language models to be both helpful and harmless
  using RLHF show substantial improvements over the baseline (non-transformed) approach.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: wang24ay
month: 0
tex_title: Transforming and Combining Rewards for Aligning Large Language Models
firstpage: 51161
lastpage: 51176
page: 51161-51176
order: 51161
cycles: false
bibtex_author: Wang, Zihao and Nagpal, Chirag and Berant, Jonathan and Eisenstein,
  Jacob and D'Amour, Alexander Nicholas and Koyejo, Sanmi and Veitch, Victor
author:
- given: Zihao
  family: Wang
- given: Chirag
  family: Nagpal
- given: Jonathan
  family: Berant
- given: Jacob
  family: Eisenstein
- given: Alexander Nicholas
  family: Dâ€™Amour
- given: Sanmi
  family: Koyejo
- given: Victor
  family: Veitch
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/wang24ay/wang24ay.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
