---
title: Stealing part of a production language model
openreview: VE3yWXt3KB
abstract: We introduce the first model-stealing attack that extracts precise, nontrivial
  information from black-box production language models like OpenAI’s ChatGPT or Google’s
  PaLM-2. Specifically, our attack recovers the embedding projection layer (up to
  symmetries) of a transformer model, given typical API access. For under $20 USD,
  our attack extracts the entire projection matrix of OpenAI’s Ada and Babbage language
  models. We thereby confirm, for the first time, that these black-box models have
  a hidden dimension of 1024 and 2048, respectively. We also recover the exact hidden
  dimension size of the GPT-3.5-turbo model, and estimate it would cost under \$2,000
  in queries to recover the entire projection matrix. We conclude with potential defenses
  and mitigations, and discuss the implications of possible future work that could
  extend our attack.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: carlini24a
month: 0
tex_title: Stealing part of a production language model
firstpage: 5680
lastpage: 5705
page: 5680-5705
order: 5680
cycles: false
bibtex_author: Carlini, Nicholas and Paleka, Daniel and Dvijotham, Krishnamurthy Dj
  and Steinke, Thomas and Hayase, Jonathan and Cooper, A. Feder and Lee, Katherine
  and Jagielski, Matthew and Nasr, Milad and Conmy, Arthur and Wallace, Eric and Rolnick,
  David and Tram\`{e}r, Florian
author:
- given: Nicholas
  family: Carlini
- given: Daniel
  family: Paleka
- given: Krishnamurthy Dj
  family: Dvijotham
- given: Thomas
  family: Steinke
- given: Jonathan
  family: Hayase
- given: A. Feder
  family: Cooper
- given: Katherine
  family: Lee
- given: Matthew
  family: Jagielski
- given: Milad
  family: Nasr
- given: Arthur
  family: Conmy
- given: Eric
  family: Wallace
- given: David
  family: Rolnick
- given: Florian
  family: Tramèr
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/carlini24a/carlini24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
