---
title: Implicit Compressibility of Overparametrized Neural Networks Trained with Heavy-Tailed
  SGD
openreview: CpgKRKBUTl
abstract: 'Neural network compression has been an increasingly important subject,
  not only due to its practical relevance, but also due to its theoretical implications,
  as there is an explicit connection between compressibility and generalization error.
  Recent studies have shown that the choice of the hyperparameters of stochastic gradient
  descent (SGD) can have an effect on the compressibility of the learned parameter
  vector. These results, however, rely on unverifiable assumptions and the resulting
  theory does not provide a practical guideline due to its implicitness. In this study,
  we propose a simple modification for SGD, such that the outputs of the algorithm
  will be provably compressible without making any nontrivial assumptions. We consider
  a one-hidden-layer neural network trained with SGD, and show that if we inject additive
  heavy-tailed noise to the iterates at each iteration, for <em>any</em> compression
  rate, there exists a level of overparametrization such that the output of the algorithm
  will be compressible with high probability. To achieve this result, we make two
  main technical contributions: (i) we prove a "propagation of chaos" result for a
  class of heavy-tailed stochastic differential equations, and (ii) we derive error
  estimates for their Euler discretization. Our experiments suggest that the proposed
  approach not only achieves increased compressibility with various models and datasets,
  but also leads to robust test performance under pruning, even in more realistic
  architectures that lie beyond our theoretical setting.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: wan24a
month: 0
tex_title: Implicit Compressibility of Overparametrized Neural Networks Trained with
  Heavy-Tailed {SGD}
firstpage: 49845
lastpage: 49866
page: 49845-49866
order: 49845
cycles: false
bibtex_author: Wan, Yijun and Barsbey, Melih and Zaidi, Abdellatif and Simsekli, Umut
author:
- given: Yijun
  family: Wan
- given: Melih
  family: Barsbey
- given: Abdellatif
  family: Zaidi
- given: Umut
  family: Simsekli
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/assets/wan24a/wan24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
