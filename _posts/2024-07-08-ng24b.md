---
title: Measuring Stochastic Data Complexity with Boltzmann Influence Functions
openreview: JNN6QHhLHB
abstract: Estimating the uncertainty of a modelâ€™s prediction on a test point is a
  crucial part of ensuring reliability and calibration under distribution shifts.A
  minimum description length approach to this problem uses the predictive normalized
  maximum likelihood (pNML) distribution, which considers every possible label for
  a data point, and decreases confidence in a prediction if other labels are also
  consistent with the model and training data. In this work we propose IF-COMP, a
  scalable and efficient approximation of the pNML distribution that linearizes the
  model with a temperature-scaled Boltzmann influence function. IF-COMP can be used
  to produce well-calibrated predictions on test points as well as measure complexity
  in both labelled and unlabelled settings. We experimentally validate IF-COMP on
  uncertainty calibration, mislabel detection, and OOD detection tasks, where it consistently
  matches or beats strong baseline methods.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: ng24b
month: 0
tex_title: Measuring Stochastic Data Complexity with Boltzmann Influence Functions
firstpage: 37553
lastpage: 37569
page: 37553-37569
order: 37553
cycles: false
bibtex_author: Ng, Nathan Hoyen and Grosse, Roger Baker and Ghassemi, Marzyeh
author:
- given: Nathan Hoyen
  family: Ng
- given: Roger Baker
  family: Grosse
- given: Marzyeh
  family: Ghassemi
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://raw.githubusercontent.com/mlresearch/v235/main/assets/ng24b/ng24b.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
