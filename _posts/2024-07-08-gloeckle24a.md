---
title: Better & Faster Large Language Models via Multi-token Prediction
openreview: pEWAcejiU2
abstract: Large language models such as GPT and Llama are trained with a next-token
  prediction loss. In this work, we suggest that training language models to predict
  multiple future tokens at once results in higher sample efficiency. More specifically,
  at each position in the training corpus, we ask the model to predict the following
  $n$ tokens using $n$ independent output heads, operating on top of a shared model
  trunk. Considering multi-token prediction as an auxiliary training task, we measure
  improved downstream capabilities with no overhead in training time for both code
  and natural language models. The method is increasingly useful for larger model
  sizes, and keeps its appeal when training for multiple epochs. Gains are especially
  pronounced on generative benchmarks like coding, where our models consistently outperform
  strong baselines by several percentage points. Our 13B parameter models solves 12%
  more problems on Human Eval and 17% more on MBPP than comparable next-token models.
  Experiments on small algorithmic tasks demonstrate that multi-token prediction is
  favorable for the development of induction heads and algorithmic reasoning capabilities.
  As an additional benefit, models trained with 4-token prediction are up to $3\times$
  faster at inference, even with large batch sizes.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: gloeckle24a
month: 0
tex_title: Better & Faster Large Language Models via Multi-token Prediction
firstpage: 15706
lastpage: 15734
page: 15706-15734
order: 15706
cycles: false
bibtex_author: Gloeckle, Fabian and Youbi Idrissi, Badr and Roziere, Baptiste and
  Lopez-Paz, David and Synnaeve, Gabriel
author:
- given: Fabian
  family: Gloeckle
- given: Badr
  family: Youbi Idrissi
- given: Baptiste
  family: Roziere
- given: David
  family: Lopez-Paz
- given: Gabriel
  family: Synnaeve
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/gloeckle24a/gloeckle24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
