---
title: Emergent Representations of Program Semantics in Language Models Trained on
  Programs
openreview: 8PTx4CpNoT
abstract: We present evidence that language models (LMs) of code can learn to represent
  the formal semantics of programs, despite being trained only to perform next-token
  prediction. Specifically, we train a Transformer model on a synthetic corpus of
  programs written in a domain-specific language for navigating 2D grid world environments.
  Each program in the corpus is preceded by a (partial) specification in the form
  of several input-output grid world states. Despite providing no further inductive
  biases, we find that a probing classifier is able to extract increasingly accurate
  representations of the <em>unobserved, intermediate</em> grid world states from
  the LM hidden states over the course of training, suggesting the LM acquires an
  emergent ability to <em>interpret</em> programs in the formal sense. We also develop
  a novel interventional baseline that enables us to disambiguate what is represented
  by the LM as opposed to learned by the probe. We anticipate that this technique
  may be generally applicable to a broad range of <em>semantic</em> probing experiments.
  In summary, this paper does not propose any new techniques for training LMs of code,
  but develops an experimental framework for and provides insights into the acquisition
  and representation of formal semantics in statistical models of code.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: jin24e
month: 0
tex_title: Emergent Representations of Program Semantics in Language Models Trained
  on Programs
firstpage: 22160
lastpage: 22184
page: 22160-22184
order: 22160
cycles: false
bibtex_author: Jin, Charles and Rinard, Martin
author:
- given: Charles
  family: Jin
- given: Martin
  family: Rinard
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://raw.githubusercontent.com/mlresearch/v235/main/assets/jin24e/jin24e.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
