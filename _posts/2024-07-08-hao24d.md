---
title: 'DPOT: Auto-Regressive Denoising Operator Transformer for Large-Scale PDE Pre-Training'
openreview: X7UnDevHOM
abstract: Pre-training has been investigated to improve the efficiency and performance
  of training neural operators in data-scarce settings. However, it is largely in
  its infancy due to the inherent complexity and diversity, such as long trajectories,
  multiple scales and varying dimensions of partial differential equations (PDEs)
  data. In this paper, we present a new auto-regressive denoising pre-training strategy,
  which allows for more stable and efficient pre-training on PDE data and generalizes
  to various downstream tasks. Moreover, by designing a flexible and scalable model
  architecture based on Fourier attention, we can easily scale up the model for large-scale
  pre-training. We train our PDE foundation model with up to 0.5B parameters on 10+
  PDE datasets with more than 100k trajectories. Extensive experiments show that we
  achieve SOTA on these benchmarks and validate the strong generalizability of our
  model to significantly enhance performance on diverse downstream PDE tasks like
  3D data.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: hao24d
month: 0
tex_title: "{DPOT}: Auto-Regressive Denoising Operator Transformer for Large-Scale
  {PDE} Pre-Training"
firstpage: 17616
lastpage: 17635
page: 17616-17635
order: 17616
cycles: false
bibtex_author: Hao, Zhongkai and Su, Chang and Liu, Songming and Berner, Julius and
  Ying, Chengyang and Su, Hang and Anandkumar, Anima and Song, Jian and Zhu, Jun
author:
- given: Zhongkai
  family: Hao
- given: Chang
  family: Su
- given: Songming
  family: Liu
- given: Julius
  family: Berner
- given: Chengyang
  family: Ying
- given: Hang
  family: Su
- given: Anima
  family: Anandkumar
- given: Jian
  family: Song
- given: Jun
  family: Zhu
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/assets/hao24d/hao24d.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
