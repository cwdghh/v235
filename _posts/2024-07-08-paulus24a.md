---
title: 'LPGD: A General Framework for Backpropagation through Embedded Optimization
  Layers'
openreview: TfWKkSAziC
abstract: Embedding parameterized optimization problems as layers into machine learning
  architectures serves as a powerful inductive bias. Training such architectures with
  stochastic gradient descent requires care, as degenerate derivatives of the embedded
  optimization problem often render the gradients uninformative. We propose Lagrangian
  Proximal Gradient Descent (LPGD), a flexible framework for training architectures
  with embedded optimization layers that seamlessly integrates into automatic differentiation
  libraries. LPGD efficiently computes meaningful replacements of the degenerate optimization
  layer derivatives by re-running the forward solver oracle on a perturbed input.
  LPGD captures various previously proposed methods as special cases, while fostering
  deep links to traditional optimization methods. We theoretically analyze our method
  and demonstrate on historical and synthetic data that LPGD converges faster than
  gradient descent even in a differentiable setup.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: paulus24a
month: 0
tex_title: "{LPGD}: A General Framework for Backpropagation through Embedded Optimization
  Layers"
firstpage: 39989
lastpage: 40014
page: 39989-40014
order: 39989
cycles: false
bibtex_author: Paulus, Anselm and Martius, Georg and Musil, V\'{\i}t
author:
- given: Anselm
  family: Paulus
- given: Georg
  family: Martius
- given: Vı́t
  family: Musil
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/paulus24a/paulus24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
