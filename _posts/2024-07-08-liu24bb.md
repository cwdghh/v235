---
title: How do Large Language Models Navigate Conflicts between Honesty and Helpfulness?
openreview: 685vj0lC9z
abstract: In day-to-day communication, people often approximate the truth — for example,
  rounding the time or omitting details — in order to be maximally helpful to the
  listener. How do large language models (LLMs) handle such nuanced trade-offs? To
  address this question, we use psychological models and experiments designed to characterize
  human behavior to analyze LLMs. We test a range of LLMs and explore how optimization
  for human preferences or inference-time reasoning affects these trade-offs. We find
  that reinforcement learning from human feedback improves both honesty and helpfulness,
  while chain-of-thought prompting skews LLMs towards helpfulness over honesty. Finally,
  GPT-4 Turbo demonstrates human-like response patterns including sensitivity to the
  conversational framing and listener’s decision context. Our findings reveal the
  conversational values internalized by LLMs and suggest that even these abstract
  values can, to a degree, be steered by zero-shot prompting.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: liu24bb
month: 0
tex_title: How do Large Language Models Navigate Conflicts between Honesty and Helpfulness?
firstpage: 31844
lastpage: 31865
page: 31844-31865
order: 31844
cycles: false
bibtex_author: Liu, Ryan and Sumers, Theodore and Dasgupta, Ishita and Griffiths,
  Thomas L.
author:
- given: Ryan
  family: Liu
- given: Theodore
  family: Sumers
- given: Ishita
  family: Dasgupta
- given: Thomas L.
  family: Griffiths
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/liu24bb/liu24bb.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
