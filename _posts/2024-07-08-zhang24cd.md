---
title: Offline Training of Language Model Agents with Functions as Learnable Weights
openreview: 2xbkWiEuR1
abstract: Researchers and practitioners have recently reframed powerful Large Language
  Models (LLMs) as <em>agents</em>, enabling them to automate complex tasks largely
  via the use of specialized functions. To facilitate the development of LLM agents,
  we present a novel paradigm of training LLM agents without modifying the LLM weights,
  which is particularly useful when the LLMs are difficult or inaccessible for modifications.
  Inspired by how humans continuously forge tools to adapt to real-world tasks, rather
  than change our biological structure to fit a static set of tools, we propose to
  progressively forge agent’s functions to better solve the downstream tasks instead
  of modifying the LLM weights. By treating the functions as learnable ‘agent parameters’
  and leveraging the fundamental idea of model training in artificial intelligence,
  we develop AgentOptimizer that employs the LLM to update agents’ functions and devise
  an <em>agent training</em> algorithm with two strategies, roll-back, and early-stop,
  to streamline the training process. With extensive experiments, we showcase that
  the agent training paradigm could significantly improve the performance of representative
  LLM agents in various downstream tasks. We also study the behavior of the agent
  training regarding aspects like the learning curve and domain transferability.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: zhang24cd
month: 0
tex_title: Offline Training of Language Model Agents with Functions as Learnable Weights
firstpage: 60315
lastpage: 60335
page: 60315-60335
order: 60315
cycles: false
bibtex_author: Zhang, Shaokun and Zhang, Jieyu and Liu, Jiale and Song, Linxin and
  Wang, Chi and Krishna, Ranjay and Wu, Qingyun
author:
- given: Shaokun
  family: Zhang
- given: Jieyu
  family: Zhang
- given: Jiale
  family: Liu
- given: Linxin
  family: Song
- given: Chi
  family: Wang
- given: Ranjay
  family: Krishna
- given: Qingyun
  family: Wu
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/zhang24cd/zhang24cd.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
