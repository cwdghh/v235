---
title: Do Large Code Models Understand Programming Concepts? Counterfactual Analysis
  for Code Predicates
openreview: ADnUzsmsLW
abstract: Large Language Modelsâ€™ success in text generation has also made them better
  at code generation and coding tasks. While a lot of work has demonstrated their
  remarkable performance on tasks such as code completion and editing, it is still
  unclear as to why. We help bridge this gap by exploring to what degree auto-regressive
  models understand the logical constructs of the underlying programs. We propose
  Counterfactual Analysis for Programming Concept Predicates (CACP) as a counterfactual
  testing framework to evaluate whether Large Code Models understand programming concepts.
  With only black-box access to the model, we use CACP to evaluate ten popular Large
  Code Models for four different programming concepts. Our findings suggest that current
  models lack understanding of concepts such as data flow and control flow.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: hooda24a
month: 0
tex_title: Do Large Code Models Understand Programming Concepts? {C}ounterfactual
  Analysis for Code Predicates
firstpage: 18738
lastpage: 18748
page: 18738-18748
order: 18738
cycles: false
bibtex_author: Hooda, Ashish and Christodorescu, Mihai and Allamanis, Miltiadis and
  Wilson, Aaron and Fawaz, Kassem and Jha, Somesh
author:
- given: Ashish
  family: Hooda
- given: Mihai
  family: Christodorescu
- given: Miltiadis
  family: Allamanis
- given: Aaron
  family: Wilson
- given: Kassem
  family: Fawaz
- given: Somesh
  family: Jha
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/assets/hooda24a/hooda24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
