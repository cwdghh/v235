---
title: 'PolySketchFormer: Fast Transformers via Sketching Polynomial Kernels'
openreview: ghYrfdJfjK
abstract: The quadratic time and memory complexity inherent to self-attention mechanisms,
  with respect to sequence length, presents a critical computational bottleneck in
  the training and deployment of large-scale Transformer-based language models. Recent
  theoretical results indicate the intractability of sub-quadratic softmax attention
  approximation under reasonable complexity assumptions. This paper addresses this
  challenge by first demonstrating that polynomial attention with high degree can
  effectively replace softmax without sacrificing model quality. Next, we develop
  polynomial sketching techniques from numerical linear algebra to achieve linear-time
  polynomial attention with approximation guarantees. Crucially, our approach achieves
  this speedup without requiring the sparsification of attention matrices. We also
  present a block-based algorithm to apply causal masking efficiently. Combining these
  techniques, we provide <em>PolySketchFormer</em>, a practical linear-time Transformer
  architecture for language modeling that offers provable guarantees. We validate
  PolySketchFormer empirically by training language models capable of handling long
  contexts. These experiments utilize both synthetic and real-world datasets (PG19,
  Wikipedia and C4) on Google Cloud TPUs. For context lengths of 32k and GPT-2 style
  models, our model achieves 2x speedup in training compared to FlashAttention of
  the fastest configuration, with no observed degradation in quality across our experiments.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: kacham24a
month: 0
tex_title: "{P}oly{S}ketch{F}ormer: Fast Transformers via Sketching Polynomial Kernels"
firstpage: 22748
lastpage: 22770
page: 22748-22770
order: 22748
cycles: false
bibtex_author: Kacham, Praneeth and Mirrokni, Vahab and Zhong, Peilin
author:
- given: Praneeth
  family: Kacham
- given: Vahab
  family: Mirrokni
- given: Peilin
  family: Zhong
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://raw.githubusercontent.com/mlresearch/v235/main/assets/kacham24a/kacham24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
