---
title: Variance-reduced Zeroth-Order Methods for Fine-Tuning Language Models
openreview: VHO4nE7v41
abstract: Fine-tuning language models (LMs) has demonstrated success in a wide array
  of downstream tasks. However, as LMs are scaled up, the memory requirements for
  backpropagation become prohibitively high. Zeroth-order (ZO) optimization methods
  can leverage memory-efficient forward passes to estimate gradients. More recently,
  MeZO, an adaptation of ZO-SGD, has been shown to consistently outperform zero-shot
  and in-context learning when combined with suitable task prompts. In this work,
  we couple ZO methods with variance reduction techniques to enhance stability and
  convergence for inference-based LM fine-tuning. We introduce Memory-Efficient Zeroth-Order
  Stochastic Variance-Reduced Gradient (MeZO-SVRG) and demonstrate its efficacy across
  multiple LM fine-tuning tasks, eliminating the reliance on task-specific prompts.
  Evaluated across a range of both masked and autoregressive LMs on benchmark GLUE
  tasks, MeZO-SVRG outperforms MeZO with up to 20% increase in test accuracies in
  both full- and partial-parameter fine-tuning settings. MeZO-SVRG benefits from reduced
  computation time as it often surpasses MeZO’s peak test accuracy with a $2\times$
  reduction in GPU-hours. MeZO-SVRG significantly reduces the required memory footprint
  compared to first-order SGD, i.e. by $2\times$ for autoregressive models. Our experiments
  highlight that MeZO-SVRG’s memory savings progressively improve compared to SGD
  with larger batch sizes.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: gautam24a
month: 0
tex_title: Variance-reduced Zeroth-Order Methods for Fine-Tuning Language Models
firstpage: 15180
lastpage: 15208
page: 15180-15208
order: 15180
cycles: false
bibtex_author: Gautam, Tanmay and Park, Youngsuk and Zhou, Hao and Raman, Parameswaran
  and Ha, Wooseok
author:
- given: Tanmay
  family: Gautam
- given: Youngsuk
  family: Park
- given: Hao
  family: Zhou
- given: Parameswaran
  family: Raman
- given: Wooseok
  family: Ha
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/assets/gautam24a/gautam24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
