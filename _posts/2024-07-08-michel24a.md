---
title: Rethinking Momentum Knowledge Distillation in Online Continual Learning
openreview: UW5nO9NGjt
abstract: Online Continual Learning (OCL) addresses the problem of training neural
  networks on a continuous data stream where multiple classification tasks emerge
  in sequence. In contrast to offline Continual Learning, data can be seen only once
  in OCL, which is a very severe constraint. In this context, replay-based strategies
  have achieved impressive results and most state-of-the-art approaches heavily depend
  on them. While Knowledge Distillation (KD) has been extensively used in offline
  Continual Learning, it remains under-exploited in OCL, despite its high potential.
  In this paper, we analyze the challenges in applying KD to OCL and give empirical
  justifications. We introduce a direct yet effective methodology for applying Momentum
  Knowledge Distillation (MKD) to many flagship OCL methods and demonstrate its capabilities
  to enhance existing approaches. In addition to improving existing state-of-the-art
  accuracy by more than $10%$ points on ImageNet100, we shed light on MKD internal
  mechanics and impacts during training in OCL. We argue that similar to replay, MKD
  should be considered a central component of OCL. The code is available at https://github.com/Nicolas1203/mkd_ocl.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: michel24a
month: 0
tex_title: Rethinking Momentum Knowledge Distillation in Online Continual Learning
firstpage: 35607
lastpage: 35622
page: 35607-35622
order: 35607
cycles: false
bibtex_author: Michel, Nicolas and Wang, Maorong and Xiao, Ling and Yamasaki, Toshihiko
author:
- given: Nicolas
  family: Michel
- given: Maorong
  family: Wang
- given: Ling
  family: Xiao
- given: Toshihiko
  family: Yamasaki
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/michel24a/michel24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
