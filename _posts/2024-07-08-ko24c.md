---
title: 'DistiLLM: Towards Streamlined Distillation for Large Language Models'
openreview: lsHZNNoC7r
abstract: 'Knowledge distillation (KD) is widely used for compressing a teacher model
  to a smaller student model, reducing its inference cost and memory footprint while
  preserving model capabilities. However, current KD methods for auto-regressive sequence
  models (e.g., large language models) suffer from missing a standardized objective
  function. Moreover, the recent use of student-generated outputs to address training-inference
  mismatches has significantly escalated computational costs. To tackle these issues,
  we introduce DistiLLM, a more effective and efficient KD framework for auto-regressive
  language models. DistiLLM comprises two components: (1) a novel skew Kullback-Leibler
  divergence loss, where we unveil and leverage its theoretical properties, and (2)
  an adaptive off-policy approach designed to enhance the efficiency in utilizing
  student-generated outputs. Extensive experiments, including instruction-following
  tasks, demonstrate the effectiveness of DistiLLM in building high-performing student
  models while achieving up to 4.3$\times$ speedup compared to recent KD methods.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: ko24c
month: 0
tex_title: "{D}isti{LLM}: Towards Streamlined Distillation for Large Language Models"
firstpage: 24872
lastpage: 24895
page: 24872-24895
order: 24872
cycles: false
bibtex_author: Ko, Jongwoo and Kim, Sungnyun and Chen, Tianyi and Yun, Se-Young
author:
- given: Jongwoo
  family: Ko
- given: Sungnyun
  family: Kim
- given: Tianyi
  family: Chen
- given: Se-Young
  family: Yun
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/ko24c/ko24c.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
