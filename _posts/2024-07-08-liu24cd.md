---
title: 'DFD: Distillng the Feature Disparity Differently for Detectors'
openreview: KI3JKFKciG
abstract: Knowledge distillation is a widely adopted model compression technique that
  has been successfully applied to object detection. In feature distillation, it is
  common practice for the student model to imitate the feature responses of the teacher
  model, with the underlying objective of improving its own abilities by reducing
  the disparity with the teacher. However, it is crucial to recognize that the disparities
  between the student and teacher are inconsistent, highlighting their varying abilities.
  In this paper, we explore the inconsistency in the disparity between teacher and
  student feature maps and analyze their impact on the efficiency of the distillation.
  We find that regions with varying degrees of difference should be treated separately,
  with different distillation constraints applied accordingly. We introduce our distillation
  method called Disparity Feature Distillation(DFD). The core idea behind DFD is to
  apply different treatments to regions with varying learning difficulties, simultaneously
  incorporating leniency and strictness. It enables the student to better assimilate
  the teacherâ€™s knowledge. Through extensive experiments, we demonstrate the effectiveness
  of our proposed DFD in achieving significant improvements. For instance, when applied
  to detectors based on ResNet50 such as RetinaNet, FasterRCNN, and RepPoints, our
  method enhances their mAP from 37.4%, 38.4%, 38.6% to 41.7%, 42.4%, 42.7%, respectively.
  Our approach also demonstrates substantial improvements on YOLO and ViT-based models.
  The code is available at https://github.com/luckin99/DFD.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: liu24cd
month: 0
tex_title: "{DFD}: Distillng the Feature Disparity Differently for Detectors"
firstpage: 32421
lastpage: 32430
page: 32421-32430
order: 32421
cycles: false
bibtex_author: Liu, Kang and Zhang, Yingyi and Zhang, Jingyun and Li, Jinmin and Wang,
  Jun and Wang, Shaoming and Yuan, Chun and Guo, Rizen
author:
- given: Kang
  family: Liu
- given: Yingyi
  family: Zhang
- given: Jingyun
  family: Zhang
- given: Jinmin
  family: Li
- given: Jun
  family: Wang
- given: Shaoming
  family: Wang
- given: Chun
  family: Yuan
- given: Rizen
  family: Guo
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://raw.githubusercontent.com/mlresearch/v235/main/assets/liu24cd/liu24cd.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
