---
title: 'Position: Data Authenticity, Consent, & Provenance for AI are all broken:
  what will it take to fix them?'
openreview: 3hSTecKy1b
abstract: New capabilities in foundation models are owed in large part to massive,
  widely-sourced, and under-documented training data collections. Existing practices
  in data collection have led to challenges in tracing authenticity, verifying consent,
  preserving privacy, addressing representation and bias, respecting copyright, and
  overall developing ethical and trustworthy foundation models. In response, regulation
  is emphasizing the need for training data transparency to understand foundation
  modelsâ€™ limitations. Based on a large-scale analysis of the foundation model training
  data landscape and existing solutions, we identify the missing infrastructure to
  facilitate responsible foundation model development practices. We examine the current
  shortcomings of common tools for tracing data authenticity, consent, and documentation,
  and outline how policymakers, developers, and data creators can facilitate responsible
  foundation model development by adopting universal data provenance standards.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: longpre24b
month: 0
tex_title: 'Position: Data Authenticity, Consent, & Provenance for {AI} are all broken:
  what will it take to fix them?'
firstpage: 32711
lastpage: 32725
page: 32711-32725
order: 32711
cycles: false
bibtex_author: Longpre, Shayne and Mahari, Robert and Obeng-Marnu, Naana and Brannon,
  William and South, Tobin and Gero, Katy Ilonka and Pentland, Alex and Kabbara, Jad
author:
- given: Shayne
  family: Longpre
- given: Robert
  family: Mahari
- given: Naana
  family: Obeng-Marnu
- given: William
  family: Brannon
- given: Tobin
  family: South
- given: Katy Ilonka
  family: Gero
- given: Alex
  family: Pentland
- given: Jad
  family: Kabbara
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://raw.githubusercontent.com/mlresearch/v235/main/assets/longpre24b/longpre24b.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
