---
title: Vector Quantization Pretraining for EEG Time Series with Random Projection
  and Phase Alignment
openreview: 7uwLvFvpis
abstract: In this paper, we propose a BERT-style self-supervised learning model, VQ-MTM
  (Vector Quantization Masked Time-Series Modeling), for the EEG time series data
  analysis. At its core, VQ-MTM comprises a theoretically grounded random-projection
  quantization module and a phase-aligning module guided by the Time-Phase-Shift Equivariance
  of Fourier Transform, the two modules can generate well-defined semantic units (akin
  to words in natural language) for the corrupted and periodic time series, thus offering
  robust and consistent learning signals for the EEG self-supervised learning. VQ-MTM
  also owns low model complexity and can easily adapt to large-scale datasets. We
  conduct experiments on five real-world datasets including two large-scale datasets
  to verify the efficacy of our proposed model, the experiment results show that VQ-MTM
  is able to consistently surpass the existing methods by large margins on both seizure
  detection and classification tasks. Our code is available at https://github.com/HaokunGUI/VQ_MTM.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: gui24a
month: 0
tex_title: Vector Quantization Pretraining for {EEG} Time Series with Random Projection
  and Phase Alignment
firstpage: 16731
lastpage: 16750
page: 16731-16750
order: 16731
cycles: false
bibtex_author: Gui, Haokun and Li, Xiucheng and Chen, Xinyang
author:
- given: Haokun
  family: Gui
- given: Xiucheng
  family: Li
- given: Xinyang
  family: Chen
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://raw.githubusercontent.com/mlresearch/v235/main/assets/gui24a/gui24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
