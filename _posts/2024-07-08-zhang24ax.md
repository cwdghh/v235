---
title: Riemannian Preconditioned LoRA for Fine-Tuning Foundation Models
openreview: IwqE4QqBew
abstract: Low-Rank Adaptation (LoRA) emerges as a popular parameter-efficient fine-tuning
  (PEFT) method, which proposes to freeze pretrained model weights and update an additive
  low-rank trainable matrix. In this work, we study the enhancement of LoRA training
  by introducing an $r\times r$ preconditioner in each gradient step where $r$ is
  the LoRA rank. We theoretically verify that the proposed preconditioner stabilizes
  feature learning with LoRA under infinite-width NN setting. Empirically, the implementation
  of this new preconditioner requires a small change to existing optimizer code and
  creates virtually minuscule storage and runtime overhead. Our experimental results
  with both large language models and text-to-image diffusion models show that with
  this new preconditioner, the convergence and reliability of SGD and AdamW can be
  significantly enhanced. Moreover, the training process becomes much more robust
  to hyperparameter choices such as learning rate. The new preconditioner can be derived
  from a novel Riemannian metric in low-rank matrix field.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: zhang24ax
month: 0
tex_title: "{R}iemannian Preconditioned {L}o{RA} for Fine-Tuning Foundation Models"
firstpage: 59641
lastpage: 59669
page: 59641-59669
order: 59641
cycles: false
bibtex_author: Zhang, Fangzhao and Pilanci, Mert
author:
- given: Fangzhao
  family: Zhang
- given: Mert
  family: Pilanci
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://raw.githubusercontent.com/mlresearch/v235/main/assets/zhang24ax/zhang24ax.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
