---
title: Reflective Policy Optimization
openreview: Cs0Xy6WETl
abstract: On-policy reinforcement learning methods, like Trust Region Policy Optimization
  (TRPO) and Proximal Policy Optimization (PPO), often demand extensive data per update,
  leading to sample inefficiency. This paper introduces Reflective Policy Optimization
  (RPO), a novel on-policy extension that amalgamates past and future state-action
  information for policy optimization. This approach empowers the agent for introspection,
  allowing modifications to its actions within the current state. Theoretical analysis
  confirms that policy performance is monotonically improved and contracts the solution
  space, consequently expediting the convergence procedure. Empirical results demonstrate
  RPOâ€™s feasibility and efficacy in two reinforcement learning benchmarks, culminating
  in superior sample efficiency. The source code of this work is available at https://github.com/Edgargan/RPO.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: gan24b
month: 0
tex_title: Reflective Policy Optimization
firstpage: 14471
lastpage: 14490
page: 14471-14490
order: 14471
cycles: false
bibtex_author: Gan, Yaozhong and Yan, Renye and Wu, Zhe and Xing, Junliang
author:
- given: Yaozhong
  family: Gan
- given: Renye
  family: Yan
- given: Zhe
  family: Wu
- given: Junliang
  family: Xing
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://raw.githubusercontent.com/mlresearch/v235/main/assets/gan24b/gan24b.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
