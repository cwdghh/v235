---
title: Langevin Policy for Safe Reinforcement Learning
openreview: xgoilgLPGD
abstract: Optimization and sampling based algorithms are two branches of methods in
  machine learning, while existing safe reinforcement learning (RL) algorithms are
  mainly based on optimization, it is still unclear whether sampling based methods
  can lead to desirable performance with safe policy. This paper formulates the Langevin
  policy for safe RL, and proposes Langevin Actor-Critic (LAC) to accelerate the process
  of policy inference. Concretely, instead of parametric policy, the proposed Langevin
  policy provides a stochastic process that directly infers actions, which is the
  numerical solver to the Langevin dynamic of actions on the continuous time. Furthermore,
  to make Langevin policy practical on RL tasks, the proposed LAC accumulates the
  transitions induced by Langevin policy and reproduces them with a generator. Finally,
  extensive empirical results show the effectiveness and superiority of LAC on the
  MuJoCo-based and Safety Gym tasks.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: lei24a
month: 0
tex_title: "{L}angevin Policy for Safe Reinforcement Learning"
firstpage: 27174
lastpage: 27190
page: 27174-27190
order: 27174
cycles: false
bibtex_author: Lei, Fenghao and Yang, Long and Wen, Shiting and Huang, Zhixiong and
  Zhang, Zhiwang and Pang, Chaoyi
author:
- given: Fenghao
  family: Lei
- given: Long
  family: Yang
- given: Shiting
  family: Wen
- given: Zhixiong
  family: Huang
- given: Zhiwang
  family: Zhang
- given: Chaoyi
  family: Pang
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/assets/lei24a/lei24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
