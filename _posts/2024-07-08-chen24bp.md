---
title: What Can Transformer Learn with Varying Depth? Case Studies on Sequence Learning
  Tasks
openreview: YNbCbcGyXE
abstract: We study the capabilities of the transformer architecture with varying depth.
  Specifically, we designed a novel set of sequence learning tasks to systematically
  evaluate and comprehend how the depth of transformer affects its ability to perform
  memorization, reasoning, generalization, and contextual generalization. We show
  a transformer with only one attention layer can excel in memorization but falls
  short in other tasks. Then, we show that exhibiting reasoning and generalization
  ability requires the transformer to have at least two attention layers, while context
  generalization ability may necessitate three attention layers. Additionally, we
  identify a class of simple operations that a single attention layer can execute,
  and show that the complex tasks can be approached as the combinations of these simple
  operations and thus can be resolved by stacking multiple attention layers. This
  sheds light on studying more practical and complex tasks beyond our design. Numerical
  experiments corroborate our theoretical findings.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: chen24bp
month: 0
tex_title: What Can Transformer Learn with Varying Depth? {C}ase Studies on Sequence
  Learning Tasks
firstpage: 7972
lastpage: 8001
page: 7972-8001
order: 7972
cycles: false
bibtex_author: Chen, Xingwu and Zou, Difan
author:
- given: Xingwu
  family: Chen
- given: Difan
  family: Zou
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/assets/chen24bp/chen24bp.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
