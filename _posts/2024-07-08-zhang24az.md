---
title: Enhancing Storage and Computational Efficiency in Federated Multimodal Learning
  for Large-Scale Models
openreview: QgvBcOsF4B
abstract: The remarkable generalization of large-scale models has recently gained
  significant attention in multimodal research. However, deploying heterogeneous large-scale
  models with different modalities under Federated Learning (FL) to protect data privacy
  imposes tremendous challenges on clients’ limited computation and storage. In this
  work, we propose M$^2$FedSA to address the above issue. We realize modularized decomposition
  of large-scale models via Split Learning (SL) and only retain privacy-sensitive
  modules on clients, alleviating storage overhead. By freezing large-scale models
  and introducing two specialized lightweight adapters, the models can better focus
  on task-specific knowledge and enhance modality-specific knowledge, improving the
  model’s adaptability to different tasks while balancing efficiency. In addition,
  M$^2$FedSA further improves performance by transferring multimodal knowledge to
  unimodal clients at both the feature and decision levels, which leverages the complementarity
  of different modalities. Extensive experiments on various multimodal classification
  tasks validate the effectiveness of our proposed M$^2$FedSA. The code is made available
  publicly at https://github.com/M2FedSA/M-2FedSA.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: zhang24az
month: 0
tex_title: Enhancing Storage and Computational Efficiency in Federated Multimodal
  Learning for Large-Scale Models
firstpage: 59685
lastpage: 59699
page: 59685-59699
order: 59685
cycles: false
bibtex_author: Zhang, Zixin and Qi, Fan and Xu, Changsheng
author:
- given: Zixin
  family: Zhang
- given: Fan
  family: Qi
- given: Changsheng
  family: Xu
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://raw.githubusercontent.com/mlresearch/v235/main/assets/zhang24az/zhang24az.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
