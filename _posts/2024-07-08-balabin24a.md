---
title: Disentanglement Learning via Topology
openreview: q0lxAs5GGO
abstract: We propose TopDis (Topological Disentanglement), a method for learning disentangled
  representations via adding a multi-scale topological loss term. Disentanglement
  is a crucial property of data representations substantial for the explainability
  and robustness of deep learning models and a step towards high-level cognition.
  The state-of-the-art methods are based on VAE and encourage the joint distribution
  of latent variables to be factorized. We take a different perspective on disentanglement
  by analyzing topological properties of data manifolds. In particular, we optimize
  the topological similarity for data manifolds traversals. To the best of our knowledge,
  our paper is the first one to propose a differentiable topological loss for disentanglement
  learning. Our experiments have shown that the proposed TopDis loss improves disentanglement
  scores such as MIG, FactorVAE score, SAP score, and DCI disentanglement score with
  respect to state-of-the-art results while preserving the reconstruction quality.
  Our method works in an unsupervised manner, permitting us to apply it to problems
  without labeled factors of variation. The TopDis loss works even when factors of
  variation are correlated. Additionally, we show how to use the proposed topological
  loss to find disentangled directions in a trained GAN.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: balabin24a
month: 0
tex_title: Disentanglement Learning via Topology
firstpage: 2474
lastpage: 2504
page: 2474-2504
order: 2474
cycles: false
bibtex_author: Balabin, Nikita and Voronkova, Daria and Trofimov, Ilya and Burnaev,
  Evgeny and Barannikov, Serguei
author:
- given: Nikita
  family: Balabin
- given: Daria
  family: Voronkova
- given: Ilya
  family: Trofimov
- given: Evgeny
  family: Burnaev
- given: Serguei
  family: Barannikov
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://raw.githubusercontent.com/mlresearch/v235/main/assets/balabin24a/balabin24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
