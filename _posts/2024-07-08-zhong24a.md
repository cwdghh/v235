---
title: 'ERQ: Error Reduction for Post-Training Quantization of Vision Transformers'
openreview: jKUWlgra9b
abstract: Post-training quantization (PTQ) for vision transformers (ViTs) has garnered
  significant attention due to its efficiency in compressing models. However, existing
  methods typically overlook the intricate interdependence between quantized weight
  and activation, leading to considerable quantization error. In this paper, we propose
  ERQ, a two-step PTQ approach meticulously crafted to sequentially reduce the quantization
  error arising from activation and weight quantization. ERQ first introduces Activation
  quantization error reduction (Aqer) that strategically formulates the minimization
  of activation quantization error as a Ridge Regression problem, tackling it by updating
  weights with full-precision. Subsequently, ERQ introduces Weight quantization error
  reduction (Wqer) that adopts an iterative approach to mitigate the quantization
  error induced by weight quantization. In each iteration, an empirically derived,
  efficient proxy is employed to refine the rounding directions of quantized weights,
  coupled with a Ridge Regression solver to curtail weight quantization error. Experimental
  results attest to the effectiveness of our approach. Notably, ERQ surpasses the
  state-of-the-art GPTQ by 22.36% in accuracy for W3A4 ViT-S.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: zhong24a
month: 0
tex_title: "{ERQ}: Error Reduction for Post-Training Quantization of Vision Transformers"
firstpage: 61664
lastpage: 61680
page: 61664-61680
order: 61664
cycles: false
bibtex_author: Zhong, Yunshan and Hu, Jiawei and Huang, You and Zhang, Yuxin and Ji,
  Rongrong
author:
- given: Yunshan
  family: Zhong
- given: Jiawei
  family: Hu
- given: You
  family: Huang
- given: Yuxin
  family: Zhang
- given: Rongrong
  family: Ji
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/assets/zhong24a/zhong24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
