---
title: 'Libra: Building Decoupled Vision System on Large Language Models'
openreview: F1drhMjN7s
abstract: In this work, we introduce <b>Libra</b>, a prototype model with a decoupled
  vision system on a large language model (LLM). The decoupled vision system decouples
  inner-modal modeling and cross-modal interaction, yielding unique visual information
  modeling and effective cross-modal comprehension. Libra is trained through discrete
  auto-regressive modeling on both vision and language inputs. Specifically, we incorporate
  a routed visual expert with a cross-modal bridge module into a pretrained LLM to
  route the vision and language flows during attention computing to enable different
  attention patterns in inner-modal modeling and cross-modal interaction scenarios.
  Experimental results demonstrate that the dedicated design of Libra achieves a strong
  MLLM baseline that rivals existing works in the image-to-text scenario with merely
  50 million training data, providing a new perspective for future multimodal foundation
  models. Code is available at https://github.com/YifanXu74/Libra.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: xu24ab
month: 0
tex_title: 'Libra: Building Decoupled Vision System on Large Language Models'
firstpage: 55371
lastpage: 55388
page: 55371-55388
order: 55371
cycles: false
bibtex_author: Xu, Yifan and Yang, Xiaoshan and Song, Yaguang and Xu, Changsheng
author:
- given: Yifan
  family: Xu
- given: Xiaoshan
  family: Yang
- given: Yaguang
  family: Song
- given: Changsheng
  family: Xu
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/xu24ab/xu24ab.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
