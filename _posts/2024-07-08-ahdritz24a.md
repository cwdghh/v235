---
title: Distinguishing the Knowable from the Unknowable with Language Models
openreview: ud4GSrqUKI
abstract: 'We study the feasibility of identifying <em>epistemic</em> uncertainty
  (reflecting a lack of knowledge), as opposed to <em>aleatoric</em> uncertainty (reflecting
  entropy in the underlying distribution), in the outputs of large language models
  (LLMs) over free-form text. In the absence of ground-truth probabilities, we explore
  a setting where, in order to (approximately) disentangle a given LLMâ€™s uncertainty,
  a significantly larger model stands in as a proxy for the ground truth. We show
  that small linear probes trained on the embeddings of frozen, pretrained models
  accurately predict when larger models will be more confident at the token level
  and that probes trained on one text domain generalize to others. Going further,
  we propose a fully unsupervised method that achieves non-trivial accuracy on the
  same task. Taken together, we interpret these results as evidence that LLMs naturally
  contain internal representations of different types of uncertainty that could potentially
  be leveraged to devise more informative indicators of model confidence in diverse
  practical settings. Code can be found at: https://github.com/KempnerInstitute/llm_uncertainty'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: ahdritz24a
month: 0
tex_title: Distinguishing the Knowable from the Unknowable with Language Models
firstpage: 503
lastpage: 549
page: 503-549
order: 503
cycles: false
bibtex_author: Ahdritz, Gustaf and Qin, Tian and Vyas, Nikhil and Barak, Boaz and
  Edelman, Benjamin L.
author:
- given: Gustaf
  family: Ahdritz
- given: Tian
  family: Qin
- given: Nikhil
  family: Vyas
- given: Boaz
  family: Barak
- given: Benjamin L.
  family: Edelman
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/ahdritz24a/ahdritz24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
