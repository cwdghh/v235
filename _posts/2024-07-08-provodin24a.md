---
title: 'Efficient Exploration in Average-Reward Constrained Reinforcement Learning:
  Achieving Near-Optimal Regret With Posterior Sampling'
openreview: njpTpkvUbO
abstract: We present a new algorithm based on posterior sampling for learning in Constrained
  Markov Decision Processes (CMDP) in the infinite-horizon undiscounted setting. The
  algorithm achieves near-optimal regret bounds while being advantageous empirically
  compared to the existing algorithms. Our main theoretical result is a Bayesian regret
  bound for each cost component of $\tilde{O} (DS\sqrt{AT})$ for any communicating
  CMDP with $S$ states, $A$ actions, and diameter $D$. This regret bound matches the
  lower bound in order of time horizon $T$ and is the best-known regret bound for
  communicating CMDPs achieved by a computationally tractable algorithm. Empirical
  results show that our posterior sampling algorithm outperforms the existing algorithms
  for constrained reinforcement learning.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: provodin24a
month: 0
tex_title: 'Efficient Exploration in Average-Reward Constrained Reinforcement Learning:
  Achieving Near-Optimal Regret With Posterior Sampling'
firstpage: 41144
lastpage: 41162
page: 41144-41162
order: 41144
cycles: false
bibtex_author: Provodin, Danil and Kaptein, Maurits Clemens and Pechenizkiy, Mykola
author:
- given: Danil
  family: Provodin
- given: Maurits Clemens
  family: Kaptein
- given: Mykola
  family: Pechenizkiy
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/provodin24a/provodin24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
