---
title: 'Beyond Chinchilla-Optimal: Accounting for Inference in Language Model Scaling
  Laws'
openreview: 0bmXrtTDUu
abstract: Large language model (LLM) scaling laws are empirical formulas that estimate
  changes in model quality as a result of increasing parameter count and training
  data. However, these formulas, including the popular Deepmind Chinchilla scaling
  laws, neglect to include the cost of inference. We modify the Chinchilla scaling
  laws to calculate the optimal LLM parameter count and pre-training data size to
  train and deploy a model of a given quality and inference demand. We conduct our
  analysis both in terms of a compute budget and real-world costs and find that LLM
  researchers expecting reasonably large inference demand ($\sim$1B requests) should
  train models smaller and longer than Chinchilla-optimal. Furthermore, we train 47
  models of varying sizes and parameter counts to validate our formula and find that
  model quality continues to improve as we scale tokens per parameter to extreme ranges
  (up to 10,000). Finally, we ablate the procedure used to fit the Chinchilla scaling
  law coefficients and find that developing scaling laws only from data collected
  at typical token/parameter ratios overestimates the impact of additional tokens
  at these extreme ranges.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: sardana24a
month: 0
tex_title: 'Beyond Chinchilla-Optimal: Accounting for Inference in Language Model
  Scaling Laws'
firstpage: 43445
lastpage: 43460
page: 43445-43460
order: 43445
cycles: false
bibtex_author: Sardana, Nikhil and Portes, Jacob and Doubov, Sasha and Frankle, Jonathan
author:
- given: Nikhil
  family: Sardana
- given: Jacob
  family: Portes
- given: Sasha
  family: Doubov
- given: Jonathan
  family: Frankle
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/sardana24a/sardana24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
