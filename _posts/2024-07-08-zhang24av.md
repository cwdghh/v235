---
title: 'Sparsest Models Elude Pruning: An Exposé of Pruning’s Current Capabilities'
openreview: DRGgT7SyC7
abstract: Pruning has emerged as a promising approach for compressing large-scale
  models, yet its effectiveness in recovering the sparsest of models has not yet been
  explored. We conducted an extensive series of 485,838 experiments, applying a range
  of state-of-the-art pruning algorithms to a synthetic dataset we created, named
  the Cubist Spiral. Our findings reveal a significant gap in performance compared
  to ideal sparse networks, which we identified through a novel combinatorial search
  algorithm. We attribute this performance gap to current pruning algorithms’ poor
  behaviour under overparameterization, their tendency to induce disconnected paths
  throughout the network, and their propensity to get stuck at suboptimal solutions,
  even when given the optimal width and initialization. This gap is concerning, given
  the simplicity of the network architectures and datasets used in our study. We hope
  that our research encourages further investigation into new pruning techniques that
  strive for true network sparsity.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: zhang24av
month: 0
tex_title: 'Sparsest Models Elude Pruning: An Exposé of Pruning’s Current Capabilities'
firstpage: 59576
lastpage: 59600
page: 59576-59600
order: 59576
cycles: false
bibtex_author: Zhang, Stephen and Papyan, Vardan
author:
- given: Stephen
  family: Zhang
- given: Vardan
  family: Papyan
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/assets/zhang24av/zhang24av.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
