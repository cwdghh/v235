---
title: 'Code as Reward: Empowering Reinforcement Learning with VLMs'
openreview: 6P88DMUDvH
abstract: Pre-trained Vision-Language Models (VLMs) are able to understand visual
  concepts, describe and decompose complex tasks into sub-tasks, and provide feedback
  on task completion. In this paper, we aim to leverage these capabilities to support
  the training of reinforcement learning (RL) agents. In principle, VLMs are well
  suited for this purpose, as they can naturally analyze image-based observations
  and provide feedback (reward) on learning progress. However, inference in VLMs is
  computationally expensive, so querying them frequently to compute rewards would
  significantly slowdown the training of an RL agent. To address this challenge, we
  propose a framework named Code as Reward (VLM-CaR). VLM-CaR produces dense reward
  functions from VLMs through code generation, thereby significantly reducing the
  computational burden of querying the VLM directly. We show that the dense rewards
  generated through our approach are very accurate across a diverse set of discrete
  and continuous environments, and can be more effective in training RL policies than
  the original sparse environment rewards.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: venuto24a
month: 0
tex_title: 'Code as Reward: Empowering Reinforcement Learning with {VLM}s'
firstpage: 49368
lastpage: 49387
page: 49368-49387
order: 49368
cycles: false
bibtex_author: Venuto, David and Islam, Mohammad Sami Nur and Klissarov, Martin and
  Precup, Doina and Yang, Sherry and Anand, Ankit
author:
- given: David
  family: Venuto
- given: Mohammad Sami Nur
  family: Islam
- given: Martin
  family: Klissarov
- given: Doina
  family: Precup
- given: Sherry
  family: Yang
- given: Ankit
  family: Anand
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/assets/venuto24a/venuto24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
