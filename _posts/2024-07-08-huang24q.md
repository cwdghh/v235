---
title: 'BiLLM: Pushing the Limit of Post-Training Quantization for LLMs'
openreview: qOl2WWOqFg
abstract: Pretrained large language models (LLMs) exhibit exceptional general language
  processing capabilities but come with significant demands on memory and computational
  resources. As a powerful compression technology, binarization can extremely reduce
  model weights to a mere 1 bit, lowering the expensive computation and memory requirements.
  However, existing quantization techniques fall short of maintaining LLM performance
  under ultra-low bit-widths. In response to this challenge, we present BiLLM, a groundbreaking
  1-bit post-training quantization scheme tailored for pretrained LLMs. Based on the
  weight distribution of LLMs, BiLLM first identifies and structurally selects salient
  weights, and minimizes the compression loss through an effective binary residual
  approximation strategy. Moreover, considering the bell-shaped distribution of the
  non-salient weights, we propose an optimal splitting search to group and binarize
  them accurately. BiLLM, for the first time, achieves high-accuracy inference (e.g.
  8.41 perplexity on LLaMA2-70B) with only 1.08-bit weights across various LLM families
  and evaluation metrics, outperforms SOTA quantization methods of LLM by significant
  margins. Moreover, BiLLM enables the binarization process of a 7-billion LLM within
  0.5 hours on a single GPU, demonstrating satisfactory time efficiency. Our code
  is available at https://github.com/Aaronhuang-778/BiLLM .
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: huang24q
month: 0
tex_title: "{B}i{LLM}: Pushing the Limit of Post-Training Quantization for {LLM}s"
firstpage: 20023
lastpage: 20042
page: 20023-20042
order: 20023
cycles: false
bibtex_author: Huang, Wei and Liu, Yangdong and Qin, Haotong and Li, Ying and Zhang,
  Shiming and Liu, Xianglong and Magno, Michele and Qi, Xiaojuan
author:
- given: Wei
  family: Huang
- given: Yangdong
  family: Liu
- given: Haotong
  family: Qin
- given: Ying
  family: Li
- given: Shiming
  family: Zhang
- given: Xianglong
  family: Liu
- given: Michele
  family: Magno
- given: Xiaojuan
  family: Qi
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/huang24q/huang24q.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
