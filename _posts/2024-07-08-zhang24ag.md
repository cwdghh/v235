---
title: Conditional Language Learning with Context
openreview: jXn1qIcjyG
abstract: Language models can learn sophisticated language understanding skills from
  fitting raw text. They also unselectively learn useless corpus statistics and biases,
  especially during finetuning on domain-specific corpora. In this paper, we propose
  a simple modification to causal language modeling called conditional finetuning,
  which performs language modeling conditioned on a context. We show that a context
  can "explain away" certain corpus statistics and make the model avoid learning them.
  In this fashion, conditional finetuning achieves selective learning from a corpus,
  learning knowledge useful for downstream tasks while avoiding learning useless corpus
  statistics like topic biases. This selective learning effect leads to less forgetting
  and better stability-plasticity tradeoff in domain finetuning, potentially benefitting
  lifelong learning with language models.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: zhang24ag
month: 0
tex_title: Conditional Language Learning with Context
firstpage: 59247
lastpage: 59263
page: 59247-59263
order: 59247
cycles: false
bibtex_author: Zhang, Xiao and Li, Miao and Wu, Ji
author:
- given: Xiao
  family: Zhang
- given: Miao
  family: Li
- given: Ji
  family: Wu
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/assets/zhang24ag/zhang24ag.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
