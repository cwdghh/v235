---
title: 'QuIP$#$: Even Better LLM Quantization with Hadamard Incoherence and Lattice
  Codebooks'
openreview: 9BrydUVcoe
abstract: 'Post-training quantization (PTQ) reduces the memory footprint of LLMs by
  quantizing their weights to low-precision. In this work, we introduce QuIP#, a weight-only
  PTQ method that achieves state-of-the-art results in extreme compression regimes
  ($\le$ 4 bits per weight) using three novel techniques. First, QuIP# improves QuIPâ€™s
  (Chee et al., 2023) incoherence processing by using the randomized Hadamard transform,
  which is faster and has better theoretical properties. Second, QuIP# uses vector
  quantization to take advantage of the ball-shaped sub-Gaussian distribution that
  incoherent weights possess: specifically, we introduce a set of hardware-efficient
  codebooks based on the highly symmetric $E_8$ lattice, which achieves the optimal
  8-dimension unit ball packing. Third, QuIP# uses fine-tuning to improve fidelity
  to the original model. Our experiments show that QuIP# outperforms existing PTQ
  methods, enables new behaviors in PTQ scaling, and supports fast inference. Our
  code can be found at https://github.com/Cornell-RelaxML/quip-sharp.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: tseng24a
month: 0
tex_title: "{Q}u{IP}$#$: Even Better {LLM} Quantization with Hadamard Incoherence
  and Lattice Codebooks"
firstpage: 48630
lastpage: 48656
page: 48630-48656
order: 48630
cycles: false
bibtex_author: Tseng, Albert and Chee, Jerry and Sun, Qingyao and Kuleshov, Volodymyr
  and De Sa, Christopher
author:
- given: Albert
  family: Tseng
- given: Jerry
  family: Chee
- given: Qingyao
  family: Sun
- given: Volodymyr
  family: Kuleshov
- given: Christopher
  family: De Sa
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/assets/tseng24a/tseng24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
