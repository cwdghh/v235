---
title: Self-Correcting Self-Consuming Loops for Generative Model Training
openreview: i0nVanexij
abstract: As synthetic data becomes higher quality and proliferates on the internet,
  machine learning models are increasingly trained on a mix of human- and machine-generated
  data. Despite the successful stories of using synthetic data for representation
  learning, using synthetic data for generative model training creates “self-consuming
  loops” which may lead to training instability or even collapse, unless certain conditions
  are met. Our paper aims to stabilize self-consuming generative model training. Our
  theoretical results demonstrate that by introducing an idealized correction function,
  which maps a data point to be more likely under the true data distribution, self-consuming
  loops can be made <em>exponentially</em> more stable. We then propose self-correction
  functions, which rely on expert knowledge (e.g. the laws of physics programmed in
  a simulator), and aim to approximate the idealized corrector automatically and at
  scale. We empirically validate the effectiveness of self-correcting self-consuming
  loops on the challenging human motion synthesis task, and observe that it successfully
  avoids model collapse, even when the ratio of synthetic data to real data is as
  high as 100%.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: gillman24a
month: 0
tex_title: Self-Correcting Self-Consuming Loops for Generative Model Training
firstpage: 15646
lastpage: 15677
page: 15646-15677
order: 15646
cycles: false
bibtex_author: Gillman, Nate and Freeman, Michael and Aggarwal, Daksh and Hsu, Chia-Hong
  and Luo, Calvin and Tian, Yonglong and Sun, Chen
author:
- given: Nate
  family: Gillman
- given: Michael
  family: Freeman
- given: Daksh
  family: Aggarwal
- given: Chia-Hong
  family: Hsu
- given: Calvin
  family: Luo
- given: Yonglong
  family: Tian
- given: Chen
  family: Sun
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/assets/gillman24a/gillman24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
