---
title: Language Models as Science Tutors
openreview: WFyolnFZOR
abstract: NLP has recently made exciting progress toward training language models
  (LMs) with strong scientific problem-solving skills. However, model development
  has not focused on real-life use-cases of LMs for science, including applications
  in education that require processing long scientific documents. To address this,
  we introduce TutorEval and TutorChat. TutorEval is a diverse question-answering
  benchmark consisting of questions about long chapters from STEM textbooks, written
  by experts. TutorEval helps measure real-life usability of LMs as scientific assistants,
  and it is the first benchmark combining long contexts, free-form generation, and
  multi-disciplinary scientific knowledge. Moreover, we show that fine-tuning base
  models with existing dialogue datasets leads to poor performance on TutorEval. Therefore,
  we create TutorChat, a dataset of 80,000 long synthetic dialogues about textbooks.
  We use TutorChat to fine-tune Llemma models with 7B and 34B parameters. These LM
  tutors specialized in math have a 32K-token context window, and they excel at TutorEval
  while performing strongly on GSM8K and MATH. Our datasets build on open-source materials,
  and we release our models, data, and evaluations publicly.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: chevalier24a
month: 0
tex_title: Language Models as Science Tutors
firstpage: 8310
lastpage: 8335
page: 8310-8335
order: 8310
cycles: false
bibtex_author: Chevalier, Alexis and Geng, Jiayi and Wettig, Alexander and Chen, Howard
  and Mizera, Sebastian and Annala, Toni and Aragon, Max and Fanlo, Arturo Rodriguez
  and Frieder, Simon and Machado, Simon and Prabhakar, Akshara and Thieu, Ellie and
  Wang, Jiachen T. and Wang, Zirui and Wu, Xindi and Xia, Mengzhou and Xia, Wenhan
  and Yu, Jiatong and Zhu, Junjie and Ren, Zhiyong and Arora, Sanjeev and Chen, Danqi
author:
- given: Alexis
  family: Chevalier
- given: Jiayi
  family: Geng
- given: Alexander
  family: Wettig
- given: Howard
  family: Chen
- given: Sebastian
  family: Mizera
- given: Toni
  family: Annala
- given: Max
  family: Aragon
- given: Arturo Rodriguez
  family: Fanlo
- given: Simon
  family: Frieder
- given: Simon
  family: Machado
- given: Akshara
  family: Prabhakar
- given: Ellie
  family: Thieu
- given: Jiachen T.
  family: Wang
- given: Zirui
  family: Wang
- given: Xindi
  family: Wu
- given: Mengzhou
  family: Xia
- given: Wenhan
  family: Xia
- given: Jiatong
  family: Yu
- given: Junjie
  family: Zhu
- given: Zhiyong
  family: Ren
- given: Sanjeev
  family: Arora
- given: Danqi
  family: Chen
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://raw.githubusercontent.com/mlresearch/v235/main/assets/chevalier24a/chevalier24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
