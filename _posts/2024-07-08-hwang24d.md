---
title: 'EVEREST: Efficient Masked Video Autoencoder by Removing Redundant Spatiotemporal
  Tokens'
openreview: nn5OPHom8t
abstract: Masked Video Autoencoder (MVA) approaches have demonstrated their potential
  by significantly outperforming previous video representation learning methods. However,
  they waste an excessive amount of computations and memory in predicting uninformative
  tokens/frames due to random masking strategies. (e.g., over 16 nodes with 128 NVIDIA
  A100 GPUs). To resolve this issue, we exploit the unequal information density among
  the patches in videos and propose EVEREST, a surprisingly efficient MVA approach
  for video representation learning that finds tokens containing rich motion features
  and discards uninformative ones during both pre-training and fine-tuning. We further
  present an information-intensive frame selection strategy that allows the model
  to focus on informative and causal frames with minimal redundancy. Our method significantly
  reduces the computation and memory requirements of MVA, enabling the pre-training
  and fine-tuning on a single machine with 8 GPUs while achieving comparable performance
  to computation- and memory-heavy baselines on multiple benchmarks and the uncurated
  Ego4D dataset. We hope that our work contributes to reducing the barrier to further
  research on video understanding.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: hwang24d
month: 0
tex_title: "{EVEREST}: Efficient Masked Video Autoencoder by Removing Redundant Spatiotemporal
  Tokens"
firstpage: 20889
lastpage: 20907
page: 20889-20907
order: 20889
cycles: false
bibtex_author: Hwang, Sunil and Yoon, Jaehong and Lee, Youngwan and Hwang, Sung Ju
author:
- given: Sunil
  family: Hwang
- given: Jaehong
  family: Yoon
- given: Youngwan
  family: Lee
- given: Sung Ju
  family: Hwang
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://raw.githubusercontent.com/mlresearch/v235/main/assets/hwang24d/hwang24d.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
