---
title: 'DoRA: Weight-Decomposed Low-Rank Adaptation'
openreview: 3d5CIRG1n2
abstract: Among the widely used parameter-efficient fine-tuning (PEFT) methods, LoRA
  and its variants have gained considerable popularity because of avoiding additional
  inference costs. However, there still often exists an accuracy gap between these
  methods and full fine-tuning (FT). In this work, we first introduce a novel weight
  decomposition analysis to investigate the inherent differences between FT and LoRA.
  Aiming to resemble the learning capacity of FT from the findings, we propose Weight-Decomposed
  Low-Rank Adaptation (DoRA). DoRA decomposes the pre-trained weight into two components,
  magnitude and direction, for fine-tuning, specifically employing LoRA for directional
  updates to efficiently minimize the number of trainable parameters. By employing
  DoRA, we enhance both the learning capacity and training stability of LoRA while
  avoiding any additional inference overhead. DoRA consistently outperforms LoRA on
  fine-tuning LLaMA, LLaVA, and VL-BART on various downstream tasks, such as commonsense
  reasoning, visual instruction tuning, and image/video-text understanding. The code
  is available at https://github.com/NVlabs/DoRA.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: liu24bn
month: 0
tex_title: "{D}o{RA}: Weight-Decomposed Low-Rank Adaptation"
firstpage: 32100
lastpage: 32121
page: 32100-32121
order: 32100
cycles: false
bibtex_author: Liu, Shih-Yang and Wang, Chien-Yi and Yin, Hongxu and Molchanov, Pavlo
  and Wang, Yu-Chiang Frank and Cheng, Kwang-Ting and Chen, Min-Hung
author:
- given: Shih-Yang
  family: Liu
- given: Chien-Yi
  family: Wang
- given: Hongxu
  family: Yin
- given: Pavlo
  family: Molchanov
- given: Yu-Chiang Frank
  family: Wang
- given: Kwang-Ting
  family: Cheng
- given: Min-Hung
  family: Chen
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/assets/liu24bn/liu24bn.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
