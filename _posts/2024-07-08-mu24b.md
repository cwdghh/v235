---
title: On the Second-Order Convergence of Biased Policy Gradient Algorithms
openreview: RfsagmV1AG
abstract: Since the objective functions of reinforcement learning problems are typically
  highly nonconvex, it is desirable that policy gradient, the most popular algorithm,
  escapes saddle points and arrives at second-order stationary points. Existing results
  only consider vanilla policy gradient algorithms with unbiased gradient estimators,
  but practical implementations under the infinite-horizon discounted reward setting
  are biased due to finite-horizon sampling. Moreover, actor-critic methods, whose
  second-order convergence has not yet been established, are also biased due to the
  critic approximation of the value function. We provide a novel second-order analysis
  of biased policy gradient methods, including the vanilla gradient estimator computed
  from Monte-Carlo sampling of trajectories as well as the double-loop actor-critic
  algorithm, where in the inner loop the critic improves the approximation of the
  value function via TD(0) learning. Separately, we also establish the convergence
  of TD(0) on Markov chains irrespective of initial state distribution.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: mu24b
month: 0
tex_title: On the Second-Order Convergence of Biased Policy Gradient Algorithms
firstpage: 36455
lastpage: 36485
page: 36455-36485
order: 36455
cycles: false
bibtex_author: Mu, Siqiao and Klabjan, Diego
author:
- given: Siqiao
  family: Mu
- given: Diego
  family: Klabjan
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/assets/mu24b/mu24b.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
