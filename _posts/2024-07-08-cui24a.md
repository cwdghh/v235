---
title: Major-Minor Mean Field Multi-Agent Reinforcement Learning
openreview: mslTE1qgLa
abstract: Multi-agent reinforcement learning (MARL) remains difficult to scale to
  many agents. Recent MARL using Mean Field Control (MFC) provides a tractable and
  rigorous approach to otherwise difficult cooperative MARL. However, the strict MFC
  assumption of many independent, weakly-interacting agents is too inflexible in practice.
  We generalize MFC to instead simultaneously model many similar and few complex agents
  â€“ as Major-Minor Mean Field Control (M3FC). Theoretically, we give approximation
  results for finite agent control, and verify the sufficiency of stationary policies
  for optimality together with a dynamic programming principle. Algorithmically, we
  propose Major-Minor Mean Field MARL (M3FMARL) for finite agent systems instead of
  the limiting system. The algorithm is shown to approximate the policy gradient of
  the underlying M3FC MDP. Finally, we demonstrate its capabilities experimentally
  in various scenarios. We observe a strong performance in comparison to state-of-the-art
  policy gradient MARL methods.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: cui24a
month: 0
tex_title: Major-Minor Mean Field Multi-Agent Reinforcement Learning
firstpage: 9603
lastpage: 9632
page: 9603-9632
order: 9603
cycles: false
bibtex_author: Cui, Kai and Fabian, Christian and Tahir, Anam and Koeppl, Heinz
author:
- given: Kai
  family: Cui
- given: Christian
  family: Fabian
- given: Anam
  family: Tahir
- given: Heinz
  family: Koeppl
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/assets/cui24a/cui24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
