---
title: 'ConvNet vs Transformer, Supervised vs CLIP: Beyond ImageNet Accuracy'
openreview: 9BGi9PEhNn
abstract: 'Modern computer vision offers a great variety of models to practitioners,
  and selecting a model from multiple options for specific applications can be challenging.
  Conventionally, competing model architectures and training protocols are compared
  by their classification accuracy on ImageNet. However, this single metric does not
  fully capture performance nuances critical for specialized tasks. In this work,
  we conduct an in-depth comparative analysis of model behaviors beyond ImageNet accuracy,
  for both ConvNet and Vision Transformer architectures, each across supervised and
  CLIP training paradigms. Although our selected models have similar ImageNet accuracies
  and compute requirements, we find that they differ in many other aspects: types
  of mistakes, output calibration, transferability, and feature invariance, among
  others. This diversity in model characteristics, not captured by traditional metrics,
  highlights the need for more nuanced analysis when choosing among different models.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: vishniakov24a
month: 0
tex_title: "{C}onv{N}et vs Transformer, Supervised vs {CLIP}: Beyond {I}mage{N}et
  Accuracy"
firstpage: 49545
lastpage: 49557
page: 49545-49557
order: 49545
cycles: false
bibtex_author: Vishniakov, Kirill and Shen, Zhiqiang and Liu, Zhuang
author:
- given: Kirill
  family: Vishniakov
- given: Zhiqiang
  family: Shen
- given: Zhuang
  family: Liu
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://raw.githubusercontent.com/mlresearch/v235/main/assets/vishniakov24a/vishniakov24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
