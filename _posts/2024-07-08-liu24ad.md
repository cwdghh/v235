---
title: 'StyDeSty: Min-Max Stylization and Destylization for Single Domain Generalization'
openreview: DBlkjCDg2i
abstract: Single domain generalization (single DG) aims at learning a robust model
  generalizable to unseen domains from only one training domain, making it a highly
  ambitious and challenging task. State-of-the-art approaches have mostly relied on
  data augmentations, such as adversarial perturbation and style enhancement, to synthesize
  new data and thus increase robustness. Nevertheless, they have largely overlooked
  the underlying coherence between the augmented domains, which in turn leads to inferior
  results in real-world scenarios. In this paper, we propose a simple yet effective
  scheme, termed as <em>StyDeSty</em>, to explicitly account for the alignment of
  the source and pseudo domains in the process of data augmentation, enabling them
  to interact with each other in a self-consistent manner and further giving rise
  to a latent domain with strong generalization power. The heart of StyDeSty lies
  in the interaction between a <em>stylization</em> module for generating novel stylized
  samples using the source domain, and a <em>destylization</em> module for transferring
  stylized and source samples to a latent domain to learn content-invariant features.
  The stylization and destylization modules work adversarially and reinforce each
  other. During inference, the destylization module transforms the input sample with
  an arbitrary style shift to the latent domain, in which the downstream tasks are
  carried out. Specifically, the location of the destylization layer within the backbone
  network is determined by a dedicated neural architecture search (NAS) strategy.
  We evaluate StyDeSty on multiple benchmarks and demonstrate that it yields encouraging
  results, outperforming the state of the art by up to 13.44% on classification accuracy.
  Codes are available https://github.com/Huage001/StyDeSty.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: liu24ad
month: 0
tex_title: "{S}ty{D}e{S}ty: Min-Max Stylization and Destylization for Single Domain
  Generalization"
firstpage: 31293
lastpage: 31311
page: 31293-31311
order: 31293
cycles: false
bibtex_author: Liu, Songhua and Jin, Xin and Yang, Xingyi and Ye, Jingwen and Wang,
  Xinchao
author:
- given: Songhua
  family: Liu
- given: Xin
  family: Jin
- given: Xingyi
  family: Yang
- given: Jingwen
  family: Ye
- given: Xinchao
  family: Wang
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/assets/liu24ad/liu24ad.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
