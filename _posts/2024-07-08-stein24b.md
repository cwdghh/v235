---
title: Towards Compositionality in Concept Learning
openreview: upO8FUwf92
abstract: Concept-based interpretability methods offer a lens into the internals of
  foundation models by decomposing their embeddings into high-level concepts. These
  concept representations are most useful when they are <em>compositional</em>, meaning
  that the individual concepts compose to explain the full sample. We show that existing
  unsupervised concept extraction methods find concepts which are not compositional.
  To automatically discover compositional concept representations, we identify two
  salient properties of such representations, and propose Compositional Concept Extraction
  (CCE) for finding concepts which obey these properties. We evaluate CCE on five
  different datasets over image and text data. Our evaluation shows that CCE finds
  more compositional concept representations than baselines and yields better accuracy
  on four downstream classification tasks.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: stein24b
month: 0
tex_title: Towards Compositionality in Concept Learning
firstpage: 46530
lastpage: 46555
page: 46530-46555
order: 46530
cycles: false
bibtex_author: Stein, Adam and Naik, Aaditya and Wu, Yinjun and Naik, Mayur and Wong,
  Eric
author:
- given: Adam
  family: Stein
- given: Aaditya
  family: Naik
- given: Yinjun
  family: Wu
- given: Mayur
  family: Naik
- given: Eric
  family: Wong
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://raw.githubusercontent.com/mlresearch/v235/main/assets/stein24b/stein24b.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
