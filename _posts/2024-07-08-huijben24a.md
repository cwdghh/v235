---
title: Residual Quantization with Implicit Neural Codebooks
openreview: NBAc36V00H
abstract: Vector quantization is a fundamental operation for data compression and
  vector search. To obtain high accuracy, multi-codebook methods represent each vector
  using codewords across several codebooks. Residual quantization (RQ) is one such
  method, which iteratively quantizes the error of the previous step. While the error
  distribution is dependent on previously-selected codewords, this dependency is not
  accounted for in conventional RQ as it uses a fixed codebook per quantization step.
  In this paper, we propose QINCo, a neural RQ variant that constructs specialized
  codebooks per step that depend on the approximation of the vector from previous
  steps. Experiments show that QINCo outperforms state-of-the-art methods by a large
  margin on several datasets and code sizes. For example, QINCo achieves better nearest-neighbor
  search accuracy using 12-byte codes than the state-of-the-art UNQ using 16 bytes
  on the BigANN1M and Deep1M datasets.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: huijben24a
month: 0
tex_title: Residual Quantization with Implicit Neural Codebooks
firstpage: 20682
lastpage: 20699
page: 20682-20699
order: 20682
cycles: false
bibtex_author: Huijben, Iris A.M. and Douze, Matthijs and Muckley, Matthew J. and
  Van Sloun, Ruud and Verbeek, Jakob
author:
- given: Iris A.M.
  family: Huijben
- given: Matthijs
  family: Douze
- given: Matthew J.
  family: Muckley
- given: Ruud
  family: Van Sloun
- given: Jakob
  family: Verbeek
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/huijben24a/huijben24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
