---
title: Revisiting the Power of Prompt for Visual Tuning
openreview: 2Y93PtAqCl
abstract: Visual prompt tuning (VPT) is a promising solution incorporating learnable
  prompt tokens to customize pre-trained models for downstream tasks. However, VPT
  and its variants often encounter challenges like prompt initialization, prompt length,
  and subpar performance in self-supervised pretraining, hindering successful contextual
  adaptation. This study commences by exploring the correlation evolvement between
  prompts and patch tokens during proficient training. Inspired by the observation
  that the prompt tokens tend to share high mutual information with patch tokens,
  we propose initializing prompts with downstream token prototypes. The strategic
  initialization, a stand-in for the previous initialization, substantially improves
  performance. To refine further, we optimize token construction with a streamlined
  pipeline that maintains excellent performance with almost no increase in computational
  expenses compared to VPT. Exhaustive experiments show our proposed approach outperforms
  existing methods by a remarkable margin. For instance, after MAE pre-training, our
  method improves accuracy by up to 10%$\sim$30% compared to VPT, and outperforms
  Full fine-tuning 19 out of 24 cases while using less than 0.4% of learnable parameters.
  Besides, the experimental results demonstrate the proposed SPT is robust to prompt
  lengths and scales well with model capacity and training data size. We finally provide
  an insightful exploration into the amount of target data facilitating the adaptation
  of pre-trained models to downstream tasks. The code is available at https://github.com/WangYZ1608/Self-Prompt-Tuning.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: wang24i
month: 0
tex_title: Revisiting the Power of Prompt for Visual Tuning
firstpage: 50233
lastpage: 50247
page: 50233-50247
order: 50233
cycles: false
bibtex_author: Wang, Yuzhu and Cheng, Lechao and Fang, Chaowei and Zhang, Dingwen
  and Duan, Manni and Wang, Meng
author:
- given: Yuzhu
  family: Wang
- given: Lechao
  family: Cheng
- given: Chaowei
  family: Fang
- given: Dingwen
  family: Zhang
- given: Manni
  family: Duan
- given: Meng
  family: Wang
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://raw.githubusercontent.com/mlresearch/v235/main/assets/wang24i/wang24i.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
