---
title: Purify Unlearnable Examples via Rate-Constrained Variational Autoencoders
openreview: 0LBNdbmQCM
abstract: Unlearnable examples (UEs) seek to maximize testing error by making subtle
  modifications to training examples that are correctly labeled. Defenses against
  these poisoning attacks can be categorized based on whether specific interventions
  are adopted during training. The first approach is training-time defense, such as
  adversarial training, which can mitigate poisoning effects but is computationally
  intensive. The other approach is pre-training purification, e.g., image short squeezing,
  which consists of several simple compressions but often encounters challenges in
  dealing with various UEs. Our work provides a novel disentanglement mechanism to
  build an efficient pre-training purification method. Firstly, we uncover rate-constrained
  variational autoencoders (VAEs), demonstrating a clear tendency to suppress the
  perturbations in UEs. We subsequently conduct a theoretical analysis for this phenomenon.
  Building upon these insights, we introduce a disentangle variational autoencoder
  (D-VAE), capable of disentangling the perturbations with learnable class-wise embeddings.
  Based on this network, a two-stage purification approach is naturally developed.
  The first stage focuses on roughly eliminating perturbations, while the second stage
  produces refined, poison-free results, ensuring effectiveness and robustness across
  various scenarios. Extensive experiments demonstrate the remarkable performance
  of our method across CIFAR-10, CIFAR-100, and a 100-class ImageNet-subset. Code
  is available at https://github.com/yuyi-sd/D-VAE.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: yu24m
month: 0
tex_title: Purify Unlearnable Examples via Rate-Constrained Variational Autoencoders
firstpage: 57678
lastpage: 57702
page: 57678-57702
order: 57678
cycles: false
bibtex_author: Yu, Yi and Wang, Yufei and Xia, Song and Yang, Wenhan and Lu, Shijian
  and Tan, Yap-Peng and Kot, Alex
author:
- given: Yi
  family: Yu
- given: Yufei
  family: Wang
- given: Song
  family: Xia
- given: Wenhan
  family: Yang
- given: Shijian
  family: Lu
- given: Yap-Peng
  family: Tan
- given: Alex
  family: Kot
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/yu24m/yu24m.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
