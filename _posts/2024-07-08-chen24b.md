---
title: How Interpretable Are Interpretable Graph Neural Networks?
openreview: F3G2udCF3Q
abstract: Interpretable graph neural networks (XGNNs ) are widely adopted in various
  scientific applications involving graph-structured data. Existing XGNNs predominantly
  adopt the attention-based mechanism to learn edge or node importance for extracting
  and making predictions with the interpretable subgraph. However, the representational
  properties and limitations of these methods remain inadequately explored. In this
  work, we present a theoretical framework that formulates interpretable subgraph
  learning with the multilinear extension of the subgraph distribution, coined as
  subgraph multilinear extension (SubMT). Extracting the desired interpretable subgraph
  requires an accurate approximation of SubMT, yet we find that the existing XGNNs
  can have a huge gap in fitting SubMT. Consequently, the SubMT approximation failure
  will lead to the degenerated interpretability of the extracted subgraphs. To mitigate
  the issue, we design a new XGNN architecture called Graph Multilinear neT (GMT),
  which is provably more powerful in approximating SubMT. We empirically validate
  our theoretical findings on a number of graph classification benchmarks. The results
  demonstrate that GMT outperforms the state-of-the-art up to 10% in terms of both
  interpretability and generalizability across 12 regular and geometric graph benchmarks.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: chen24b
month: 0
tex_title: How Interpretable Are Interpretable Graph Neural Networks?
firstpage: 6413
lastpage: 6456
page: 6413-6456
order: 6413
cycles: false
bibtex_author: Chen, Yongqiang and Bian, Yatao and Han, Bo and Cheng, James
author:
- given: Yongqiang
  family: Chen
- given: Yatao
  family: Bian
- given: Bo
  family: Han
- given: James
  family: Cheng
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://raw.githubusercontent.com/mlresearch/v235/main/assets/chen24b/chen24b.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
