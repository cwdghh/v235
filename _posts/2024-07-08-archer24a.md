---
title: Practical Performance Guarantees for Pipelined DNN Inference
openreview: S3xqyEaST9
abstract: We optimize pipeline parallelism for deep neural network (DNN) inference
  by partitioning model graphs into $k$ stages and minimizing the running time of
  the bottleneck stage, including communication. We give practical and effective algorithms
  for this NP-hard problem, but our emphasis is on tackling the practitionerâ€™s dilemma
  of deciding when a solution is good enough. To this end, we design novel mixed integer
  programming (MIP) relaxations for proving lower bounds. Applying these methods to
  a diverse testbed of 369 production models, for $k \in \\{2, 4, 8, 16, 32, 64\\}$,
  we empirically show that these lower bounds are strong enough to be useful in practice.
  Our lower bounds are substantially stronger than standard combinatorial bounds.
  For example, evaluated via geometric means across a production testbed with $k =
  16$ pipeline stages, our MIP formulations raise the lower bound from 0.4598 to 0.9452,
  expressed as a fraction of the best partition found. In other words, our improved
  lower bounds close the optimality gap by a factor of 9.855x.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: archer24a
month: 0
tex_title: Practical Performance Guarantees for Pipelined {DNN} Inference
firstpage: 1655
lastpage: 1671
page: 1655-1671
order: 1655
cycles: false
bibtex_author: Archer, Aaron and Fahrbach, Matthew and Liu, Kuikui and Prabhu, Prakash
author:
- given: Aaron
  family: Archer
- given: Matthew
  family: Fahrbach
- given: Kuikui
  family: Liu
- given: Prakash
  family: Prabhu
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://raw.githubusercontent.com/mlresearch/v235/main/assets/archer24a/archer24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
