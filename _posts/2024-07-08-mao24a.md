---
title: 'Position: Graph Foundation Models Are Already Here'
openreview: Edz0QXKKAo
abstract: Graph Foundation Models (GFMs) are emerging as a significant research topic
  in the graph domain, aiming to develop graph models trained on extensive and diverse
  data to enhance their applicability across various tasks and domains. Developing
  GFMs presents unique challenges over traditional Graph Neural Networks (GNNs), which
  are typically trained from scratch for specific tasks on particular datasets. The
  primary challenge in constructing GFMs lies in effectively leveraging vast and diverse
  graph data to achieve positive transfer. Drawing inspiration from existing foundation
  models in the CV and NLP domains, we propose a novel perspective for the GFM development
  by advocating for a "graph vocabulary‚Äù, in which the basic transferable units underlying
  graphs encode the invariance on graphs. We ground the graph vocabulary construction
  from essential aspects including network analysis, expressiveness, and stability.
  Such a vocabulary perspective can potentially advance the future GFM design in line
  with the neural scaling laws. All relevant resources with GFM design can be found
  here.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: mao24a
month: 0
tex_title: 'Position: Graph Foundation Models Are Already Here'
firstpage: 34670
lastpage: 34692
page: 34670-34692
order: 34670
cycles: false
bibtex_author: Mao, Haitao and Chen, Zhikai and Tang, Wenzhuo and Zhao, Jianan and
  Ma, Yao and Zhao, Tong and Shah, Neil and Galkin, Mikhail and Tang, Jiliang
author:
- given: Haitao
  family: Mao
- given: Zhikai
  family: Chen
- given: Wenzhuo
  family: Tang
- given: Jianan
  family: Zhao
- given: Yao
  family: Ma
- given: Tong
  family: Zhao
- given: Neil
  family: Shah
- given: Mikhail
  family: Galkin
- given: Jiliang
  family: Tang
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/mao24a/mao24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
