---
title: How do Transformers Perform In-Context Autoregressive Learning ?
openreview: kZbTkpnafR
abstract: Transformers have achieved state-of-the-art performance in language modeling
  tasks. However, the reasons behind their tremendous success are still unclear. In
  this paper, towards a better understanding, we train a Transformer model on a simple
  next token prediction task, where sequences are generated as a first-order autoregressive
  process $s_{t+1} = W s_t$. We show how a trained Transformer predicts the next token
  by first learning $W$ in-context, then applying a prediction mapping. We call the
  resulting procedure <em>in-context autoregressive learning</em>. More precisely,
  focusing on commuting orthogonal matrices $W$, we first show that a trained one-layer
  linear Transformer implements one step of gradient descent for the minimization
  of an inner objective function, when considering augmented tokens. When the tokens
  are not augmented, we characterize the global minima of a one-layer diagonal linear
  multi-head Transformer. Importantly, we exhibit orthogonality between heads and
  show that positional encoding captures trigonometric relations in the data. On the
  experimental side, we consider the general case of non-commuting orthogonal matrices
  and generalize our theoretical findings.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: sander24a
month: 0
tex_title: How do Transformers Perform In-Context Autoregressive Learning ?
firstpage: 43235
lastpage: 43254
page: 43235-43254
order: 43235
cycles: false
bibtex_author: Sander, Michael Eli and Giryes, Raja and Suzuki, Taiji and Blondel,
  Mathieu and Peyr\'{e}, Gabriel
author:
- given: Michael Eli
  family: Sander
- given: Raja
  family: Giryes
- given: Taiji
  family: Suzuki
- given: Mathieu
  family: Blondel
- given: Gabriel
  family: Peyr√©
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://raw.githubusercontent.com/mlresearch/v235/main/assets/sander24a/sander24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
