---
title: 'SparQ Attention: Bandwidth-Efficient LLM Inference'
openreview: OS5dqxmmtl
abstract: The computational difficulties of large language model (LLM) inference remain
  a significant obstacle to their widespread deployment. The need for many applications
  to support long input sequences and process them in large batches typically causes
  token-generation to be bottlenecked by data transfer. For this reason, we introduce
  <b>SparQ Attention</b>, a technique for increasing the inference throughput of LLMs
  by utilising memory bandwidth more efficiently within the attention layers, through
  selective fetching of the cached history. Our proposed technique can be applied
  directly to off-the-shelf LLMs during inference, without requiring any modification
  to the pre-training setup or additional fine-tuning. We show that SparQ Attention
  brings up to 8x savings in attention data transfers without substantial drops in
  accuracy, by evaluating Llama 2 and 3, Mistral, Gemma and Pythia models on a wide
  range of downstream tasks.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: ribar24a
month: 0
tex_title: "{S}par{Q} Attention: Bandwidth-Efficient {LLM} Inference"
firstpage: 42558
lastpage: 42583
page: 42558-42583
order: 42558
cycles: false
bibtex_author: Ribar, Luka and Chelombiev, Ivan and Hudlass-Galley, Luke and Blake,
  Charlie and Luschi, Carlo and Orr, Douglas
author:
- given: Luka
  family: Ribar
- given: Ivan
  family: Chelombiev
- given: Luke
  family: Hudlass-Galley
- given: Charlie
  family: Blake
- given: Carlo
  family: Luschi
- given: Douglas
  family: Orr
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/ribar24a/ribar24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
