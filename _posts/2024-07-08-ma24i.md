---
title: Do Transformer World Models Give Better Policy Gradients?
openreview: Uoved2xD81
abstract: 'A natural approach for reinforcement learning is to predict future rewards
  by unrolling a neural network world model, and to backpropagate through the resulting
  computational graph to learn a control policy. However, this method often becomes
  impractical for long horizons, since typical world models induce hard-to-optimize
  loss landscapes. Transformers are known to efficiently propagate gradients over
  long horizons: could they be the solution to this problem? Surprisingly, we show
  that commonly-used transformer world models produce circuitous gradient paths, which
  can be detrimental to long-range policy gradients. To tackle this challenge, we
  propose a class of world models called Action-conditioned World Models (AWMs), designed
  to provide more direct routes for gradient propagation. We integrate such AWMs into
  a policy gradient framework that underscores the relationship between network architectures
  and the policy gradient updates they inherently represent. We demonstrate that AWMs
  can generate optimization landscapes that are easier to navigate even when compared
  to those from the simulator itself. This property allows transformer AWMs to produce
  better policies than competitive baselines in realistic long-horizon tasks.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: ma24i
month: 0
tex_title: Do Transformer World Models Give Better Policy Gradients?
firstpage: 33855
lastpage: 33879
page: 33855-33879
order: 33855
cycles: false
bibtex_author: Ma, Michel and Ni, Tianwei and Gehring, Clement and D'Oro, Pierluca
  and Bacon, Pierre-Luc
author:
- given: Michel
  family: Ma
- given: Tianwei
  family: Ni
- given: Clement
  family: Gehring
- given: Pierluca
  family: Dâ€™Oro
- given: Pierre-Luc
  family: Bacon
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://raw.githubusercontent.com/mlresearch/v235/main/assets/ma24i/ma24i.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
