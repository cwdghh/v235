---
title: Policy Evaluation for Variance in Average Reward Reinforcement Learning
openreview: bID9PiBFpT
abstract: We consider an average reward reinforcement learning (RL) problem and work
  with asymptotic variance as a risk measure to model safety-critical applications.
  We design a temporal-difference (TD) type algorithm tailored for policy evaluation
  in this context. Our algorithm is based on linear stochastic approximation of an
  equivalent formulation of the asymptotic variance in terms of the solution of the
  Poisson equation. We consider both the tabular and linear function approximation
  settings, and establish $\tilde {O}(1/k)$ finite time convergence rate, where $k$
  is the number of steps of the algorithm. Our work paves the way for developing actor-critic
  style algorithms for variance-constrained RL. To the best of our knowledge, our
  result provides the first sequential estimator for asymptotic variance of a Markov
  chain with provable finite sample guarantees, which is of independent interest.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: agrawal24a
month: 0
tex_title: Policy Evaluation for Variance in Average Reward Reinforcement Learning
firstpage: 471
lastpage: 502
page: 471-502
order: 471
cycles: false
bibtex_author: Agrawal, Shubhada and A, Prashanth L and Maguluri, Siva Theja
author:
- given: Shubhada
  family: Agrawal
- given: Prashanth L
  family: A
- given: Siva Theja
  family: Maguluri
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/agrawal24a/agrawal24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
