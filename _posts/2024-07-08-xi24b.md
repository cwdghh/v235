---
title: 'Jetfire: Efficient and Accurate Transformer Pretraining with INT8 Data Flow
  and Per-Block Quantization'
openreview: ltzTHGFF5i
abstract: Pretraining transformers are generally time-consuming. Fully quantized training
  (FQT) is a promising approach to speed up pretraining. However, most FQT methods
  adopt a quantize-compute-dequantize procedure, which often leads to suboptimal speedup
  and significant performance degradation when used in transformers due to the high
  memory access overheads and low-precision computations. In this work, we propose
  Jetfire, an efficient and accurate INT8 training method specific to transformers.
  Our method features an INT8 data flow to optimize memory access and a per-block
  quantization method to maintain the accuracy of pretrained transformers. Extensive
  experiments demonstrate that our INT8 FQT method achieves comparable accuracy to
  the FP16 training baseline and outperforms the existing INT8 training works for
  transformers. Moreover, for a standard transformer block, our method offers an end-to-end
  training speedup of 1.42x and a 1.49x memory reduction compared to the FP16 baseline.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: xi24b
month: 0
tex_title: 'Jetfire: Efficient and Accurate Transformer Pretraining with {INT}8 Data
  Flow and Per-Block Quantization'
firstpage: 54049
lastpage: 54063
page: 54049-54063
order: 54049
cycles: false
bibtex_author: Xi, Haocheng and Chen, Yuxiang and Zhao, Kang and Teh, Kai Jun and
  Chen, Jianfei and Zhu, Jun
author:
- given: Haocheng
  family: Xi
- given: Yuxiang
  family: Chen
- given: Kang
  family: Zhao
- given: Kai Jun
  family: Teh
- given: Jianfei
  family: Chen
- given: Jun
  family: Zhu
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://raw.githubusercontent.com/mlresearch/v235/main/assets/xi24b/xi24b.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
