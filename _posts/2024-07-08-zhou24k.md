---
title: Bounding the Excess Risk for Linear Models Trained on Marginal-Preserving,
  Differentially-Private, Synthetic Data
openreview: XKxuTZRCXq
abstract: The growing use of machine learning (ML) has raised concerns that an ML
  model may reveal private information about an individual who has contributed to
  the training dataset. To prevent leakage of sensitive data, we consider using differentially-
  private (DP), synthetic training data instead of real training data to train an
  ML model. A key desirable property of synthetic data is its ability to preserve
  the low-order marginals of the original distribution. Our main contribution comprises
  novel upper and lower bounds on the excess empirical risk of linear models trained
  on such synthetic data, for continuous and Lipschitz loss functions. We perform
  extensive experimentation alongside our theoretical results.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: zhou24k
month: 0
tex_title: Bounding the Excess Risk for Linear Models Trained on Marginal-Preserving,
  Differentially-Private, Synthetic Data
firstpage: 61979
lastpage: 62001
page: 61979-62001
order: 61979
cycles: false
bibtex_author: Zhou, Yvonne and Liang, Mingyu and Brugere, Ivan and Dervovic, Danial
  and Polychroniadou, Antigoni and Wu, Min and Dachman-Soled, Dana
author:
- given: Yvonne
  family: Zhou
- given: Mingyu
  family: Liang
- given: Ivan
  family: Brugere
- given: Danial
  family: Dervovic
- given: Antigoni
  family: Polychroniadou
- given: Min
  family: Wu
- given: Dana
  family: Dachman-Soled
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/assets/zhou24k/zhou24k.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
