---
title: Scaling Laws for Fine-Grained Mixture of Experts
openreview: yoqdlynCRs
abstract: Mixture of Experts (MoE) models have emerged as a primary solution for reducing
  the computational cost of Large Language Models. In this work, we analyze their
  scaling properties, highlighting certain arbitrary assumptions present in the existing
  literature. In particular, we introduce a new hyperparameter, granularity, the modification
  of which allows for the optimal adjustment of the size of experts. Subsequently,
  we present scaling laws for fine-grained MoE, taking into account the number of
  training tokens, model size, and granularity. Using these scaling laws, we derive
  the optimal training configuration for a given computational budget. Furthermore,
  in contrast with previous works, we demonstrate that the gap in efficiency between
  dense and MoE models grows as we scale up the model size and training budget.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: ludziejewski24a
month: 0
tex_title: Scaling Laws for Fine-Grained Mixture of Experts
firstpage: 33270
lastpage: 33288
page: 33270-33288
order: 33270
cycles: false
bibtex_author: Ludziejewski, Jan and Krajewski, Jakub and Adamczewski, Kamil and Pi\'{o}ro,
  Maciej and Krutul, Micha{\l} and Antoniak, Szymon and Ciebiera, Kamil and Kr\'{o}l,
  Krystian and Odrzyg\'{o}\'{z}d\'{z}, Tomasz and Sankowski, Piotr and Cygan, Marek
  and Jaszczur, Sebastian
author:
- given: Jan
  family: Ludziejewski
- given: Jakub
  family: Krajewski
- given: Kamil
  family: Adamczewski
- given: Maciej
  family: Pióro
- given: Michał
  family: Krutul
- given: Szymon
  family: Antoniak
- given: Kamil
  family: Ciebiera
- given: Krystian
  family: Król
- given: Tomasz
  family: Odrzygóźdź
- given: Piotr
  family: Sankowski
- given: Marek
  family: Cygan
- given: Sebastian
  family: Jaszczur
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/ludziejewski24a/ludziejewski24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
