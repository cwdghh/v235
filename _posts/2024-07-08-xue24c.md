---
title: 'OpenMoE: An Early Effort on Open Mixture-of-Experts Language Models'
openreview: 1YDeZU8Lt5
abstract: 'To help the open-source community have a better understanding of Mixture-of-Experts
  (MoE) based large language models (LLMs), we train and release OpenMoE, a series
  of fully open-sourced and reproducible decoder-only MoE LLMs, ranging from 650M
  to 34B parameters and trained on up to over 1T tokens. Our investigation confirms
  that MoE-based LLMs can offer a more favorable cost-effectiveness trade-off than
  dense LLMs, highlighting the potential effectiveness for future LLM development.
  One more important contribution of this study is an in-depth analysis of the routing
  mechanisms within our OpenMoE models, leading to three significant findings: Context-Independent
  Specialization, Early Routing Learning, and Drop-towards-the-End. We discovered
  that routing decisions in MoE models are predominantly based on token IDs, with
  minimal context relevance. The token-to-expert assignments are determined early
  in the pre-training phase and remain largely unchanged. This imperfect routing can
  result in performance degradation, particularly in sequential tasks like multi-turn
  conversations, where tokens appearing later in a sequence are more likely to be
  dropped. Finally, we rethink our design based on the above-mentioned observations
  and analysis. To facilitate future MoE LLM development, we propose potential strategies
  for mitigating the issues we found and further improving off-the-shelf MoE LLM designs.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: xue24c
month: 0
tex_title: "{O}pen{M}o{E}: An Early Effort on Open Mixture-of-Experts Language Models"
firstpage: 55625
lastpage: 55655
page: 55625-55655
order: 55625
cycles: false
bibtex_author: Xue, Fuzhao and Zheng, Zian and Fu, Yao and Ni, Jinjie and Zheng, Zangwei
  and Zhou, Wangchunshu and You, Yang
author:
- given: Fuzhao
  family: Xue
- given: Zian
  family: Zheng
- given: Yao
  family: Fu
- given: Jinjie
  family: Ni
- given: Zangwei
  family: Zheng
- given: Wangchunshu
  family: Zhou
- given: Yang
  family: You
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://raw.githubusercontent.com/mlresearch/v235/main/assets/xue24c/xue24c.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
