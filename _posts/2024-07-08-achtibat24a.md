---
title: 'AttnLRP: Attention-Aware Layer-Wise Relevance Propagation for Transformers'
openreview: emtXYlBrNF
abstract: Large Language Models are prone to biased predictions and hallucinations,
  underlining the paramount importance of understanding their model-internal reasoning
  process. However, achieving faithful attributions for the entirety of a black-box
  transformer model and maintaining computational efficiency is an unsolved challenge.
  By extending the Layer-wise Relevance Propagation attribution method to handle attention
  layers, we address these challenges effectively. While partial solutions exist,
  our method is the first to faithfully and holistically attribute not only input
  but also latent representations of transformer models with the computational efficiency
  similar to a single backward pass. Through extensive evaluations against existing
  methods on LLaMa 2, Mixtral 8x7b, Flan-T5 and vision transformer architectures,
  we demonstrate that our proposed approach surpasses alternative methods in terms
  of faithfulness and enables the understanding of latent representations, opening
  up the door for concept-based explanations. We provide an LRP library at https://github.com/rachtibat/LRP-eXplains-Transformers.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: achtibat24a
month: 0
tex_title: "{A}ttn{LRP}: Attention-Aware Layer-Wise Relevance Propagation for Transformers"
firstpage: 135
lastpage: 168
page: 135-168
order: 135
cycles: false
bibtex_author: Achtibat, Reduan and Hatefi, Sayed Mohammad Vakilzadeh and Dreyer,
  Maximilian and Jain, Aakriti and Wiegand, Thomas and Lapuschkin, Sebastian and Samek,
  Wojciech
author:
- given: Reduan
  family: Achtibat
- given: Sayed Mohammad Vakilzadeh
  family: Hatefi
- given: Maximilian
  family: Dreyer
- given: Aakriti
  family: Jain
- given: Thomas
  family: Wiegand
- given: Sebastian
  family: Lapuschkin
- given: Wojciech
  family: Samek
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/achtibat24a/achtibat24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
