---
title: A Provably Effective Method for Pruning Experts in Fine-tuned Sparse Mixture-of-Experts
openreview: 1oU4FKpVx5
abstract: The sparsely gated mixture of experts (MoE) architecture sends different
  inputs to different subnetworks (experts), through trainable routers. MoE reduces
  the training computation significantly for large models, but its deployment can
  be still memory/computation expensive for some downstream tasks. Model pruning is
  a popular approach to reduce inference computation, but its application in MoE architecture
  is largely unexplored. To the best of our knowledge, this paper provides the first
  provably efficient technique for pruning experts in fine-tuned MoE models. We theoretically
  prove that prioritizing the pruning of the experts with a smaller change of the
  routerâ€™s $l_2$ norm from the pre-trained model guarantees the preservation of test
  accuracy, while significantly reducing the model size and the computational requirements.
  Although our theoretical analysis is centered on binary classification tasks on
  simplified MoE architecture, our expert pruning method is verified on large vision
  MoE models such as V-MoE and $\text{E}^3$-MoE fine-tuned on benchmark datasets such
  as CIFAR-10, CIFAR-100, and ImageNet.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: chowdhury24a
month: 0
tex_title: A Provably Effective Method for Pruning Experts in Fine-tuned Sparse Mixture-of-Experts
firstpage: 8815
lastpage: 8847
page: 8815-8847
order: 8815
cycles: false
bibtex_author: Chowdhury, Mohammed Nowaz Rabbani and Wang, Meng and El Maghraoui,
  Kaoutar and Wang, Naigang and Chen, Pin-Yu and Carothers, Christopher
author:
- given: Mohammed Nowaz Rabbani
  family: Chowdhury
- given: Meng
  family: Wang
- given: Kaoutar
  family: El Maghraoui
- given: Naigang
  family: Wang
- given: Pin-Yu
  family: Chen
- given: Christopher
  family: Carothers
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://raw.githubusercontent.com/mlresearch/v235/main/assets/chowdhury24a/chowdhury24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
