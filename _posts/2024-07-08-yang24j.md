---
title: 'Deeper or Wider: A Perspective from Optimal Generalization Error with Sobolev
  Loss'
openreview: IWi6iLZeRG
abstract: Constructing the architecture of a neural network is a challenging pursuit
  for the machine learning community, and the dilemma of whether to go deeper or wider
  remains a persistent question. This paper explores a comparison between deeper neural
  networks (DeNNs) with a flexible number of layers and wider neural networks (WeNNs)
  with limited hidden layers, focusing on their optimal generalization error in Sobolev
  losses. Analytical investigations reveal that the architecture of a neural network
  can be significantly influenced by various factors, including the number of sample
  points, parameters within the neural networks, and the regularity of the loss function.
  Specifically, a higher number of parameters tends to favor WeNNs, while an increased
  number of sample points and greater regularity in the loss function lean towards
  the adoption of DeNNs. We ultimately apply this theory to address partial differential
  equations using deep Ritz and physics-informed neural network (PINN) methods, guiding
  the design of neural networks.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: yang24j
month: 0
tex_title: 'Deeper or Wider: A Perspective from Optimal Generalization Error with
  Sobolev Loss'
firstpage: 56109
lastpage: 56138
page: 56109-56138
order: 56109
cycles: false
bibtex_author: Yang, Yahong and He, Juncai
author:
- given: Yahong
  family: Yang
- given: Juncai
  family: He
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://raw.githubusercontent.com/mlresearch/v235/main/assets/yang24j/yang24j.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
