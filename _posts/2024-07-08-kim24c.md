---
title: 'LayerMerge: Neural Network Depth Compression through Layer Pruning and Merging'
openreview: uDoy7AGvEC
abstract: Recent works show that reducing the number of layers in a convolutional
  neural network can enhance efficiency while maintaining the performance of the network.
  Existing depth compression methods remove redundant non-linear activation functions
  and merge the consecutive convolution layers into a single layer. However, these
  methods suffer from a critical drawback; the kernel size of the merged layers becomes
  larger, significantly undermining the latency reduction gained from reducing the
  depth of the network. We show that this problem can be addressed by jointly pruning
  convolution layers and activation functions. To this end, we propose <em>LayerMerge</em>,
  a novel depth compression method that selects which activation layers and convolution
  layers to remove, to achieve a desired inference speed-up while minimizing performance
  loss. Since the corresponding selection problem involves an exponential search space,
  we formulate a novel surrogate optimization problem and efficiently solve it via
  dynamic programming. Empirical results demonstrate that our method consistently
  outperforms existing depth compression and layer pruning methods on various network
  architectures, both on image classification and generation tasks. We release the
  code at https://github.com/snu-mllab/LayerMerge.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: kim24c
month: 0
tex_title: "{L}ayer{M}erge: Neural Network Depth Compression through Layer Pruning
  and Merging"
firstpage: 23825
lastpage: 23842
page: 23825-23842
order: 23825
cycles: false
bibtex_author: Kim, Jinuk and El Halabi, Marwa and Ji, Mingi and Song, Hyun Oh
author:
- given: Jinuk
  family: Kim
- given: Marwa
  family: El Halabi
- given: Mingi
  family: Ji
- given: Hyun Oh
  family: Song
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/kim24c/kim24c.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
