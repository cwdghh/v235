---
title: Self-Play Fine-Tuning Converts Weak Language Models to Strong Language Models
openreview: O4cHTxW9BS
abstract: Harnessing the power of human-annotated data through Supervised Fine-Tuning
  (SFT) is pivotal for advancing Large Language Models (LLMs). In this paper, we delve
  into the prospect of growing a strong LLM out of a weak one without the need for
  acquiring additional human-annotated data. We propose a new fine-tuning method called
  Self-Play fIne-tuNing (SPIN), which starts from a supervised fine-tuned model. At
  the heart of SPIN lies a self-play mechanism, where the LLM refines its capability
  by playing against instances of itself. More specifically, the LLM generates its
  own training data from its previous iterations, refining its policy by discerning
  these self-generated responses from those obtained from human-annotated data. Our
  method progressively elevates the LLM from a nascent model to a formidable one,
  unlocking the full potential of human-annotated demonstration data for SFT. Theoretically,
  we prove that the global optimum to the training objective function of our method
  is achieved only when the LLM policy aligns with the target data distribution. Empirically,
  we evaluate our method on several benchmark datasets including the HuggingFace Open
  LLM Leaderboard, MT-Bench, and datasets from Big-Bench. Our results show that SPIN
  can significantly improve the LLMâ€™s performance across a variety of benchmarks and
  even outperform models trained through direct preference optimization (DPO) supplemented
  with extra GPT-4 preference data. This sheds light on the promise of self-play,
  enabling the achievement of human-level performance in LLMs without the need for
  expert opponents.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: chen24j
month: 0
tex_title: Self-Play Fine-Tuning Converts Weak Language Models to Strong Language
  Models
firstpage: 6621
lastpage: 6642
page: 6621-6642
order: 6621
cycles: false
bibtex_author: Chen, Zixiang and Deng, Yihe and Yuan, Huizhuo and Ji, Kaixuan and
  Gu, Quanquan
author:
- given: Zixiang
  family: Chen
- given: Yihe
  family: Deng
- given: Huizhuo
  family: Yuan
- given: Kaixuan
  family: Ji
- given: Quanquan
  family: Gu
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/chen24j/chen24j.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
