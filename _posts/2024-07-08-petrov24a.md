---
title: Prompting a Pretrained Transformer Can Be a Universal Approximator
openreview: 3mQ6ZKTSQl
abstract: Despite the widespread adoption of prompting, prompt tuning and prefix-tuning
  of transformer models, our theoretical understanding of these fine-tuning methods
  remains limited. A key question is whether one can arbitrarily modify the behavior
  of a pretrained model by prompting or prefix-tuning it. Formally, whether prompting
  and prefix-tuning a pretrained model can universally approximate sequence-to-sequence
  functions. This paper answers in the affirmative and demonstrates that much smaller
  pretrained models than previously thought can be universal approximators when prefixed.
  In fact, prefix-tuning a single attention head is sufficient to approximate any
  continuous function making the attention mechanism uniquely suited for universal
  approximation. Moreover, any sequence-to-sequence function can be approximated by
  prefixing a transformer with depth linear in the sequence length. Beyond these density-type
  results, we also offer Jackson-type bounds on the length of the prefix needed to
  approximate a function to a desired precision.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: petrov24a
month: 0
tex_title: Prompting a Pretrained Transformer Can Be a Universal Approximator
firstpage: 40523
lastpage: 40550
page: 40523-40550
order: 40523
cycles: false
bibtex_author: Petrov, Aleksandar and Torr, Philip and Bibi, Adel
author:
- given: Aleksandar
  family: Petrov
- given: Philip
  family: Torr
- given: Adel
  family: Bibi
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/petrov24a/petrov24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
