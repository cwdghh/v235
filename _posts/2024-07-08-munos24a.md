---
title: Nash Learning from Human Feedback
openreview: Y5AmNYiyCQ
abstract: Reinforcement learning from human feedback (RLHF) has emerged as the main
  paradigm for aligning large language models (LLMs) with human preferences. Traditionally,
  RLHF involves the initial step of learning a reward model from pairwise human feedback,
  i.e., expressed as preferences between pairs of text generations. Subsequently,
  the LLM’s policy is fine-tuned to maximize the reward through a reinforcement learning
  algorithm. In this study, we introduce an alternative pipeline for the fine-tuning
  of LLMs using pairwise human feedback. Our approach entails the initial learning
  of a pairwise preference model, which is conditioned on two inputs (instead of a
  single input in the case of a reward model) given a prompt, followed by the pursuit
  of a policy that consistently generates responses preferred over those generated
  by any competing policy, thus defining the Nash equilibrium of this preference model.
  We term this approach Nash learning from human feedback (NLHF). In the context of
  a tabular policy representation, we present a novel algorithmic solution, Nash-MD,
  founded on the principles of mirror descent. This algorithm produces a sequence
  of policies, with the last iteration converging to the regularized Nash equilibrium.
  Additionally, we explore parametric representations of policies and introduce gradient
  descent algorithms for deep-learning architectures. We illustrate the effectiveness
  of our approach by presenting experimental results on a text summarization task.
  We believe NLHF offers a compelling avenue for fine-tuning LLMs and enhancing the
  alignment of LLMs with human preferences.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: munos24a
month: 0
tex_title: "{N}ash Learning from Human Feedback"
firstpage: 36743
lastpage: 36768
page: 36743-36768
order: 36743
cycles: false
bibtex_author: Munos, Remi and Valko, Michal and Calandriello, Daniele and Gheshlaghi
  Azar, Mohammad and Rowland, Mark and Guo, Zhaohan Daniel and Tang, Yunhao and Geist,
  Matthieu and Mesnard, Thomas and Fiegel, C\^{o}me and Michi, Andrea and Selvi, Marco
  and Girgin, Sertan and Momchev, Nikola and Bachem, Olivier and Mankowitz, Daniel
  J and Precup, Doina and Piot, Bilal
author:
- given: Remi
  family: Munos
- given: Michal
  family: Valko
- given: Daniele
  family: Calandriello
- given: Mohammad
  family: Gheshlaghi Azar
- given: Mark
  family: Rowland
- given: Zhaohan Daniel
  family: Guo
- given: Yunhao
  family: Tang
- given: Matthieu
  family: Geist
- given: Thomas
  family: Mesnard
- given: Côme
  family: Fiegel
- given: Andrea
  family: Michi
- given: Marco
  family: Selvi
- given: Sertan
  family: Girgin
- given: Nikola
  family: Momchev
- given: Olivier
  family: Bachem
- given: Daniel J
  family: Mankowitz
- given: Doina
  family: Precup
- given: Bilal
  family: Piot
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/assets/munos24a/munos24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
