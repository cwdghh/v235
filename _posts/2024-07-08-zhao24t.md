---
title: When Will Gradient Regularization Be Harmful?
openreview: 60vC1FY0dZ
abstract: Gradient regularization (GR), which aims to penalize the gradient norm atop
  the loss function, has shown promising results in training modern over-parameterized
  deep neural networks. However, can we trust this powerful technique? This paper
  reveals that GR can cause performance degeneration in adaptive optimization scenarios,
  particularly with learning rate warmup. Our empirical and theoretical analyses suggest
  this is due to GR inducing instability and divergence in gradient statistics of
  adaptive optimizers at the initial training stage. Inspired by the warmup heuristic,
  we propose three GR warmup strategies, each relaxing the regularization effect to
  a certain extent during the warmup course to ensure the accurate and stable accumulation
  of gradients. With experiments on Vision Transformer family, we confirm the three
  GR warmup strategies can effectively circumvent these issues, thereby largely improving
  the model performance. Meanwhile, we note that scalable models tend to rely more
  on the GR warmup, where the performance can be improved by up to 3% on Cifar10 compared
  to baseline GR. Code is available at https://github.com/zhaoyang-0204/gnp.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: zhao24t
month: 0
tex_title: When Will Gradient Regularization Be Harmful?
firstpage: 61144
lastpage: 61158
page: 61144-61158
order: 61144
cycles: false
bibtex_author: Zhao, Yang and Zhang, Hao and Hu, Xiuyuan
author:
- given: Yang
  family: Zhao
- given: Hao
  family: Zhang
- given: Xiuyuan
  family: Hu
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/assets/zhao24t/zhao24t.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
