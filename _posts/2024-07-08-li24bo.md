---
title: What Improves the Generalization of Graph Transformers? A Theoretical Dive
  into the Self-attention and Positional Encoding
openreview: mJhXlsZzzE
abstract: Graph Transformers, which incorporate self-attention and positional encoding,
  have recently emerged as a powerful architecture for various graph learning tasks.
  Despite their impressive performance, the complex non-convex interactions across
  layers and the recursive graph structure have made it challenging to establish a
  theoretical foundation for learning and generalization. This study introduces the
  first theoretical investigation of a shallow Graph Transformer for semi-supervised
  node classification, comprising a self-attention layer with relative positional
  encoding and a two-layer perception. Focusing on a graph data model with discriminative
  nodes that determine node labels and non-discriminative nodes that are class-irrelevant,
  we characterize the sample complexity required to achieve a desirable generalization
  error by training with stochastic gradient descent (SGD). This paper provides the
  quantitative characterization of the sample complexity and number of iterations
  for convergence dependent on the fraction of discriminative nodes, the dominant
  patterns, and the initial model errors. Furthermore, we demonstrate that self-attention
  and positional encoding enhance generalization by making the attention map sparse
  and promoting the core neighborhood during training, which explains the superior
  feature representation of Graph Transformers. Our theoretical results are supported
  by empirical experiments on synthetic and real-world benchmarks.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: li24bo
month: 0
tex_title: What Improves the Generalization of Graph Transformers? {A} Theoretical
  Dive into the Self-attention and Positional Encoding
firstpage: 28784
lastpage: 28829
page: 28784-28829
order: 28784
cycles: false
bibtex_author: Li, Hongkang and Wang, Meng and Ma, Tengfei and Liu, Sijia and Zhang,
  Zaixi and Chen, Pin-Yu
author:
- given: Hongkang
  family: Li
- given: Meng
  family: Wang
- given: Tengfei
  family: Ma
- given: Sijia
  family: Liu
- given: Zaixi
  family: Zhang
- given: Pin-Yu
  family: Chen
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/li24bo/li24bo.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
