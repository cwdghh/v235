---
title: 'Feature Reuse and Scaling: Understanding Transfer Learning with Protein Language
  Models'
openreview: wdTiuvd0fR
abstract: Large pretrained protein language models (PLMs) have improved protein property
  and structure prediction from sequences via transfer learning, in which weights
  and representations from PLMs are repurposed for downstream tasks. Although PLMs
  have shown great promise, currently there is little understanding of how the features
  learned by pretraining relate to and are useful for downstream tasks. We perform
  a systematic analysis of transfer learning using PLMs, conducting 370 experiments
  across a comprehensive suite of factors including different downstream tasks, architectures,
  model sizes, model depths, and pretraining time. We observe that while almost all
  downstream tasks do benefit from pretrained models compared to naive sequence representations,
  for the majority of tasks performance does not scale with pretraining, and instead
  relies on low-level features learned early in pretraining. Our results point to
  a mismatch between current PLM pretraining paradigms and most applications of these
  models, indicating a need for better pretraining methods.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: li24a
month: 0
tex_title: 'Feature Reuse and Scaling: Understanding Transfer Learning with Protein
  Language Models'
firstpage: 27351
lastpage: 27375
page: 27351-27375
order: 27351
cycles: false
bibtex_author: Li, Francesca-Zhoufan and Amini, Ava P and Yue, Yisong and Yang, Kevin
  K and Lu, Alex Xijie
author:
- given: Francesca-Zhoufan
  family: Li
- given: Ava P
  family: Amini
- given: Yisong
  family: Yue
- given: Kevin K
  family: Yang
- given: Alex Xijie
  family: Lu
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://raw.githubusercontent.com/mlresearch/v235/main/assets/li24a/li24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
