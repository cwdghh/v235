---
title: Sub-token ViT Embedding via Stochastic Resonance Transformers
openreview: 6DBvBcW770
abstract: 'Vision Transformer (ViT) architectures represent images as collections
  of high-dimensional vectorized tokens, each corresponding to a rectangular non-overlapping
  patch. This representation trades spatial granularity for embedding dimensionality,
  and results in semantically rich but spatially coarsely quantized feature maps.
  In order to retrieve spatial details beneficial to fine-grained inference tasks
  we propose a training-free method inspired by "stochastic resonance." Specifically,
  we perform sub-token spatial transformations to the input data, and aggregate the
  resulting ViT features after applying the inverse transformation. The resulting
  "Stochastic Resonance Transformer" (SRT) retains the rich semantic information of
  the original representation, but grounds it on a finer-scale spatial domain, partly
  mitigating the coarse effect of spatial tokenization. SRT is applicable across any
  layer of any ViT architecture, consistently boosting performance on several tasks
  including segmentation, classification, depth estimation, and others by up to 14.9%
  without the need for any fine-tuning. Code: https://github.com/donglao/srt.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: lao24a
month: 0
tex_title: Sub-token {V}i{T} Embedding via Stochastic Resonance Transformers
firstpage: 25995
lastpage: 26006
page: 25995-26006
order: 25995
cycles: false
bibtex_author: Lao, Dong and Wu, Yangchao and Liu, Tian Yu and Wong, Alex and Soatto,
  Stefano
author:
- given: Dong
  family: Lao
- given: Yangchao
  family: Wu
- given: Tian Yu
  family: Liu
- given: Alex
  family: Wong
- given: Stefano
  family: Soatto
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/assets/lao24a/lao24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
