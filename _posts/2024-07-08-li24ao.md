---
title: Enhancing Class-Imbalanced Learning with Pre-Trained Guidance through Class-Conditional
  Knowledge Distillation
openreview: O4nXWHPl6g
abstract: In class-imbalanced learning, the scarcity of information about minority
  classes presents challenges in obtaining generalizable features for these classes.
  Leveraging large-scale pre-trained models with powerful generalization capabilities
  as teacher models can help fill this information gap. Traditional knowledge distillation
  transfers the label distribution $p(\boldsymbol{y}|\boldsymbol{x})$ predicted by
  the teacher model to the student model. However, this method falls short on imbalanced
  data as it fails to capture the class-conditional probability distribution $p(\boldsymbol{x}|\boldsymbol{y})$
  from the teacher model, which is crucial for enhancing generalization. To overcome
  this, we propose Class-Conditional Knowledge Distillation (CCKD), a novel approach
  that enables learning of the teacher modelâ€™s class-conditional probability distribution
  during the distillation process. Additionally, we introduce Augmented CCKD (ACCKD),
  which involves distillation on a constructed class-balanced dataset (formed through
  data mixing) and feature imitation on the entire dataset to further facilitate the
  learning of features. Experimental results on various imbalanced datasets demonstrate
  an average accuracy improvement of 7.4% using our method.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: li24ao
month: 0
tex_title: Enhancing Class-Imbalanced Learning with Pre-Trained Guidance through Class-Conditional
  Knowledge Distillation
firstpage: 28204
lastpage: 28221
page: 28204-28221
order: 28204
cycles: false
bibtex_author: Li, Lan and Li, Xin-Chun and Ye, Han-Jia and Zhan, De-Chuan
author:
- given: Lan
  family: Li
- given: Xin-Chun
  family: Li
- given: Han-Jia
  family: Ye
- given: De-Chuan
  family: Zhan
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/assets/li24ao/li24ao.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
