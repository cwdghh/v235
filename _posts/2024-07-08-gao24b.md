---
title: Energy-based Backdoor Defense without Task-Specific Samples and Model Retraining
openreview: TJ6tVNt6Y4
abstract: 'Backdoor defense is crucial to ensure the safety and robustness of machine
  learning models when under attack. However, most existing methods specialize in
  either the detection or removal of backdoors, but seldom both. While few works have
  addressed both, these methods rely on strong assumptions or entail significant overhead
  costs, such as the need of task-specific samples for detection and model retraining
  for removal. Hence, the key challenge is how to reduce overhead and relax unrealistic
  assumptions. In this work, we propose two Energy-Based BAckdoor defense methods,
  called EBBA and EBBA+, that can achieve both backdoored model detection and backdoor
  removal with low overhead. Our contributions are twofold: First, we offer theoretical
  analysis for our observation that a predefined target label is more likely to occur
  among the top results for various samples. Inspired by this, we develop an enhanced
  energy-based technique, called EBBA, to detect backdoored models without task-specific
  samples (i.e., samples from any tasks). Secondly, we theoretically analyze that
  after data corruption, the original clean label of a poisoned sample is more likely
  to be predicted as a top output by the model, a sharp contrast to clean samples.
  Accordingly, we extend EBBA to develop EBBA+, a new transferred energy approach
  to efficiently detect poisoned images and remove backdoors without model retraining.
  Extensive experiments on multiple benchmark datasets demonstrate the superior performance
  of our methods over baselines in both backdoor detection and removal. Notably, the
  proposed methods can effectively detect backdoored model and poisoned images as
  well as remove backdoors at the same time.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: gao24b
month: 0
tex_title: Energy-based Backdoor Defense without Task-Specific Samples and Model Retraining
firstpage: 14611
lastpage: 14637
page: 14611-14637
order: 14611
cycles: false
bibtex_author: Gao, Yudong and Chen, Honglong and Sun, Peng and Li, Zhe and Li, Junjian
  and Shao, Huajie
author:
- given: Yudong
  family: Gao
- given: Honglong
  family: Chen
- given: Peng
  family: Sun
- given: Zhe
  family: Li
- given: Junjian
  family: Li
- given: Huajie
  family: Shao
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://raw.githubusercontent.com/mlresearch/v235/main/assets/gao24b/gao24b.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
