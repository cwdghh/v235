---
title: Gated Linear Attention Transformers with Hardware-Efficient Training
openreview: ia5XvxFUJT
abstract: Transformers with linear attention allow for efficient parallel training
  but can simultaneously be formulated as an RNN with 2D (matrix-valued) hidden states,
  thus enjoying linear-time inference complexity. However, linear attention generally
  underperforms ordinary softmax attention. Moreover, current implementations of linear
  attention lack I/O-awareness and are thus slower than highly optimized implementations
  of softmax attention. This work describes a hardware-efficient algorithm for linear
  attention that trades off memory movement against parallelizability. The resulting
  implementation, dubbed FlashLinearAttention, is faster than FlashAttention-2 as
  a standalone layer even on short sequence lengths (e.g., 1K). We then generalize
  this algorithm to a more expressive variant of linear attention with data-dependent
  gates. When used as a replacement for the standard attention layer in Transformers,
  the resulting gated linear attention (GLA) Transformer is found to perform competitively
  against the LLaMA-architecture Transformer as well recent linear-time-inference
  baselines such as RetNet and Mamba on moderate-scale language modeling experiments.
  GLA Transformer is especially effective at length generalization, enabling a model
  trained on 2K to generalize to sequences longer than 20K without significant perplexity
  degradations. For training speed, the GLA Transformer has higher throughput than
  a similarly-sized Mamba model.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: yang24ab
month: 0
tex_title: Gated Linear Attention Transformers with Hardware-Efficient Training
firstpage: 56501
lastpage: 56523
page: 56501-56523
order: 56501
cycles: false
bibtex_author: Yang, Songlin and Wang, Bailin and Shen, Yikang and Panda, Rameswar
  and Kim, Yoon
author:
- given: Songlin
  family: Yang
- given: Bailin
  family: Wang
- given: Yikang
  family: Shen
- given: Rameswar
  family: Panda
- given: Yoon
  family: Kim
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/yang24ab/yang24ab.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
