---
title: 'Positional Knowledge is All You Need: Position-induced Transformer (PiT) for
  Operator Learning'
openreview: VOcsmIBiXE
abstract: 'Operator learning for Partial Differential Equations (PDEs) is rapidly
  emerging as a promising approach for surrogate modeling of intricate systems. Transformers
  with the self-attention mechanism—a powerful tool originally designed for natural
  language processing—have recently been adapted for operator learning. However, they
  confront challenges, including high computational demands and limited interpretability.
  This raises a critical question: <em>Is there a more efficient attention mechanism
  for Transformer-based operator learning?</em> This paper proposes the Position-induced
  Transformer (PiT), built on an innovative position-attention mechanism, which demonstrates
  significant advantages over the classical self-attention in operator learning. Position-attention
  draws inspiration from numerical methods for PDEs. Different from self-attention,
  position-attention is induced by only the spatial interrelations of sampling positions
  for input functions of the operators, and does not rely on the input function values
  themselves, thereby greatly boosting efficiency. PiT exhibits superior performance
  over current state-of-the-art neural operators in a variety of complex operator
  learning tasks across diverse PDE benchmarks. Additionally, PiT possesses an enhanced
  discretization convergence feature, compared to the widely-used Fourier neural operator.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: chen24au
month: 0
tex_title: 'Positional Knowledge is All You Need: Position-induced Transformer ({P}i{T})
  for Operator Learning'
firstpage: 7526
lastpage: 7552
page: 7526-7552
order: 7526
cycles: false
bibtex_author: Chen, Junfeng and Wu, Kailiang
author:
- given: Junfeng
  family: Chen
- given: Kailiang
  family: Wu
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/assets/chen24au/chen24au.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
