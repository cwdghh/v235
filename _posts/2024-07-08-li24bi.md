---
title: 'LoRAP: Transformer Sub-Layers Deserve Differentiated Structured Compression
  for Large Language Models'
openreview: mhI5nc5QwX
abstract: Large language models (LLMs) show excellent performance in difficult tasks,
  but they often require massive memories and computational resources. How to reduce
  the parameter scale of LLMs has become research hotspots. In this study, we get
  an important observation that the multi-head self-attention (MHA) sub-layer of Transformer
  exhibits noticeable low-rank structure, while the feed-forward network (FFN) sub-layer
  does not. With this regard, we design a novel structured compression method LoRAP,
  which organically combines <b>Lo</b>w-<b>R</b>ank matrix approximation <b>A</b>nd
  structured <b>P</b>runing. For the MHA sub-layer, we proposal an input activation
  weighted singular value decomposition method and allocate different parameter amounts
  for each weight matrix based on the differences in low-rank properties of matrices.For
  the FFN sub-layer, we propose a gradient-free structured channel pruning method
  and save the least important 1% of parameters which actually play a vital role in
  model performance. Extensive evaluations on zero-shot perplexity and zero-shot task
  classification indicate that our proposal is superior to previous structured compression
  rivals under multiple compression ratios. Our code will be released soon.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: li24bi
month: 0
tex_title: "{L}o{RAP}: Transformer Sub-Layers Deserve Differentiated Structured Compression
  for Large Language Models"
firstpage: 28657
lastpage: 28672
page: 28657-28672
order: 28657
cycles: false
bibtex_author: Li, Guangyan and Tang, Yongqiang and Zhang, Wensheng
author:
- given: Guangyan
  family: Li
- given: Yongqiang
  family: Tang
- given: Wensheng
  family: Zhang
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://raw.githubusercontent.com/mlresearch/v235/main/assets/li24bi/li24bi.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
