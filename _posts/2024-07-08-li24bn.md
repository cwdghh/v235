---
title: How Do Nonlinear Transformers Learn and Generalize in In-Context Learning?
openreview: I4HTPws9P6
abstract: Transformer-based large language models have displayed impressive in-context
  learning capabilities, where a pre-trained model can handle new tasks without fine-tuning
  by simply augmenting the query with some input-output examples from that task. Despite
  the empirical success, the mechanics of how to train a Transformer to achieve ICL
  and the corresponding ICL capacity is mostly elusive due to the technical challenges
  of analyzing the nonconvex training problems resulting from the nonlinear self-attention
  and nonlinear activation in Transformers. To the best of our knowledge, this paper
  provides the first theoretical analysis of the training dynamics of Transformers
  with nonlinear self-attention and nonlinear MLP, together with the ICL generalization
  capability of the resulting model. Focusing on a group of binary classification
  tasks, we train Transformers using data from a subset of these tasks and quantify
  the impact of various factors on the ICL generalization performance on the remaining
  unseen tasks with and without data distribution shifts. We also analyze how different
  components in the learned Transformers contribute to the ICL performance. Furthermore,
  we provide the first theoretical analysis of how model pruning affects ICL performance
  and prove that proper magnitude-based pruning can have a minimal impact on ICL while
  reducing inference costs. These theoretical findings are justified through numerical
  experiments.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: li24bn
month: 0
tex_title: How Do Nonlinear Transformers Learn and Generalize in In-Context Learning?
firstpage: 28734
lastpage: 28783
page: 28734-28783
order: 28734
cycles: false
bibtex_author: Li, Hongkang and Wang, Meng and Lu, Songtao and Cui, Xiaodong and Chen,
  Pin-Yu
author:
- given: Hongkang
  family: Li
- given: Meng
  family: Wang
- given: Songtao
  family: Lu
- given: Xiaodong
  family: Cui
- given: Pin-Yu
  family: Chen
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://raw.githubusercontent.com/mlresearch/v235/main/assets/li24bn/li24bn.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
