---
title: 'Sparse Cocktail: Every Sparse Pattern Every Sparse Ratio All At Once'
openreview: OrVl8R13Wy
abstract: Sparse Neural Networks (SNNs) have received voluminous attention for mitigating
  the explosion in computational costs and memory footprints of modern deep neural
  networks. Despite their popularity, most state-of-the-art training approaches seek
  to find a single high-quality sparse subnetwork with a preset sparsity pattern and
  ratio, making them inadequate to satiate platform and resource variability. Recently
  proposed approaches attempt to jointly train multiple subnetworks (we term as “sparse
  co-training") with a <em>fixed sparsity pattern</em>, to allow switching sparsity
  ratios subject to resource requirements. In this work, we take one more step forward
  and expand the scope of sparse co-training to cover diverse sparsity patterns and
  multiple sparsity ratios <em>at once</em>. We introduce <b>Sparse Cocktail</b>,
  the first sparse co-training framework that co-trains a suite of sparsity patterns
  simultaneously, loaded with multiple sparsity ratios which facilitate harmonious
  switch across various sparsity patterns and ratios at inference depending on the
  hardware availability. More specifically, Sparse Cocktail alternatively trains subnetworks
  generated from different sparsity patterns with a gradual increase in sparsity ratios
  across patterns and relies on an <em>unified mask generation process</em> and the
  <em>Dense Pivot Co-training</em> to ensure the subnetworks of different patterns
  orchestrate their shared parameters without canceling each other’s performance.
  Experiment results on image classification, object detection, and instance segmentation
  illustrate the favorable effectiveness and flexibility of Sparse Cocktail, pointing
  to a promising direction for sparse co-training. Codes will be released.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: li24av
month: 0
tex_title: 'Sparse Cocktail: Every Sparse Pattern Every Sparse Ratio All At Once'
firstpage: 28368
lastpage: 28386
page: 28368-28386
order: 28368
cycles: false
bibtex_author: Li, Zhangheng and Liu, Shiwei and Chen, Tianlong and Jaiswal, Ajay
  Kumar and Zhang, Zhenyu and Wang, Dilin and Krishnamoorthi, Raghuraman and Chang,
  Shiyu and Wang, Zhangyang
author:
- given: Zhangheng
  family: Li
- given: Shiwei
  family: Liu
- given: Tianlong
  family: Chen
- given: Ajay Kumar
  family: Jaiswal
- given: Zhenyu
  family: Zhang
- given: Dilin
  family: Wang
- given: Raghuraman
  family: Krishnamoorthi
- given: Shiyu
  family: Chang
- given: Zhangyang
  family: Wang
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/li24av/li24av.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
