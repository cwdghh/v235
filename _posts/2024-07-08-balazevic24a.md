---
title: Memory Consolidation Enables Long-Context Video Understanding
openreview: qeFgvVVAJ2
abstract: Most transformer-based video encoders are limited to short temporal contexts
  due to their quadratic complexity. While various attempts have been made to extend
  this context, this has often come at the cost of both conceptual and computational
  complexity. We propose to instead re-purpose existing pre-trained video transformers
  by simply fine-tuning them to attend to memories derived non-parametrically from
  past activations. By leveraging redundancy reduction, our memory-consolidated vision
  transformer (MC-ViT) effortlessly extends its context far into the past and exhibits
  excellent scaling behavior when learning from longer videos. In doing so, MC-ViT
  sets a new state-of-the-art in long-context video understanding on EgoSchema, Perception
  Test, and Diving48, outperforming methods that benefit from orders of magnitude
  more parameters.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: balazevic24a
month: 0
tex_title: Memory Consolidation Enables Long-Context Video Understanding
firstpage: 2527
lastpage: 2542
page: 2527-2542
order: 2527
cycles: false
bibtex_author: Balazevic, Ivana and Shi, Yuge and Papalampidi, Pinelopi and Chaabouni,
  Rahma and Koppula, Skanda and Henaff, Olivier J
author:
- given: Ivana
  family: Balazevic
- given: Yuge
  family: Shi
- given: Pinelopi
  family: Papalampidi
- given: Rahma
  family: Chaabouni
- given: Skanda
  family: Koppula
- given: Olivier J
  family: Henaff
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://raw.githubusercontent.com/mlresearch/v235/main/assets/balazevic24a/balazevic24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
