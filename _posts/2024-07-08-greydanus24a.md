---
title: Scaling Down Deep Learning with MNIST-1D
openreview: n9pru4bJU9
abstract: 'Although deep learning models have taken on commercial and political relevance,
  key aspects of their training and operation remain poorly understood. This has sparked
  interest in science of deep learning projects, many of which require large amounts
  of time, money, and electricity. But how much of this research really needs to occur
  at scale? In this paper, we introduce MNIST-1D: a minimalist, procedurally generated,
  low-memory, and low-compute alternative to classic deep learning benchmarks. Although
  the dimensionality of MNIST-1D is only 40 and its default training set size only
  4000, MNIST-1D can be used to study inductive biases of different deep architectures,
  find lottery tickets, observe deep double descent, metalearn an activation function,
  and demonstrate guillotine regularization in self-supervised learning. All these
  experiments can be conducted on a GPU or often even on a CPU within minutes, allowing
  for fast prototyping, educational use cases, and cutting-edge research on a low
  budget.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: greydanus24a
month: 0
tex_title: Scaling Down Deep Learning with {MNIST}-1{D}
firstpage: 16404
lastpage: 16415
page: 16404-16415
order: 16404
cycles: false
bibtex_author: Greydanus, Samuel James and Kobak, Dmitry
author:
- given: Samuel James
  family: Greydanus
- given: Dmitry
  family: Kobak
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/assets/greydanus24a/greydanus24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
