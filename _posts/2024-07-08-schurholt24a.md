---
title: Towards Scalable and Versatile Weight Space Learning
openreview: ug2uoAZ9c2
abstract: Learning representations of well-trained neural network models holds the
  promise to provide an understanding of the inner workings of those models. However,
  previous work has either faced limitations when processing larger networks or was
  task-specific to either discriminative or generative tasks. This paper introduces
  the SANE approach to weight-space learning. SANE overcomes previous limitations
  by learning task-agnostic representations of neural networks that are scalable to
  larger models of varying architectures and that show capabilities beyond a single
  task. Our method extends the idea of <em>hyper-representations</em> towards sequential
  processing of subsets of neural network weights, thus allowing one to embed larger
  neural networks as a set of tokens into the learned representation space. SANE reveals
  global model information from layer-wise embeddings, and it can sequentially generate
  unseen neural network models, which was unattainable with previous <em>hyper-representation</em>
  learning methods. Extensive empirical evaluation demonstrates that SANE matches
  or exceeds state-of-the-art performance on several weight representation learning
  benchmarks, particularly in initialization for new tasks and larger ResNet architectures.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: schurholt24a
month: 0
tex_title: Towards Scalable and Versatile Weight Space Learning
firstpage: 43947
lastpage: 43966
page: 43947-43966
order: 43947
cycles: false
bibtex_author: Sch\"{u}rholt, Konstantin and Mahoney, Michael W. and Borth, Damian
author:
- given: Konstantin
  family: Sch√ºrholt
- given: Michael W.
  family: Mahoney
- given: Damian
  family: Borth
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/schurholt24a/schurholt24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
