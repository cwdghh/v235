---
title: 'The Benefits of Reusing Batches for Gradient Descent in Two-Layer Networks:
  Breaking the Curse of Information and Leap Exponents'
openreview: iKkFruh4d5
abstract: We investigate the training dynamics of two-layer neural networks when learning
  multi-index target functions. We focus on multi-pass gradient descent (GD) that
  reuses the batches multiple times and show that it significantly changes the conclusion
  about which functions are learnable compared to single-pass gradient descent. In
  particular, multi-pass GD with finite stepsize is found to overcome the limitations
  of gradient flow and single-pass GD given by the information exponent (Ben Arous
  et al., 2021) and leap exponent (Abbe et al., 2023) of the target function. We show
  that upon re-using batches, the network achieves in just two time steps an overlap
  with the target subspace even for functions not satisfying the staircase property
  (Abbe et al., 2021). We characterize the (broad) class of functions efficiently
  learned in finite time. The proof of our results is based on the analysis of the
  Dynamical Mean-Field Theory (DMFT). We further provide a closed-form description
  of the dynamical process of the low-dimensional projections of the weights, and
  numerical experiments illustrating the theory.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: dandi24a
month: 0
tex_title: 'The Benefits of Reusing Batches for Gradient Descent in Two-Layer Networks:
  Breaking the Curse of Information and Leap Exponents'
firstpage: 9991
lastpage: 10016
page: 9991-10016
order: 9991
cycles: false
bibtex_author: Dandi, Yatin and Troiani, Emanuele and Arnaboldi, Luca and Pesce, Luca
  and Zdeborova, Lenka and Krzakala, Florent
author:
- given: Yatin
  family: Dandi
- given: Emanuele
  family: Troiani
- given: Luca
  family: Arnaboldi
- given: Luca
  family: Pesce
- given: Lenka
  family: Zdeborova
- given: Florent
  family: Krzakala
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://raw.githubusercontent.com/mlresearch/v235/main/assets/dandi24a/dandi24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
