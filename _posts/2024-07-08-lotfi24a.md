---
title: Non-Vacuous Generalization Bounds for Large Language Models
openreview: 6Kg9p8URlj
abstract: Modern language models can contain billions of parameters, raising the question
  of whether they can generalize beyond the training data or simply parrot their training
  corpora. We provide the first non-vacuous generalization bounds for pretrained large
  language models (LLMs), indicating that language models are capable of discovering
  regularities that generalize to unseen data. In particular, we derive a compression
  bound that is valid for the unbounded log-likelihood loss using prediction smoothing,
  and we extend the bound to handle subsampling, making bound computation 900 times
  faster on massive datasets. To achieve the extreme level of compression required
  for non-vacuous bounds, we devise SubLoRA, a simple low-dimensional nonlinear parameterization
  that leads to non-vacuous generalization bounds for very large models with up to
  849 million parameters. Finally, we use our bounds to understand LLM generalization
  and find that larger models have better generalization bounds and are more compressible
  than smaller models.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: lotfi24a
month: 0
tex_title: Non-Vacuous Generalization Bounds for Large Language Models
firstpage: 32801
lastpage: 32818
page: 32801-32818
order: 32801
cycles: false
bibtex_author: Lotfi, Sanae and Finzi, Marc Anton and Kuang, Yilun and Rudner, Tim
  G. J. and Goldblum, Micah and Wilson, Andrew Gordon
author:
- given: Sanae
  family: Lotfi
- given: Marc Anton
  family: Finzi
- given: Yilun
  family: Kuang
- given: Tim G. J.
  family: Rudner
- given: Micah
  family: Goldblum
- given: Andrew Gordon
  family: Wilson
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/lotfi24a/lotfi24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
