---
title: 'Two Heads Are Better Than One: Boosting Graph Sparse Training via Semantic
  and Topological Awareness'
openreview: WDgV1BJEW0
abstract: 'Graph Neural Networks (GNNs) excel in various graph learning tasks but
  face computational challenges when applied to large-scale graphs. A promising solution
  is to remove non-essential edges to reduce the computational overheads in GNN. Previous
  literature generally falls into two categories: topology-guided and semantic-guided.
  The former maintains certain graph topological properties yet often underperforms
  on GNNs. % due to low integration with neural network training. The latter performs
  well at lower sparsity on GNNs but faces performance collapse at higher sparsity
  levels. With this in mind, we propose a new research line and concept termed <b>Graph
  Sparse Training</b> <b>(GST)</b>, which dynamically manipulates sparsity at the
  data level. Specifically, GST initially constructs a topology & semantic anchor
  at a low training cost, followed by performing dynamic sparse training to align
  the sparse graph with the anchor. We introduce the <b>Equilibria Sparsification
  Principle</b> to guide this process, balancing the preservation of both topological
  and semantic information. Ultimately, GST produces a sparse graph with maximum topological
  integrity and no performance degradation. Extensive experiments on 6 datasets and
  5 backbones showcase that GST <b>(I)</b> identifies subgraphs at higher graph sparsity
  levels ($1.67%\sim15.85%$$\uparrow$) than state-of-the-art sparsification methods,
  <b>(II)</b> preserves more key spectral properties, <b>(III)</b> achieves $1.27-3.42\times$
  speedup in GNN inference and <b>(IV)</b> successfully helps graph adversarial defense
  and graph lottery tickets.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: zhang24bx
month: 0
tex_title: 'Two Heads Are Better Than One: Boosting Graph Sparse Training via Semantic
  and Topological Awareness'
firstpage: 60197
lastpage: 60219
page: 60197-60219
order: 60197
cycles: false
bibtex_author: Zhang, Guibin and Yue, Yanwei and Wang, Kun and Fang, Junfeng and Sui,
  Yongduo and Wang, Kai and Liang, Yuxuan and Cheng, Dawei and Pan, Shirui and Chen,
  Tianlong
author:
- given: Guibin
  family: Zhang
- given: Yanwei
  family: Yue
- given: Kun
  family: Wang
- given: Junfeng
  family: Fang
- given: Yongduo
  family: Sui
- given: Kai
  family: Wang
- given: Yuxuan
  family: Liang
- given: Dawei
  family: Cheng
- given: Shirui
  family: Pan
- given: Tianlong
  family: Chen
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/zhang24bx/zhang24bx.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
