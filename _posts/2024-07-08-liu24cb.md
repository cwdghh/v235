---
title: 'Timer: Generative Pre-trained Transformers Are Large Time Series Models'
openreview: bYRYb7DMNo
abstract: 'Deep learning has contributed remarkably to the advancement of time series
  analysis. Still, deep models can encounter performance bottlenecks in real-world
  data-scarce scenarios, which can be concealed due to the performance saturation
  with small models on current benchmarks. Meanwhile, large models have demonstrated
  great powers in these scenarios through large-scale pre-training. Continuous progress
  has been achieved with the emergence of large language models, exhibiting unprecedented
  abilities such as few-shot generalization, scalability, and task generality, which
  are however absent in small deep models. To change the status quo of training scenario-specific
  small models from scratch, this paper aims at the early development of large time
  series models (LTSM). During pre-training, we curate large-scale datasets with up
  to 1 billion time points, unify heterogeneous time series into single-series sequence
  (S3) format, and develop the GPT-style architecture toward LTSMs. To meet diverse
  application needs, we convert forecasting, imputation, and anomaly detection of
  time series into a unified generative task. The outcome of this study is a Time
  Series Transformer (Timer), which is generative pre-trained by next token prediction
  and adapted to various downstream tasks with promising capabilities as an LTSM.
  Code and datasets are available at: https://github.com/thuml/Large-Time-Series-Model.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: liu24cb
month: 0
tex_title: 'Timer: Generative Pre-trained Transformers Are Large Time Series Models'
firstpage: 32369
lastpage: 32399
page: 32369-32399
order: 32369
cycles: false
bibtex_author: Liu, Yong and Zhang, Haoran and Li, Chenyu and Huang, Xiangdong and
  Wang, Jianmin and Long, Mingsheng
author:
- given: Yong
  family: Liu
- given: Haoran
  family: Zhang
- given: Chenyu
  family: Li
- given: Xiangdong
  family: Huang
- given: Jianmin
  family: Wang
- given: Mingsheng
  family: Long
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://raw.githubusercontent.com/mlresearch/v235/main/assets/liu24cb/liu24cb.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
