---
title: Universal Gradient Methods for Stochastic Convex Optimization
openreview: Wnhp34K5jR
abstract: We develop universal gradient methods for Stochastic Convex Optimization
  (SCO). Our algorithms automatically adapt not only to the oracle’s noise but also
  to the Hölder smoothness of the objective function without a priori knowledge of
  the particular setting. The key ingredient is a novel strategy for adjusting step-size
  coefficients in the Stochastic Gradient Method (SGD). Unlike AdaGrad, which accumulates
  gradient norms, our Universal Gradient Method accumulates appropriate combinations
  of gradientand iterate differences. The resulting algorithm has state-of-the-art
  worst-case convergence rate guarantees for the entire Hölder class including, in
  particular, both nonsmooth functions and those with Lipschitz continuous gradient.
  We also present the Universal Fast Gradient Method for SCO enjoying optimal efficiency
  estimates.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: rodomanov24a
month: 0
tex_title: Universal Gradient Methods for Stochastic Convex Optimization
firstpage: 42620
lastpage: 42646
page: 42620-42646
order: 42620
cycles: false
bibtex_author: Rodomanov, Anton and Kavis, Ali and Wu, Yongtao and Antonakopoulos,
  Kimon and Cevher, Volkan
author:
- given: Anton
  family: Rodomanov
- given: Ali
  family: Kavis
- given: Yongtao
  family: Wu
- given: Kimon
  family: Antonakopoulos
- given: Volkan
  family: Cevher
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/rodomanov24a/rodomanov24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
