---
title: The Illusion of State in State-Space Models
openreview: QZgo9JZpLq
abstract: 'State-space models (SSMs) have emerged as a potential alternative architecture
  for building large language models (LLMs) compared to the previously ubiquitous
  transformer architecture. One theoretical weakness of transformers is that they
  cannot express certain kinds of sequential computation and state tracking (Merrill
  & Sabharwal, 2023), which SSMs are explicitly designed to address via their close
  architectural similarity to recurrent neural networks (RNNs). <em>But do SSMs truly
  have an advantage (over transformers) in expressive power for state tracking?</em>
  Surprisingly, the answer is no. Our analysis reveals that the expressive power of
  SSMs is limited very similarly to transformers: SSMs cannot express computation
  outside the complexity class $\mathsf{TC}^0$. In particular, this means they cannot
  solve simple state-tracking problems like permutation composition. It follows that
  SSMs are provably unable to accurately track chess moves with certain notation,
  evaluate code, or track entities in a long narrative. To supplement our formal analysis,
  we report experiments showing that Mamba-style SSMs indeed struggle with state tracking.
  Thus, despite its recurrent formulation, the "state‚Äù in an SSM is an illusion: SSMs
  have similar expressiveness limitations to non-recurrent models like transformers,
  which may fundamentally limit their ability to solve real-world state-tracking problems.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: merrill24a
month: 0
tex_title: The Illusion of State in State-Space Models
firstpage: 35492
lastpage: 35506
page: 35492-35506
order: 35492
cycles: false
bibtex_author: Merrill, William and Petty, Jackson and Sabharwal, Ashish
author:
- given: William
  family: Merrill
- given: Jackson
  family: Petty
- given: Ashish
  family: Sabharwal
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/assets/merrill24a/merrill24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
