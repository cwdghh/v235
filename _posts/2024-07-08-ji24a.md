---
title: Advancing Dynamic Sparse Training by Exploring Optimization Opportunities
openreview: szRHR9XGrY
abstract: Dynamic Sparse Training (DST) is an effective approach for addressing the
  substantial training resource requirements posed by the ever-increasing size of
  the Deep Neural Networks (DNNs). Characterized by its dynamic "train-prune-grow‚Äù
  schedule during training, DST implicitly develops a bi-level structure for training
  the weights while discovering a subnetwork topology. However, such a structure is
  consistently overlooked by the current DST algorithms for further optimization opportunities,
  and these algorithms, on the other hand, solely optimize the weights while determining
  masks heuristically. In this paper, we extensively study DST algorithms and argue
  that the training scheme of DST naturally forms a bi-level problem in which the
  updating of weight and mask is interdependent. Based on this observation, we introduce
  a novel efficient training framework called BiDST, which for the first time, introduces
  bi-level optimization methodology into dynamic sparse training domain. Unlike traditional
  partial-heuristic DST schemes, which suffer from sub-optimal search efficiency for
  masks and miss the opportunity to fully explore the topological space of neural
  networks, BiDST excels at discovering excellent sparse patterns by optimizing mask
  and weight simultaneously, resulting in maximum 2.62% higher accuracy, 2.1$\times$
  faster execution speed, and 25$\times$ reduced overhead. Code available at https://github.com/jjsrf/BiDST-ICML2024.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: ji24a
month: 0
tex_title: Advancing Dynamic Sparse Training by Exploring Optimization Opportunities
firstpage: 21606
lastpage: 21619
page: 21606-21619
order: 21606
cycles: false
bibtex_author: Ji, Jie and Li, Gen and Yin, Lu and Qin, Minghai and Yuan, Geng and
  Guo, Linke and Liu, Shiwei and Ma, Xiaolong
author:
- given: Jie
  family: Ji
- given: Gen
  family: Li
- given: Lu
  family: Yin
- given: Minghai
  family: Qin
- given: Geng
  family: Yuan
- given: Linke
  family: Guo
- given: Shiwei
  family: Liu
- given: Xiaolong
  family: Ma
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/ji24a/ji24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
