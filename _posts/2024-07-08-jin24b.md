---
title: 'LLM Maybe LongLM: SelfExtend LLM Context Window Without Tuning'
openreview: nkOMLBIiI7
abstract: 'It is well known that LLMs cannot generalize well to long contexts whose
  lengths are larger than the training sequence length. This poses challenges when
  employing LLMs for processing long input sequences during inference. In this work,
  we argue that LLMs themselves have inherent capabilities to handles s long contexts
  without fine-tuning. To achieve this goal, we propose SelfExtend to extend the context
  window of LLMs by constructing bi-level attention information: the grouped attention
  and the neighbor attention. The grouped attention captures the dependencies among
  tokens that are far apart, while neighbor attention captures dependencies among
  adjacent tokens within a specified range. The two-level attentions are computed
  based on the original model’s self-attention mechanism during inference. With minor
  code modification, our SelfExtend can effortlessly extend existing LLMs’ context
  window without any fine-tuning. We conduct comprehensive experiments on multiple
  benchmarks and the results show that our SelfExtend can effectively extend existing
  LLMs’ context window length.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: jin24b
month: 0
tex_title: "{LLM} Maybe {L}ong{LM}: {S}elf{E}xtend {LLM} Context Window Without Tuning"
firstpage: 22099
lastpage: 22114
page: 22099-22114
order: 22099
cycles: false
bibtex_author: Jin, Hongye and Han, Xiaotian and Yang, Jingfeng and Jiang, Zhimeng
  and Liu, Zirui and Chang, Chia-Yuan and Chen, Huiyuan and Hu, Xia
author:
- given: Hongye
  family: Jin
- given: Xiaotian
  family: Han
- given: Jingfeng
  family: Yang
- given: Zhimeng
  family: Jiang
- given: Zirui
  family: Liu
- given: Chia-Yuan
  family: Chang
- given: Huiyuan
  family: Chen
- given: Xia
  family: Hu
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/jin24b/jin24b.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
