---
title: Knowledge Distillation with Auxiliary Variable
openreview: 91QmrfztSP
abstract: Knowledge distillation (KD) provides an efficient framework for transferring
  knowledge from a teacher model to a student model by aligning their predictive distributions.
  The existing KD methods adopt the same strategy as the teacher to formulate the
  studentâ€™s predictive distribution. However, employing the same distribution-modeling
  strategy typically causes sub-optimal knowledge transfer due to the discrepancy
  in model capacity between teacher and student models. Designing student-friendly
  teachers contributes to alleviating the capacity discrepancy, while it requires
  either complicated or student-specific training schemes. To cast off this dilemma,
  we propose to introduce an auxiliary variable to promote the ability of the student
  to model predictive distribution. The auxiliary variable is defined to be related
  to target variables, which will boost the model prediction. Specifically, we reformulate
  the predictive distribution with the auxiliary variable, deriving a novel objective
  function of KD. Theoretically, we provide insights to explain why the proposed objective
  function can outperform the existing KD methods. Experimentally, we demonstrate
  that the proposed objective function can considerably and consistently outperform
  existing KD methods.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: peng24a
month: 0
tex_title: Knowledge Distillation with Auxiliary Variable
firstpage: 40185
lastpage: 40199
page: 40185-40199
order: 40185
cycles: false
bibtex_author: Peng, Bo and Fang, Zhen and Zhang, Guangquan and Lu, Jie
author:
- given: Bo
  family: Peng
- given: Zhen
  family: Fang
- given: Guangquan
  family: Zhang
- given: Jie
  family: Lu
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://raw.githubusercontent.com/mlresearch/v235/main/assets/peng24a/peng24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
