---
title: Defining Neural Network Architecture through Polytope Structures of Datasets
openreview: qXoqV40imX
abstract: Current theoretical and empirical research in neural networks suggests that
  complex datasets require large network architectures for thorough classification,
  yet the precise nature of this relationship remains unclear. This paper tackles
  this issue by defining upper and lower bounds for neural network widths, which are
  informed by the polytope structure of the dataset in question. We also delve into
  the application of these principles to simplicial complexes and specific manifold
  shapes, explaining how the requirement for network width varies in accordance with
  the geometric complexity of the dataset. Moreover, we develop an algorithm to investigate
  a converse situation where the polytope structure of a dataset can be inferred from
  its corresponding trained neural networks. Through our algorithm, it is established
  that popular datasets such as MNIST, Fashion-MNIST, and CIFAR10 can be efficiently
  encapsulated using no more than two polytopes with a small number of faces.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: lee24q
month: 0
tex_title: Defining Neural Network Architecture through Polytope Structures of Datasets
firstpage: 26789
lastpage: 26836
page: 26789-26836
order: 26789
cycles: false
bibtex_author: Lee, Sangmin and Mammadov, Abbas and Ye, Jong Chul
author:
- given: Sangmin
  family: Lee
- given: Abbas
  family: Mammadov
- given: Jong Chul
  family: Ye
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://raw.githubusercontent.com/mlresearch/v235/main/assets/lee24q/lee24q.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
