---
title: Simple Ingredients for Offline Reinforcement Learning
openreview: japBn31gXC
abstract: 'Offline reinforcement learning algorithms have proven effective on datasets
  highly connected to the target downstream task. Yet, by leveraging a novel testbed
  (MOOD) in which trajectories come from heterogeneous sources, we show that existing
  methods struggle with diverse data: their performance considerably deteriorates
  as data collected for related but different tasks is simply added to the offline
  buffer. In light of this finding, we conduct a large empirical study where we formulate
  and test several hypotheses to explain this failure. Surprisingly, we find that
  targeted scale, more than algorithmic considerations, is the key factor influencing
  performance. We show that simple methods like AWAC and IQL with increased policy
  size overcome the paradoxical failure modes from the inclusion of additional data
  in MOOD, and notably outperform prior state-of-the-art algorithms on the canonical
  D4RL benchmark.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: cetin24a
month: 0
tex_title: Simple Ingredients for Offline Reinforcement Learning
firstpage: 6020
lastpage: 6047
page: 6020-6047
order: 6020
cycles: false
bibtex_author: Cetin, Edoardo and Tirinzoni, Andrea and Pirotta, Matteo and Lazaric,
  Alessandro and Ollivier, Yann and Touati, Ahmed
author:
- given: Edoardo
  family: Cetin
- given: Andrea
  family: Tirinzoni
- given: Matteo
  family: Pirotta
- given: Alessandro
  family: Lazaric
- given: Yann
  family: Ollivier
- given: Ahmed
  family: Touati
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/assets/cetin24a/cetin24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
