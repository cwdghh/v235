---
title: The Surprising Effectiveness of Skip-Tuning in Diffusion Sampling
openreview: 2pYTCy4GUV
abstract: With the incorporation of the UNet architecture, diffusion probabilistic
  models have become a dominant force in image generation tasks. One key design in
  UNet is the skip connections between the encoder and decoder blocks. Although skip
  connections have been shown to improve training stability and model performance,
  we point out that such shortcuts can be a limiting factor for the complexity of
  the transformation. As the sampling steps decrease, the generation process and the
  role of the UNet get closer to the push-forward transformations from Gaussian distribution
  to the target, posing a challenge for the networkâ€™s complexity. To address this
  challenge, we propose Skip-Tuning, a simple yet surprisingly effective training-free
  tuning method on the skip connections. For instance, our method can achieve 100%
  FID improvement for pretrained EDM on ImageNet 64 with only 19 NFEs (1.75), breaking
  the limit of ODE samplers regardless of sampling steps. Surprisingly, the improvement
  persists when we increase the number of sampling steps and can even surpass the
  best result from EDM-2 (1.58) with only 39 NFEs (1.57). Comprehensive exploratory
  experiments are conducted to shed light on the surprising effectiveness of our Skip-Tuning.
  We observe that while Skip-Tuning increases the score-matching losses in the pixel
  space, the losses in the feature space are reduced, particularly at intermediate
  noise levels, which coincide with the most effective range accounting for image
  quality improvement.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: ma24r
month: 0
tex_title: The Surprising Effectiveness of Skip-Tuning in Diffusion Sampling
firstpage: 34053
lastpage: 34074
page: 34053-34074
order: 34053
cycles: false
bibtex_author: Ma, Jiajun and Xue, Shuchen and Hu, Tianyang and Wang, Wenjia and Liu,
  Zhaoqiang and Li, Zhenguo and Ma, Zhi-Ming and Kawaguchi, Kenji
author:
- given: Jiajun
  family: Ma
- given: Shuchen
  family: Xue
- given: Tianyang
  family: Hu
- given: Wenjia
  family: Wang
- given: Zhaoqiang
  family: Liu
- given: Zhenguo
  family: Li
- given: Zhi-Ming
  family: Ma
- given: Kenji
  family: Kawaguchi
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/ma24r/ma24r.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
