---
title: Automated Evaluation of Retrieval-Augmented Language Models with Task-Specific
  Exam Generation
openreview: 4jqOV6NlUz
abstract: We propose a new method to measure the task-specific accuracy of Retrieval-Augmented
  Large Language Models (RAG). Evaluation is performed by scoring the RAG on an automatically-generated
  synthetic exam composed of multiple choice questions based on the corpus of documents
  associated with the task. Our method is an automated, cost-efficient, interpretable,
  and robust strategy to select the optimal components for a RAG system. We leverage
  Item Response Theory (IRT) to estimate the quality of an exam and its informativeness
  on task-specific accuracy. IRT also provides a natural way to iteratively improve
  the exam by eliminating the exam questions that are not sufficiently informative
  about a modelâ€™s ability. We demonstrate our approach on four new open-ended Question-Answering
  tasks based on Arxiv abstracts, StackExchange questions, AWS DevOps troubleshooting
  guides, and SEC filings. In addition, our experiments reveal more general insights
  into factors impacting RAG performance like size, retrieval mechanism, prompting
  and fine-tuning. Most notably, our findings show that choosing the right retrieval
  algorithms often leads to bigger performance gains than simply using a larger language
  model.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: guinet24a
month: 0
tex_title: Automated Evaluation of Retrieval-Augmented Language Models with Task-Specific
  Exam Generation
firstpage: 16773
lastpage: 16801
page: 16773-16801
order: 16773
cycles: false
bibtex_author: Guinet, Gauthier and Omidvar-Tehrani, Behrooz and Deoras, Anoop and
  Callot, Laurent
author:
- given: Gauthier
  family: Guinet
- given: Behrooz
  family: Omidvar-Tehrani
- given: Anoop
  family: Deoras
- given: Laurent
  family: Callot
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://raw.githubusercontent.com/mlresearch/v235/main/assets/guinet24a/guinet24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
