---
title: Transformers Provably Learn Sparse Token Selection While Fully-Connected Nets
  Cannot
openreview: qjqlhWDcId
abstract: The transformer architecture has prevailed in various deep learning settings
  due to its exceptional capabilities to select and compose structural information.
  Motivated by these capabilities, Sanford et al. (2023) proposed the <em>sparse token
  selection</em> task, in which transformers excel while fully-connected networks
  (FCNs) fail in the worst case. Building upon that, we strengthen the FCN lower bound
  to an average-case setting and establish an algorithmic separation of transformers
  over FCNs. Specifically, a one-layer transformer trained with gradient descent provably
  learns the sparse token selection task and, surprisingly, exhibits strong out-of-distribution
  length generalization. We provide empirical simulations to justify our theoretical
  findings.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: wang24ca
month: 0
tex_title: Transformers Provably Learn Sparse Token Selection While Fully-Connected
  Nets Cannot
firstpage: 51854
lastpage: 51912
page: 51854-51912
order: 51854
cycles: false
bibtex_author: Wang, Zixuan and Wei, Stanley and Hsu, Daniel and Lee, Jason D.
author:
- given: Zixuan
  family: Wang
- given: Stanley
  family: Wei
- given: Daniel
  family: Hsu
- given: Jason D.
  family: Lee
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/assets/wang24ca/wang24ca.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
