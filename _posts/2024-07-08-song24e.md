---
title: Sparse is Enough in Fine-tuning Pre-trained Large Language Models
openreview: 10hu2D3hAg
abstract: With the prevalence of pre-training-fine-tuning paradigm, how to efficiently
  adapt the pre-trained model to the downstream tasks has been an intriguing issue.
  $\textbf{P}$arameter-$\textbf{E}$fficient $\textbf{F}$ine-$\textbf{T}$uning(PEFT)
  methods have been proposed for low-cost adaptation. Although PEFT has demonstrated
  effectiveness and been widely applied, the underlying principles are still unclear.
  In this paper, we adopt the PAC-Bayesian generalization error bound, viewing pre-training
  as a shift of prior distribution which leads to a tighter bound for generalization
  error. We validate this shift from the perspectives of oscillations in the loss
  landscape and the quasi-sparsity in gradient distribution. Based on this, we propose
  a gradient-based sparse fine-tuning algorithm, named $\textbf{S}$parse $\textbf{I}$ncrement
  $\textbf{F}$ine-$\textbf{T}$uning(SIFT), and validate its effectiveness on a range
  of tasks including the GLUE Benchmark and Instruction-tuning. The code is accessible
  at https://github.com/song-wx/SIFT/.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: song24e
month: 0
tex_title: Sparse is Enough in Fine-tuning Pre-trained Large Language Models
firstpage: 46121
lastpage: 46135
page: 46121-46135
order: 46121
cycles: false
bibtex_author: Song, Weixi and Li, Zuchao and Zhang, Lefei and Zhao, Hai and Du, Bo
author:
- given: Weixi
  family: Song
- given: Zuchao
  family: Li
- given: Lefei
  family: Zhang
- given: Hai
  family: Zhao
- given: Bo
  family: Du
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/assets/song24e/song24e.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
