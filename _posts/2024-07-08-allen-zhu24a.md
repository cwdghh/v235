---
title: 'Physics of Language Models: Part 3.1, Knowledge Storage and Extraction'
openreview: 5x788rqbcj
abstract: Large language models (LLMs) can store a vast amount of world knowledge,
  often extractable via question-answering (e.g., "What is Abraham Lincoln’s birthday?”).
  However, do they answer such questions based on exposure to similar questions during
  training (i.e., cheating), or by genuinely learning to extract knowledge from sources
  like Wikipedia? In this paper, we investigate this issue using a controlled biography
  dataset. We find a strong correlation between the model’s ability to extract knowledge
  and various <em>diversity measures</em> of the training data. <b>Essentially</b>,
  for knowledge to be reliably extracted, it must be sufficiently augmented (e.g.,
  through paraphrasing, sentence shuffling) <em>during pretraining</em>. Without such
  augmentation, knowledge may be memorized but not extractable, leading to 0% accuracy,
  regardless of subsequent instruction fine-tuning. To understand why this occurs,
  we employ (nearly) linear probing to demonstrate a strong connection between the
  observed correlation and <em>how the model internally encodes knowledge</em> — whether
  it is linearly encoded in the hidden embeddings of entity names or distributed across
  other token embeddings in the training text. <b>This paper provides several key
  recommendations for LLM pretraining in the industry:</b> (1) rewrite the pretraining
  data — using small, auxiliary models — to provide knowledge augmentation, and (2)
  incorporate more instruction-finetuning data into the pretraining stage before it
  becomes too late.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: allen-zhu24a
month: 0
tex_title: 'Physics of Language Models: Part 3.1, Knowledge Storage and Extraction'
firstpage: 1067
lastpage: 1077
page: 1067-1077
order: 1067
cycles: false
bibtex_author: Allen-Zhu, Zeyuan and Li, Yuanzhi
author:
- given: Zeyuan
  family: Allen-Zhu
- given: Yuanzhi
  family: Li
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://raw.githubusercontent.com/mlresearch/v235/main/assets/allen-zhu24a/allen-zhu24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
