---
title: Scaling Exponents Across Parameterizations and Optimizers
openreview: 0ksNeD1SJT
abstract: Robust and effective scaling of models from small to large width typically
  requires the precise adjustment of many algorithmic and architectural details, such
  as parameterization and optimizer choices. In this work, we propose a new perspective
  on parameterization by investigating a key assumption in prior work about the alignment
  between parameters and data and derive new theoretical results under weaker assumptions
  and a broader set of optimizers. Our extensive empirical investigation includes
  <em>tens of thousands</em> of models trained with <em>all combinations of</em> three
  optimizers, four parameterizations, several alignment assumptions, more than a dozen
  learning rates, and fourteen model sizes up to 27B parameters. We find that the
  best learning rate scaling prescription would often have been excluded by the assumptions
  in prior work. Our results show that all parameterizations, not just maximal update
  parameterization (muP), can achieve hyperparameter transfer; moreover, our novel
  per-layer learning rate prescription for standard parameterization outperforms muP.
  Finally, we demonstrate that an overlooked aspect of parameterization, the epsilon
  parameter in Adam, must be scaled correctly to avoid gradient underflow and propose
  <em>Adam-atan2</em>, a new numerically stable, scale-invariant version of Adam that
  eliminates the epsilon hyperparameter entirely.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: everett24a
month: 0
tex_title: Scaling Exponents Across Parameterizations and Optimizers
firstpage: 12666
lastpage: 12700
page: 12666-12700
order: 12666
cycles: false
bibtex_author: Everett, Katie E and Xiao, Lechao and Wortsman, Mitchell and Alemi,
  Alexander A and Novak, Roman and Liu, Peter J and Gur, Izzeddin and Sohl-Dickstein,
  Jascha and Kaelbling, Leslie Pack and Lee, Jaehoon and Pennington, Jeffrey
author:
- given: Katie E
  family: Everett
- given: Lechao
  family: Xiao
- given: Mitchell
  family: Wortsman
- given: Alexander A
  family: Alemi
- given: Roman
  family: Novak
- given: Peter J
  family: Liu
- given: Izzeddin
  family: Gur
- given: Jascha
  family: Sohl-Dickstein
- given: Leslie Pack
  family: Kaelbling
- given: Jaehoon
  family: Lee
- given: Jeffrey
  family: Pennington
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://raw.githubusercontent.com/mlresearch/v235/main/assets/everett24a/everett24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
