---
title: 'Codebook Features: Sparse and Discrete Interpretability for Neural Networks'
openreview: VZ5A0LPbnc
abstract: Understanding neural networks is challenging in part because of the dense,
  continuous nature of their hidden states. We explore whether we can train neural
  networks to have hidden states that are sparse, discrete, and more interpretable
  by quantizing their continuous features into what we call codebook features. Codebook
  features are produced by finetuning neural networks with vector quantization bottlenecks
  at each layer, producing a network whose hidden features are the sum of a small
  number of discrete vector codes chosen from a larger codebook. Surprisingly, we
  find that neural networks can operate under this extreme bottleneck with only modest
  degradation in performance. In addition, we can control a modelâ€™s behavior by finding
  codes that activate on a desired behavior, then activating those same codes during
  generation. We first validate codebook features on a finite state machine dataset
  with far more hidden states than neurons. In this setting, our approach overcomes
  the superposition problem by assigning states to distinct codes, and we find that
  we can make the neural network behave as if it is in a different state by activating
  the code for that state. We then train Transformer language models with up to 410M
  parameters on two natural language datasets. We identify codes in these models representing
  diverse, disentangled concepts (ranging from negative emotions to months of the
  year) and find that we can guide the model to generate different topics and pronoun
  genders by activating these codes during inference. Overall, codebook features appear
  to be a promising unit of analysis and control for neural networks and interpretability.
  Our codebase and models are open-sourced at this URL.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: tamkin24a
month: 0
tex_title: 'Codebook Features: Sparse and Discrete Interpretability for Neural Networks'
firstpage: 47535
lastpage: 47563
page: 47535-47563
order: 47535
cycles: false
bibtex_author: Tamkin, Alex and Taufeeque, Mohammad and Goodman, Noah
author:
- given: Alex
  family: Tamkin
- given: Mohammad
  family: Taufeeque
- given: Noah
  family: Goodman
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/assets/tamkin24a/tamkin24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
