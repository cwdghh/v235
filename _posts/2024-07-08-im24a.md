---
title: Understanding the Learning Dynamics of Alignment with Human Feedback
openreview: Hy88Jp0kQT
abstract: 'Aligning large language models (LLMs) with human intentions has become
  a critical task for safely deploying models in real-world systems. While existing
  alignment approaches have seen empirical success, theoretically understanding how
  these methods affect model behavior remains an open question. Our work provides
  an initial attempt to theoretically analyze the learning dynamics of human preference
  alignment. We formally show how the distribution of preference datasets influences
  the rate of model updates and provide rigorous guarantees on the training accuracy.
  Our theory also reveals an intricate phenomenon where the optimization is prone
  to prioritizing certain behaviors with higher preference distinguishability. We
  empirically validate our findings on contemporary LLMs and alignment tasks, reinforcing
  our theoretical insights and shedding light on considerations for future alignment
  approaches. Disclaimer: This paper contains potentially offensive text; reader discretion
  is advised.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: im24a
month: 0
tex_title: Understanding the Learning Dynamics of Alignment with Human Feedback
firstpage: 20983
lastpage: 21006
page: 20983-21006
order: 20983
cycles: false
bibtex_author: Im, Shawn and Li, Yixuan
author:
- given: Shawn
  family: Im
- given: Yixuan
  family: Li
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/assets/im24a/im24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
