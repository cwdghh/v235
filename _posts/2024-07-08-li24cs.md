---
title: 'Algorithmic Stability Unleashed: Generalization Bounds with Unbounded Losses'
openreview: 6yQ5mIYxjj
abstract: One of the central problems of statistical learning theory is quantifying
  the generalization ability of learning algorithms within a probabilistic framework.
  Algorithmic stability is a powerful tool for deriving generalization bounds, however,
  it typically builds on a critical assumption that losses are bounded. In this paper,
  we relax this condition to unbounded loss functions with subweibull diameter. This
  gives new generalization bounds for algorithmic stability and also includes existing
  results of subgaussian and subexponential diameters as specific cases. Furthermore,
  we provide a refined stability analysis by developing generalization bounds which
  can be $\sqrt{n}$-times faster than the previous results, where $n$ is the sample
  size. Our main technical contribution is general concentration inequalities for
  subweibull random variables, which may be of independent interest.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: li24cs
month: 0
tex_title: 'Algorithmic Stability Unleashed: Generalization Bounds with Unbounded
  Losses'
firstpage: 29489
lastpage: 29510
page: 29489-29510
order: 29489
cycles: false
bibtex_author: Li, Shaojie and Zhu, Bowei and Liu, Yong
author:
- given: Shaojie
  family: Li
- given: Bowei
  family: Zhu
- given: Yong
  family: Liu
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/li24cs/li24cs.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
