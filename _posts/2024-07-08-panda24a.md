---
title: A New Linear Scaling Rule for Private Adaptive Hyperparameter Optimization
openreview: 5kXNMDpUVF
abstract: An open problem in differentially private deep learning is hyperparameter
  optimization (HPO). DP-SGD introduces new hyperparameters and complicates existing
  ones, forcing researchers to painstakingly tune hyperparameters with hundreds of
  trials, which in turn makes it impossible to account for the privacy cost of HPO
  without destroying the utility. We propose an adaptive HPO method that uses cheap
  trials (in terms of privacy cost and runtime) to estimate optimal hyperparameters
  and scales them up. We obtain state-of-the-art performance on 22 benchmark tasks,
  across computer vision and natural language processing, across pretraining and finetuning,
  across architectures and a wide range of $\varepsilon \in [0.01,8.0]$, all while
  accounting for the privacy cost of HPO.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: panda24a
month: 0
tex_title: A New Linear Scaling Rule for Private Adaptive Hyperparameter Optimization
firstpage: 39364
lastpage: 39399
page: 39364-39399
order: 39364
cycles: false
bibtex_author: Panda, Ashwinee and Tang, Xinyu and Mahloujifar, Saeed and Sehwag,
  Vikash and Mittal, Prateek
author:
- given: Ashwinee
  family: Panda
- given: Xinyu
  family: Tang
- given: Saeed
  family: Mahloujifar
- given: Vikash
  family: Sehwag
- given: Prateek
  family: Mittal
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://raw.githubusercontent.com/mlresearch/v235/main/assets/panda24a/panda24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
