---
title: Embodied CoT Distillation From LLM To Off-the-shelf Agents
openreview: M4Htd52HMH
abstract: We address the challenge of utilizing large language models (LLMs) for complex
  embodied tasks, in the environment where decision-making systems operate timely
  on capacity-limited, off-the-shelf devices. We present DeDer, a framework for decomposing
  and distilling the embodied reasoning capabilities from LLMs to efficient, small
  language model (sLM)-based policies. In DeDer, the decision-making process of LLM-based
  strategies is restructured into a hierarchy with a reasoning-policy and planning-policy.
  The reasoning-policy is distilled from the data that is generated through the embodied
  in-context learning and self-verification of an LLM, so it can produce effective
  rationales. The planning-policy, guided by the rationales, can render optimized
  plans efficiently. In turn, DeDer allows for adopting sLMs for both policies, deployed
  on off-the-shelf devices. Furthermore, to enhance the quality of intermediate rationales,
  specific to embodied tasks, we devise the embodied knowledge graph, and to generate
  multiple rationales timely through a single inference, we also use the contrastively
  prompted attention model. Our experiments with the ALFRED benchmark demonstrate
  that DeDer surpasses leading language planning and distillation approaches, indicating
  the applicability and efficiency of sLM-based embodied policies derived through
  DeDer.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: choi24d
month: 0
tex_title: Embodied {C}o{T} Distillation From {LLM} To Off-the-shelf Agents
firstpage: 8702
lastpage: 8721
page: 8702-8721
order: 8702
cycles: false
bibtex_author: Choi, Wonje and Kim, Woo Kyung and Yoo, Minjong and Woo, Honguk
author:
- given: Wonje
  family: Choi
- given: Woo Kyung
  family: Kim
- given: Minjong
  family: Yoo
- given: Honguk
  family: Woo
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/assets/choi24d/choi24d.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
