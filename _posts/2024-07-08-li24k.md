---
title: Evolving Subnetwork Training for Large Language Models
openreview: DbMm8pmoAP
abstract: 'Large language models have ushered in a new era of artificial intelligence
  research. However, their substantial training costs hinder further development and
  widespread adoption. In this paper, inspired by the redundancy in the parameters
  of large language models, we propose a novel training paradigm: Evolving Subnetwork
  Training (EST). EST samples subnetworks from the layers of the large language model
  and from commonly used modules within each layer, Multi-Head Attention (MHA) and
  Multi-Layer Perceptron (MLP). By gradually increasing the size of the subnetworks
  during the training process, EST can save the cost of training. We apply EST to
  train GPT2 model and TinyLlama model, resulting in 26.7% FLOPs saving for GPT2 and
  25.0% for TinyLlama without an increase in loss on the pre-training dataset. Moreover,
  EST leads to performance improvements in downstream tasks, indicating that it benefits
  generalization. Additionally, we provide intuitive theoretical studies based on
  training dynamics and Dropout theory to ensure the feasibility of EST.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: li24k
month: 0
tex_title: Evolving Subnetwork Training for Large Language Models
firstpage: 27547
lastpage: 27562
page: 27547-27562
order: 27547
cycles: false
bibtex_author: Li, Hanqi and Chen, Lu and Ma, Da and Wu, Zijian and Zhu, Su and Yu,
  Kai
author:
- given: Hanqi
  family: Li
- given: Lu
  family: Chen
- given: Da
  family: Ma
- given: Zijian
  family: Wu
- given: Su
  family: Zhu
- given: Kai
  family: Yu
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://raw.githubusercontent.com/mlresearch/v235/main/assets/li24k/li24k.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
