---
title: 'ETHER: Efficient Finetuning of Large-Scale Models with Hyperplane Reflections'
openreview: yPDTXQwUPy
abstract: Parameter-efficient finetuning (PEFT) has become ubiquitous to adapt foundation
  models to downstream task requirements while retaining their generalization ability.
  However, the amount of additionally introduced parameters and compute for successful
  adaptation and hyperparameter searches can explode quickly, especially when deployed
  at scale to serve numerous individual requests. To ensure effective, parameter-efficient,
  and hyperparameter-robust adaptation, we propose the <em>ETHER</em> transformation
  family, which performs Efficient fineTuning via HypErplane Reflections. By design,
  <em>ETHER</em> transformations require <em>a minimal number of parameters</em>,
  are <em>less likely to deteriorate model performance</em>, and exhibit <em>robustness
  to hyperparameter and learning rate choices</em>. In particular, we introduce <em>ETHER</em>
  and its relaxation <em>ETHER+</em>, which match or outperform existing PEFT methods
  with significantly fewer parameters ($\sim$$10$-$100$ times lower than LoRA or OFT)
  across multiple image synthesis and natural language tasks without <em>exhaustive
  hyperparameter tuning</em>. Finally, we investigate the recent emphasis on Hyperspherical
  Energy retention for adaptation and raise questions on its practical utility. The
  code is available at https://github.com/mwbini/ether.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: bini24a
month: 0
tex_title: "{ETHER}: Efficient Finetuning of Large-Scale Models with Hyperplane Reflections"
firstpage: 4007
lastpage: 4026
page: 4007-4026
order: 4007
cycles: false
bibtex_author: Bini, Massimo and Roth, Karsten and Akata, Zeynep and Khoreva, Anna
author:
- given: Massimo
  family: Bini
- given: Karsten
  family: Roth
- given: Zeynep
  family: Akata
- given: Anna
  family: Khoreva
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://raw.githubusercontent.com/mlresearch/v235/main/assets/bini24a/bini24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
