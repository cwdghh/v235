---
title: 'Position: Compositional Generative Modeling: A Single Model is Not All You
  Need'
openreview: SoNexFx8qz
abstract: Large monolithic generative models trained on massive amounts of data have
  become an increasingly dominant approach in AI research. In this paper, we argue
  that we should instead construct large generative systems by composing smaller generative
  models together. We show how such a compositional generative approach enables us
  to learn distributions in a more data-efficient manner, enabling generalization
  to parts of the data distribution unseen at training time. We further show how this
  enables us to program and construct new generative models for tasks completely unseen
  at training. Finally, we show that in many cases, we can discover separate compositional
  components from data.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: du24d
month: 0
tex_title: 'Position: Compositional Generative Modeling: A Single Model is Not All
  You Need'
firstpage: 11721
lastpage: 11732
page: 11721-11732
order: 11721
cycles: false
bibtex_author: Du, Yilun and Kaelbling, Leslie Pack
author:
- given: Yilun
  family: Du
- given: Leslie Pack
  family: Kaelbling
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://raw.githubusercontent.com/mlresearch/v235/main/assets/du24d/du24d.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
