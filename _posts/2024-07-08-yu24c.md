---
title: Activation-Descent Regularization for Input Optimization of ReLU Networks
openreview: IArWwIim8M
abstract: We present a new approach for input optimization of ReLU networks that explicitly
  takes into account the effect of changes in activation patterns. We analyze local
  optimization steps in both the input space and the space of activation patterns
  to propose methods with superior local descent properties. To accomplish this, we
  convert the discrete space of activation patterns into differentiable representations
  and propose regularization terms that improve each descent step. Our experiments
  demonstrate the effectiveness of the proposed input-optimization methods for improving
  the state-of-the-art in various areas, such as adversarial learning, generative
  modeling, and reinforcement learning.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: yu24c
month: 0
tex_title: Activation-Descent Regularization for Input Optimization of {R}e{LU} Networks
firstpage: 57441
lastpage: 57458
page: 57441-57458
order: 57441
cycles: false
bibtex_author: Yu, Hongzhan and Gao, Sicun
author:
- given: Hongzhan
  family: Yu
- given: Sicun
  family: Gao
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/assets/yu24c/yu24c.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
