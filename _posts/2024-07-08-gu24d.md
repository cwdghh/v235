---
title: Data-free Distillation of Diffusion Models with Bootstrapping
openreview: jw2f9v59g0
abstract: Diffusion models have demonstrated great potential for generating diverse
  images. However, their performance often suffers from slow generation due to iterative
  denoising. Knowledge distillation has been recently proposed as a remedy which can
  reduce the number of inference steps to one or a few, without significant quality
  degradation. However, existing distillation methods either require significant amounts
  of offline computation for generating synthetic training data from the teacher model,
  or need to perform expensive online learning with the help of real data. In this
  work, we present a novel technique called BOOT, that overcomes these limitations
  with an efficient data-free distillation algorithm. The core idea is to learn a
  time-conditioned model that predicts the output of a pre-trained diffusion model
  teacher given any time-step. Such a model can be efficiently trained based on bootstrapping
  from two consecutive sampled steps. Furthermore, our method can be easily adapted
  to large-scale text-to-image diffusion models, which are challenging for previous
  methods given the fact that the training sets are often large and difficult to access.
  We demonstrate the effectiveness of our approach on several benchmark datasets in
  the DDIM setting, achieving comparable generation quality while being orders of
  magnitude faster than the diffusion teacher. The text-to-image results show that
  the proposed approach is able to handle highly complex distributions, shedding light
  on more efficient generative modeling.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: gu24d
month: 0
tex_title: Data-free Distillation of Diffusion Models with Bootstrapping
firstpage: 16622
lastpage: 16646
page: 16622-16646
order: 16622
cycles: false
bibtex_author: Gu, Jiatao and Wang, Chen and Zhai, Shuangfei and Zhang, Yizhe and
  Liu, Lingjie and Susskind, Joshua M.
author:
- given: Jiatao
  family: Gu
- given: Chen
  family: Wang
- given: Shuangfei
  family: Zhai
- given: Yizhe
  family: Zhang
- given: Lingjie
  family: Liu
- given: Joshua M.
  family: Susskind
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/gu24d/gu24d.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
