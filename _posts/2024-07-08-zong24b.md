---
title: Fool Your (Vision and) Language Model with Embarrassingly Simple Permutations
openreview: IUijgjJgWO
abstract: Large language and vision-language models are rapidly being deployed in
  practice thanks to their impressive capabilities in instruction following, in-context
  learning, and so on. This raises an urgent need to carefully analyse their robustness
  so that stakeholders can understand if and when such models are trustworthy enough
  to be relied upon in any given application. In this paper, we highlight a specific
  vulnerability in popular models, namely permutation sensitivity in multiple-choice
  question answering (MCQA). Specifically, we show empirically that popular models
  are vulnerable to adversarial permutation in answer sets for multiple-choice prompting,
  which is surprising as models should ideally be as invariant to prompt permutation
  as humans are. These vulnerabilities persist across various model sizes, and exist
  in very recent language and vision-language models. Code to reproduce all experiments
  is provided in supplementary materials.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: zong24b
month: 0
tex_title: Fool Your ({V}ision and) Language Model with Embarrassingly Simple Permutations
firstpage: 62892
lastpage: 62913
page: 62892-62913
order: 62892
cycles: false
bibtex_author: Zong, Yongshuo and Yu, Tingyang and Chavhan, Ruchika and Zhao, Bingchen
  and Hospedales, Timothy
author:
- given: Yongshuo
  family: Zong
- given: Tingyang
  family: Yu
- given: Ruchika
  family: Chavhan
- given: Bingchen
  family: Zhao
- given: Timothy
  family: Hospedales
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/assets/zong24b/zong24b.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
