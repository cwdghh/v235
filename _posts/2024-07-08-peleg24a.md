---
title: 'Bias of Stochastic Gradient Descent or the Architecture: Disentangling the
  Effects of Overparameterization of Neural Networks'
openreview: znz261CQK7
abstract: Neural networks typically generalize well when fitting the data perfectly,
  even though they are heavily overparameterized. Many factors have been pointed out
  as the reason for this phenomenon, including an implicit bias of stochastic gradient
  descent (SGD) and a possible simplicity bias arising from the neural network architecture.
  The goal of this paper is to disentangle the factors that influence generalization
  stemming from optimization and architectural choices by studying <em>random</em>
  and <em>SGD-optimized</em> networks that achieve zero training error. We experimentally
  show, in the low sample regime, that overparameterization in terms of increasing
  width is beneficial for generalization, and this benefit is due to the bias of SGD
  and not due to an architectural bias. In contrast, for increasing depth, overparameterization
  is detrimental for generalization, but random and SGD-optimized networks behave
  similarly, so this can be attributed to an architectural bias.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: peleg24a
month: 0
tex_title: 'Bias of Stochastic Gradient Descent or the Architecture: Disentangling
  the Effects of Overparameterization of Neural Networks'
firstpage: 40154
lastpage: 40184
page: 40154-40184
order: 40154
cycles: false
bibtex_author: Peleg, Amit and Hein, Matthias
author:
- given: Amit
  family: Peleg
- given: Matthias
  family: Hein
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://raw.githubusercontent.com/mlresearch/v235/main/assets/peleg24a/peleg24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
