---
title: 'UP2ME: Univariate Pre-training to Multivariate Fine-tuning as a General-purpose
  Framework for Multivariate Time Series Analysis'
openreview: aR3uxWlZhX
abstract: Despite the success of self-supervised pre-training in texts and images,
  applying it to multivariate time series (MTS) falls behind tailored methods for
  tasks like forecasting, imputation and anomaly detection. We propose a general-purpose
  framework, named UP2ME (<b>U</b>nivariate <b>P</b>re-training to <b>M</b>ultivariate
  Fin<b>e</b>-tuning). It conducts task-agnostic pre-training when downstream tasks
  are unspecified. Once the task and setting (e.g. forecasting length) are determined,
  it gives sensible solutions with frozen pre-trained parameters, which has not been
  achieved before. UP2ME is further refined by fine-tuning. A univariate-to-multivariate
  paradigm is devised to address the heterogeneity of temporal and cross-channel dependencies.
  In univariate pre-training, univariate instances with diverse lengths are generated
  for Masked AutoEncoder (MAE) pre-training, discarding cross-channel dependency.
  The pre-trained model handles downstream tasks by formulating them into specific
  mask-reconstruction problems. In multivariate fine-tuning, it constructs a dependency
  graph among channels using the pre-trained encoder to enhance cross-channel dependency
  capture. Experiments on eight real-world datasets show its SOTA performance in forecasting
  and imputation, approaching task-specific performance in anomaly detection. Our
  code is available at https://github.com/Thinklab-SJTU/UP2ME.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: zhang24al
month: 0
tex_title: "{UP}2{ME}: Univariate Pre-training to Multivariate Fine-tuning as a General-purpose
  Framework for Multivariate Time Series Analysis"
firstpage: 59358
lastpage: 59381
page: 59358-59381
order: 59358
cycles: false
bibtex_author: Zhang, Yunhao and Liu, Minghao and Zhou, Shengyang and Yan, Junchi
author:
- given: Yunhao
  family: Zhang
- given: Minghao
  family: Liu
- given: Shengyang
  family: Zhou
- given: Junchi
  family: Yan
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://raw.githubusercontent.com/mlresearch/v235/main/assets/zhang24al/zhang24al.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
