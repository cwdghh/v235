---
title: Learning Modality Knowledge Alignment for Cross-Modality Transfer
openreview: lmiurzioja
abstract: Cross-modality transfer aims to leverage large pretrained models to complete
  tasks that may not belong to the modality of pretraining data. Existing works achieve
  certain success in extending classical finetuning to cross-modal scenarios, yet
  we still lack understanding about the influence of modality gap on the transfer.
  In this work, a series of experiments focusing on the source representation quality
  during transfer are conducted, revealing the connection between larger modality
  gap and lesser knowledge reuse which means ineffective transfer. We then formalize
  the gap as the knowledge misalignment between modalities using conditional distribution
  $P(Y|X)$. Towards this problem, we present <b>Mo</b>dality k<b>N</b>owledge <b>A</b>lignment
  (MoNA), a meta-learning approach that learns target data transformation to reduce
  the modality knowledge discrepancy ahead of the transfer. Experiments show that
  the approach significantly improves upon cross-modal finetuning methods, and most
  importantly leads to better reuse of source modality knowledge.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: ma24d
month: 0
tex_title: Learning Modality Knowledge Alignment for Cross-Modality Transfer
firstpage: 33777
lastpage: 33793
page: 33777-33793
order: 33777
cycles: false
bibtex_author: Ma, Wenxuan and Li, Shuang and Cai, Lincan and Kang, Jingxuan
author:
- given: Wenxuan
  family: Ma
- given: Shuang
  family: Li
- given: Lincan
  family: Cai
- given: Jingxuan
  family: Kang
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/assets/ma24d/ma24d.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
