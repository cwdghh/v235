---
title: Towards a Better Theoretical Understanding of Independent Subnetwork Training
openreview: XUc29ydmLX
abstract: Modern advancements in large-scale machine learning would be impossible
  without the paradigm of data-parallel distributed computing. Since distributed computing
  with large-scale models imparts excessive pressure on communication channels, significant
  recent research has been directed toward co-designing communication compression
  strategies and training algorithms with the goal of reducing communication costs.
  While pure data parallelism allows better data scaling, it suffers from poor model
  scaling properties. Indeed, compute nodes are severely limited by memory constraints,
  preventing further increases in model size. For this reason, the latest achievements
  in training giant neural network models also rely on some form of model parallelism.
  In this work, we take a closer theoretical look at Independent Subnetwork Training
  (IST), which is a recently proposed and highly effective technique for solving the
  aforementioned problems. We identify fundamental differences between IST and alternative
  approaches, such as distributed methods with compressed communication, and provide
  a precise analysis of its optimization performance on a quadratic model.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: shulgin24a
month: 0
tex_title: Towards a Better Theoretical Understanding of Independent Subnetwork Training
firstpage: 45258
lastpage: 45285
page: 45258-45285
order: 45258
cycles: false
bibtex_author: Shulgin, Egor and Richt\'{a}rik, Peter
author:
- given: Egor
  family: Shulgin
- given: Peter
  family: Richt√°rik
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://raw.githubusercontent.com/mlresearch/v235/main/assets/shulgin24a/shulgin24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
