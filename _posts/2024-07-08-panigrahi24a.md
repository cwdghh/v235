---
title: Trainable Transformer in Transformer
openreview: JcxlFe2fGC
abstract: Recent works attribute the capability of in-context learning (ICL) in large
  pre-trained language models to implicitly simulating and fine-tuning an internal
  model (e.g., linear or 2-layer MLP) during inference. However, such constructions
  require large memory overhead, which makes simulation of more sophisticated internal
  models intractable. In this work, we propose a new efficient construction, Transformer
  in Transformer (in short, TINT), that allows a transformer to simulate and fine-tune
  more complex models during inference (e.g., pre-trained language models). In particular,
  we introduce innovative approximation techniques that allow a TINT model with less
  than 2 billion parameters to simulate and fine-tune a 125 million parameter transformer
  model within a single forward pass. TINT accommodates many common transformer variants
  and its design ideas also improve the efficiency of past instantiations of simple
  models inside transformers. We conduct end-to-end experiments to validate the internal
  fine-tuning procedure of TINT on various language modeling and downstream tasks.
  For example, even with a limited one-step budget, we observe TINT for a OPT-125M
  model improves performance by 4 âˆ’ 16% absolute on average compared to OPT-125M.
  These findings suggest that large pre-trained language models are capable of performing
  intricate subroutines. To facilitate further work, a modular and extensible codebase
  for TINT is included.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: panigrahi24a
month: 0
tex_title: Trainable Transformer in Transformer
firstpage: 39448
lastpage: 39492
page: 39448-39492
order: 39448
cycles: false
bibtex_author: Panigrahi, Abhishek and Malladi, Sadhika and Xia, Mengzhou and Arora,
  Sanjeev
author:
- given: Abhishek
  family: Panigrahi
- given: Sadhika
  family: Malladi
- given: Mengzhou
  family: Xia
- given: Sanjeev
  family: Arora
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/panigrahi24a/panigrahi24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
