---
title: Neuro-Symbolic Temporal Point Processes
openreview: HDrXBr26UI
abstract: Our goal is to $\textit{efficiently}$ discover a compact set of temporal
  logic rules to explain irregular events of interest. We introduce a neural-symbolic
  rule induction framework within the temporal point process model. The negative log-likelihood
  is the loss that guides the learning, where the explanatory logic rules and their
  weights are learned end-to-end in a $\textit{differentiable}$ way. Specifically,
  predicates and logic rules are represented as $\textit{vector embeddings}$, where
  the predicate embeddings are fixed and the rule embeddings are trained via gradient
  descent to obtain the most appropriate compositional representations of the predicate
  embeddings. To make the rule learning process more efficient and flexible, we adopt
  a $\textit{sequential covering algorithm}$, which progressively adds rules to the
  model and removes the event sequences that have been explained until all event sequences
  have been covered. All the found rules will be fed back to the models for a final
  rule embedding and weight refinement. Our approach showcases notable efficiency
  and accuracy across synthetic and real datasets, surpassing state-of-the-art baselines
  by a wide margin in terms of efficiency.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: yang24ag
month: 0
tex_title: Neuro-Symbolic Temporal Point Processes
firstpage: 56665
lastpage: 56680
page: 56665-56680
order: 56665
cycles: false
bibtex_author: Yang, Yang and Yang, Chao and Li, Boyang and Fu, Yinghao and Li, Shuang
author:
- given: Yang
  family: Yang
- given: Chao
  family: Yang
- given: Boyang
  family: Li
- given: Yinghao
  family: Fu
- given: Shuang
  family: Li
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/yang24ag/yang24ag.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
