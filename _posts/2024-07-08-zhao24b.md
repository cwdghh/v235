---
title: 'Long Is More for Alignment: A Simple but Tough-to-Beat Baseline for Instruction
  Fine-Tuning'
openreview: 0AZAjkXhit
abstract: There is a consensus that instruction fine-tuning of LLMs requires high-quality
  data, but what are they? LIMA (NeurIPS 2023) and AlpaGasus (ICLR 2024) are state-of-the-art
  methods for selecting such high-quality examples, either via manual curation or
  using GPT-3.5-Turbo as a quality scorer. We show that the extremely simple baseline
  of selecting the 1,000 instructions with longest responses—that intuitively contain
  more learnable information and are harder to overfit—from standard datasets can
  consistently outperform these sophisticated methods according to GPT-4 and PaLM-2
  as judges, while remaining competitive on the Open LLM benchmarks that test factual
  knowledge. We demonstrate this for several LLMs (Llama-2-7B, Llama-2-13B, Mistral-7B-v0.1)
  and datasets (Alpaca-52k, Evol-Instruct-70k). In addition, a lightweight refinement
  of such long instructions can further improve the abilities of the fine-tuned LLMs,
  and allows us to obtain competitive results on MT-Bench and the 2nd highest-ranked
  Llama-2-7B-based model on AlpacaEval 2.0, while training on only 1,000 examples
  and no extra preference data. We also conduct a thorough analysis of our models
  to ensure that their enhanced performance is not simply due to GPT-4’s preference
  for longer responses. Overall, our findings suggest that fine-tuning on the longest
  responses should be the default baseline for any work on instruction fine-tuning.
  We provide our code in this GitHub repository.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: zhao24b
month: 0
tex_title: 'Long Is More for Alignment: A Simple but Tough-to-Beat Baseline for Instruction
  Fine-Tuning'
firstpage: 60674
lastpage: 60703
page: 60674-60703
order: 60674
cycles: false
bibtex_author: Zhao, Hao and Andriushchenko, Maksym and Croce, Francesco and Flammarion,
  Nicolas
author:
- given: Hao
  family: Zhao
- given: Maksym
  family: Andriushchenko
- given: Francesco
  family: Croce
- given: Nicolas
  family: Flammarion
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/zhao24b/zhao24b.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
