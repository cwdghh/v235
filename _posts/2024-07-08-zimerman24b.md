---
title: Viewing Transformers Through the Lens of Long Convolutions Layers
openreview: nOyj26YdIQ
abstract: Despite their dominance in modern DL and, especially, NLP domains, transformer
  architectures exhibit sub-optimal performance on long-range tasks compared to recent
  layers that are specifically designed for this purpose. In this work, drawing inspiration
  from key attributes of longrange layers, such as state-space layers, linear RNN
  layers, and global convolution layers, we demonstrate that minimal modifications
  to the transformer architecture can significantly enhance performance on the Long
  Range Arena (LRA) benchmark, thus narrowing the gap with these specialized layers.
  We identify that two key principles for long-range tasks are (i) incorporating an
  inductive bias towards smoothness, and (ii) locality. As we show, integrating these
  ideas into the attention mechanism improves results with a negligible amount of
  additional computation and without any additional trainable parameters. Our theory
  and experiments also shed light on the reasons for the inferior performance of transformers
  on long-range tasks and identify critical properties that are essential for successfully
  capturing long-range dependencies.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: zimerman24b
month: 0
tex_title: Viewing Transformers Through the Lens of Long Convolutions Layers
firstpage: 62815
lastpage: 62831
page: 62815-62831
order: 62815
cycles: false
bibtex_author: Zimerman, Itamar and Wolf, Lior
author:
- given: Itamar
  family: Zimerman
- given: Lior
  family: Wolf
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/assets/zimerman24b/zimerman24b.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
