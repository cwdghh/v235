---
title: 'Repeat After Me: Transformers are Better than State Space Models at Copying'
openreview: duRRoGeoQT
abstract: Transformers are the dominant architecture for sequence modeling, but there
  is growing interest in models that use a fixed-size latent state that does not depend
  on the sequence length, which we refer to as ”generalized state space models” (GSSMs).
  In this paper we show that while GSSMs are promising in terms of inference-time
  efficiency, they are limited compared to transformer models on tasks that require
  copying from the input context. We start with a theoretical analysis of the simple
  task of string copying and prove that a two layer transformer can copy strings of
  exponential length while GSSMs are fundamentally limited by their fixed-size latent
  state. Empirically, we find that transformers outperform GSSMs in terms of efficiency
  and generalization on synthetic tasks that require copying the context. Finally,
  we evaluate pretrained large language models and find that transformer models dramatically
  outperform state space models at copying and retrieving information from context.
  Taken together, these results suggest a fundamental gap between transformers and
  GSSMs on tasks of practical interest.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: jelassi24a
month: 0
tex_title: 'Repeat After Me: Transformers are Better than State Space Models at Copying'
firstpage: 21502
lastpage: 21521
page: 21502-21521
order: 21502
cycles: false
bibtex_author: Jelassi, Samy and Brandfonbrener, David and Kakade, Sham M. and Malach,
  Eran
author:
- given: Samy
  family: Jelassi
- given: David
  family: Brandfonbrener
- given: Sham M.
  family: Kakade
- given: Eran
  family: Malach
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/assets/jelassi24a/jelassi24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
