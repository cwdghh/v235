---
title: Outlier-aware Slicing for Post-Training Quantization in Vision Transformer
openreview: Uh5XN9d2J4
abstract: 'Post-Training Quantization (PTQ) is a vital technique for network compression
  and acceleration, gaining prominence as model sizes increase. This paper addresses
  a critical challenge in PTQ: <b>the severe impact of outliers on the accuracy of
  quantized transformer architectures.</b> Specifically, we introduce the concept
  of ‘reconstruction granularity’ as a novel solution to this issue, which has been
  overlooked in previous works. Our work provides theoretical insights into the role
  of reconstruction granularity in mitigating the outlier problem in transformer models.
  This theoretical framework is supported by empirical analysis, demonstrating that
  varying reconstruction granularities significantly influence quantization performance.
  Our findings indicate that different architectural designs necessitate distinct
  optimal reconstruction granularities. For instance, the multi-stage Swin Transformer
  architecture benefits from finer granularity, a deviation from the trends observed
  in ViT and DeiT models. We further develop an algorithm for determining the optimal
  reconstruction granularity for various ViT models, achieving state-of-the-art (SOTA)
  performance in PTQ. For example, applying our method to $4$-bit quantization, the
  Swin-Base model achieves a Top-1 accuracy of $82.24%$ on the ImageNet classification
  task. This result surpasses the RepQ-ViT by $3.92%$ ($82.24%$ VS $78.32%$). Similarly,
  our approach elevates the ViT-Small to a Top-1 accuracy of $80.50%$, outperforming
  NoisyQuant by $3.64%$ ($80.50%$ VS $76.86%$). Codes are available in Supplementary
  Materials.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: ma24f
month: 0
tex_title: Outlier-aware Slicing for Post-Training Quantization in Vision Transformer
firstpage: 33811
lastpage: 33825
page: 33811-33825
order: 33811
cycles: false
bibtex_author: Ma, Yuexiao and Li, Huixia and Zheng, Xiawu and Ling, Feng and Xiao,
  Xuefeng and Wang, Rui and Wen, Shilei and Chao, Fei and Ji, Rongrong
author:
- given: Yuexiao
  family: Ma
- given: Huixia
  family: Li
- given: Xiawu
  family: Zheng
- given: Feng
  family: Ling
- given: Xuefeng
  family: Xiao
- given: Rui
  family: Wang
- given: Shilei
  family: Wen
- given: Fei
  family: Chao
- given: Rongrong
  family: Ji
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://raw.githubusercontent.com/mlresearch/v235/main/assets/ma24f/ma24f.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
