---
title: 'Theory of Consistency Diffusion Models: Distribution Estimation Meets Fast
  Sampling'
openreview: pAPykbqUHf
abstract: Diffusion models have revolutionized various application domains, including
  computer vision and audio generation. Despite the state-of-the-art performance,
  diffusion models are known for their slow sample generation due to the extensive
  number of steps involved. In response, consistency models have been developed to
  merge multiple steps in the sampling process, thereby significantly boosting the
  speed of sample generation without compromising quality. This paper contributes
  towards the first statistical theory for consistency models, formulating their training
  as a distribution discrepancy minimization problem. Our analysis yields statistical
  estimation rates based on the Wasserstein distance for consistency models, matching
  those of vanilla diffusion models. Additionally, our results encompass the training
  of consistency models through both distillation and isolation methods, demystifying
  their underlying advantage.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: dou24a
month: 0
tex_title: 'Theory of Consistency Diffusion Models: Distribution Estimation Meets
  Fast Sampling'
firstpage: 11592
lastpage: 11612
page: 11592-11612
order: 11592
cycles: false
bibtex_author: Dou, Zehao and Chen, Minshuo and Wang, Mengdi and Yang, Zhuoran
author:
- given: Zehao
  family: Dou
- given: Minshuo
  family: Chen
- given: Mengdi
  family: Wang
- given: Zhuoran
  family: Yang
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/dou24a/dou24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
