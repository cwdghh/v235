---
title: Universal Consistency of Wide and Deep ReLU Neural Networks and Minimax Optimal
  Convergence Rates for Kolmogorov-Donoho Optimal Function Classes
openreview: OVn8FpeBpG
abstract: In this paper, we prove the universal consistency of wide and deep ReLU
  neural network classifiers. We also give sufficient conditions for a class of probability
  measures for which classifiers based on neural networks achieve minimax optimal
  rates of convergence. The result applies to a wide range of known function classes.
  In particular, while most previous works impose explicit smoothness assumptions
  on the regression function, our framework encompasses more general settings. The
  proposed neural networks are either the minimizers of the $0$-$1$ loss that exhibit
  a benign overfitting behavior.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: ko24b
month: 0
tex_title: Universal Consistency of Wide and Deep {R}e{LU} Neural Networks and Minimax
  Optimal Convergence Rates for Kolmogorov-Donoho Optimal Function Classes
firstpage: 24859
lastpage: 24871
page: 24859-24871
order: 24859
cycles: false
bibtex_author: Ko, Hyunouk and Huo, Xiaoming
author:
- given: Hyunouk
  family: Ko
- given: Xiaoming
  family: Huo
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/assets/ko24b/ko24b.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
