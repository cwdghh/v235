---
title: A Theoretical Analysis of Backdoor Poisoning Attacks in Convolutional Neural
  Networks
openreview: SfcB4cVvPz
abstract: The rising threat of backdoor poisoning attacks (BPAs) on Deep Neural Networks
  (DNNs) has become a significant concern in recent years. In such attacks, the adversaries
  strategically target a specific class and generate a poisoned training set. The
  neural network (NN), well-trained on the poisoned training set, is able to predict
  any input with the trigger pattern as the targeted label, while maintaining accurate
  outputs for clean inputs. However, why the BPAs work remains less explored. To fill
  this gap, we employ a dirty-label attack and conduct a detailed analysis of BPAs
  in a two-layer convolutional neural network. We provide theoretical insights and
  results on the effectiveness of BPAs. Our experimental results on two real-world
  datasets validate our theoretical findings.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: li24at
month: 0
tex_title: A Theoretical Analysis of Backdoor Poisoning Attacks in Convolutional Neural
  Networks
firstpage: 28309
lastpage: 28342
page: 28309-28342
order: 28309
cycles: false
bibtex_author: Li, Boqi and Liu, Weiwei
author:
- given: Boqi
  family: Li
- given: Weiwei
  family: Liu
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/li24at/li24at.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
