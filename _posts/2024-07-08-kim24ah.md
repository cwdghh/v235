---
title: An Infinite-Width Analysis on the Jacobian-Regularised Training of a Neural
  Network
openreview: 8AeuhCgRRv
abstract: The recent theoretical analysis of deep neural networks in their infinite-width
  limits has deepened our understanding of initialisation, feature learning, and training
  of those networks, and brought new practical techniques for finding appropriate
  hyperparameters, learning network weights, and performing inference. In this paper,
  we broaden this line of research by showing that this infinite-width analysis can
  be extended to the Jacobian of a deep neural network. We show that a multilayer
  perceptron (MLP) and its Jacobian at initialisation jointly converge to a Gaussian
  process (GP) as the widths of the MLPâ€™s hidden layers go to infinity and characterise
  this GP. We also prove that in the infinite-width limit, the evolution of the MLP
  under the so-called robust training (i.e., training with a regulariser on the Jacobian)
  is described by a linear first-order ordinary differential equation that is determined
  by a variant of the Neural Tangent Kernel. We experimentally show the relevance
  of our theoretical claims to wide finite networks, and empirically analyse the properties
  of kernel regression solution to obtain an insight into Jacobian regularisation.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: kim24ah
month: 0
tex_title: An Infinite-Width Analysis on the {J}acobian-Regularised Training of a
  Neural Network
firstpage: 24584
lastpage: 24657
page: 24584-24657
order: 24584
cycles: false
bibtex_author: Kim, Taeyoung and Yang, Hongseok
author:
- given: Taeyoung
  family: Kim
- given: Hongseok
  family: Yang
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/kim24ah/kim24ah.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
