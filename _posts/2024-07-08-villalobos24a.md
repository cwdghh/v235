---
title: 'Position: Will we run out of data? Limits of LLM scaling based on human-generated
  data'
openreview: ViZcgDQjyG
abstract: We investigate the potential constraints on LLM scaling posed by the availability
  of public human-generated text data. We forecast the growing demand for training
  data based on current trends and estimate the total stock of public human text data.
  Our findings indicate that if current LLM development trends continue, models will
  be trained on datasets roughly equal in size to the available stock of public human
  text data between 2026 and 2032, or slightly earlier if models are overtrained.
  We explore how progress in language modeling can continue when human-generated text
  datasets cannot be scaled any further. We argue that synthetic data generation,
  transfer learning from data-rich domains, and data efficiency improvements might
  support further progress.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: villalobos24a
month: 0
tex_title: 'Position: Will we run out of data? {L}imits of {LLM} scaling based on
  human-generated data'
firstpage: 49523
lastpage: 49544
page: 49523-49544
order: 49523
cycles: false
bibtex_author: Villalobos, Pablo and Ho, Anson and Sevilla, Jaime and Besiroglu, Tamay
  and Heim, Lennart and Hobbhahn, Marius
author:
- given: Pablo
  family: Villalobos
- given: Anson
  family: Ho
- given: Jaime
  family: Sevilla
- given: Tamay
  family: Besiroglu
- given: Lennart
  family: Heim
- given: Marius
  family: Hobbhahn
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://raw.githubusercontent.com/mlresearch/v235/main/assets/villalobos24a/villalobos24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
