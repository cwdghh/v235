---
title: What Will My Model Forget? Forecasting Forgotten Examples in Language Model
  Refinement
openreview: bzNwexOPWm
abstract: Language models deployed in the wild make errors. However, simply updating
  the model with the corrected error instances causes catastrophic forgettingâ€”the
  updated model makes errors on instances learned during the instruction tuning or
  upstream training phase. Randomly replaying upstream data yields unsatisfactory
  performance and often comes with high variance and poor controllability. To this
  end, we try to forecast upstream examples that will be forgotten due to a model
  update for improved controllability of the replay process and interpretability.
  We train forecasting models given a collection of online learned examples and corresponding
  forgotten upstream pre-training examples. We propose a partially interpretable forecasting
  model based on the observation that changes in pre-softmax logit scores of pretraining
  examples resemble that of online learned examples, which performs decently on BART
  but fails on T5 models. We further show a black-box classifier based on inner products
  of example representations achieves better forecasting performance over a series
  of setups. Finally, we show that we reduce forgetting of upstream pretraining examples
  by replaying examples that are forecasted to be forgotten, demonstrating the practical
  utility of forecasting example forgetting.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: jin24d
month: 0
tex_title: What Will My Model Forget? {F}orecasting Forgotten Examples in Language
  Model Refinement
firstpage: 22145
lastpage: 22159
page: 22145-22159
order: 22145
cycles: false
bibtex_author: Jin, Xisen and Ren, Xiang
author:
- given: Xisen
  family: Jin
- given: Xiang
  family: Ren
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/jin24d/jin24d.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
