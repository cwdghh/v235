---
title: Learning to Explore for Stochastic Gradient MCMC
openreview: aECamk9izk
abstract: Bayesian Neural Networks(BNNs) with high-dimensional parameters pose a challenge
  for posterior inference due to the multi-modality of the posterior distributions.
  Stochastic Gradient Markov Chain Monte Carlo(SGMCMC) with cyclical learning rate
  scheduling is a promising solution, but it requires a large number of sampling steps
  to explore high-dimensional multi-modal posteriors, making it computationally expensive.
  In this paper, we propose a meta-learning strategy to build SGMCMC which can efficiently
  explore the multi-modal target distributions. Our algorithm allows the learned SGMCMC
  to quickly explore the high-density region of the posterior landscape. Also, we
  show that this exploration property is transferrable to various tasks, even for
  the ones unseen during a meta-training stage. Using popular image classification
  benchmarks and a variety of downstream tasks, we demonstrate that our method significantly
  improves the sampling efficiency, achieving better performance than vanilla SGMCMC
  without incurring significant computational overhead.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: kim24k
month: 0
tex_title: Learning to Explore for Stochastic Gradient {MCMC}
firstpage: 24015
lastpage: 24039
page: 24015-24039
order: 24015
cycles: false
bibtex_author: Kim, Seunghyun and Jung, Seohyeon and Kim, Seonghyeon and Lee, Juho
author:
- given: Seunghyun
  family: Kim
- given: Seohyeon
  family: Jung
- given: Seonghyeon
  family: Kim
- given: Juho
  family: Lee
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/assets/kim24k/kim24k.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
