---
title: Tuning-Free Stochastic Optimization
openreview: A6fmX9QCEa
abstract: Large-scale machine learning problems make the cost of hyperparameter tuning
  ever more prohibitive. This creates a need for algorithms that can tune themselves
  on-the-fly. We formalize the notion of <em>“tuning-free”</em> algorithms that can
  match the performance of optimally-tuned optimization algorithms up to polylogarithmic
  factors given only loose hints on the relevant problem parameters. We consider in
  particular algorithms that can match optimally-tuned Stochastic Gradient Descent
  (SGD). When the domain of optimization is bounded, we show tuning-free matching
  of SGD is possible and achieved by several existing algorithms. We prove that for
  the task of minimizing a convex and smooth or Lipschitz function over an unbounded
  domain, tuning-free optimization is impossible. We discuss conditions under which
  tuning-free optimization is possible even over unbounded domains. In particular,
  we show that the recently proposed DoG and DoWG algorithms are tuning-free when
  the noise distribution is sufficiently well-behaved. For the task of finding a stationary
  point of a smooth and potentially nonconvex function, we give a variant of SGD that
  matches the best-known high-probability convergence rate for tuned SGD at only an
  additional polylogarithmic cost. However, we also give an impossibility result that
  shows no algorithm can hope to match the optimal expected convergence rate for tuned
  SGD with high probability.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: khaled24a
month: 0
tex_title: Tuning-Free Stochastic Optimization
firstpage: 23622
lastpage: 23661
page: 23622-23661
order: 23622
cycles: false
bibtex_author: Khaled, Ahmed and Jin, Chi
author:
- given: Ahmed
  family: Khaled
- given: Chi
  family: Jin
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/khaled24a/khaled24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
