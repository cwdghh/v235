---
title: Learning Latent Space Hierarchical EBM Diffusion Models
openreview: o9uOuIwhZK
abstract: This work studies the learning problem of the energy-based prior model and
  the multi-layer generator model. The multi-layer generator model, which contains
  multiple layers of latent variables organized in a top-down hierarchical structure,
  typically assumes the Gaussian prior model. Such a prior model can be limited in
  modelling expressivity, which results in a gap between the generator posterior and
  the prior model, known as the prior hole problem. Recent works have explored learning
  the energy-based (EBM) prior model as a second-stage, complementary model to bridge
  the gap. However, the EBM defined on a multi-layer latent space can be highly multi-modal,
  which makes sampling from such marginal EBM prior challenging in practice, resulting
  in ineffectively learned EBM. To tackle the challenge, we propose to leverage the
  diffusion probabilistic scheme to mitigate the burden of EBM sampling and thus facilitate
  EBM learning. Our extensive experiments demonstrate a superior performance of our
  diffusion-learned EBM prior on various challenging tasks.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: cui24b
month: 0
tex_title: Learning Latent Space Hierarchical {EBM} Diffusion Models
firstpage: 9633
lastpage: 9645
page: 9633-9645
order: 9633
cycles: false
bibtex_author: Cui, Jiali and Han, Tian
author:
- given: Jiali
  family: Cui
- given: Tian
  family: Han
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://raw.githubusercontent.com/mlresearch/v235/main/assets/cui24b/cui24b.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
