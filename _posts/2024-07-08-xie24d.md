---
title: Improving SAM Requires Rethinking its Optimization Formulation
openreview: k7G4N1x7f9
abstract: This paper rethinks Sharpness-Aware Minimization (SAM), which is originally
  formulated as a zero-sum game where the weights of a network and a bounded perturbation
  try to minimize/maximize, respectively, the same differentiable loss. To fundamentally
  improve this design, we argue that SAM should instead be reformulated using the
  0-1 loss. As a continuous relaxation, we follow the simple conventional approach
  where the minimizing (maximizing) player uses an upper bound (lower bound) surrogate
  to the 0-1 loss. This leads to a novel formulation of SAM as a bilevel optimization
  problem, dubbed as BiSAM. BiSAM with newly designed lower-bound surrogate loss indeed
  constructs stronger perturbation. Through numerical evidence, we show that BiSAM
  consistently results in improved performance when compared to the original SAM and
  variants, while enjoying similar computational complexity. Our code is available
  at https://github.com/LIONS-EPFL/BiSAM.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: xie24d
month: 0
tex_title: Improving {SAM} Requires Rethinking its Optimization Formulation
firstpage: 54475
lastpage: 54487
page: 54475-54487
order: 54475
cycles: false
bibtex_author: Xie, Wanyun and Latorre, Fabian and Antonakopoulos, Kimon and Pethick,
  Thomas and Cevher, Volkan
author:
- given: Wanyun
  family: Xie
- given: Fabian
  family: Latorre
- given: Kimon
  family: Antonakopoulos
- given: Thomas
  family: Pethick
- given: Volkan
  family: Cevher
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://raw.githubusercontent.com/mlresearch/v235/main/assets/xie24d/xie24d.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
