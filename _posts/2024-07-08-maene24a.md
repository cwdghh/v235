---
title: On the Hardness of Probabilistic Neurosymbolic Learning
openreview: vxPmrxKe0J
abstract: The limitations of purely neural learning have sparked an interest in probabilistic
  neurosymbolic models, which combine neural networks with probabilistic logical reasoning.
  As these neurosymbolic models are trained with gradient descent, we study the complexity
  of differentiating probabilistic reasoning. We prove that although approximating
  these gradients is intractable in general, it becomes tractable during training.
  Furthermore, we introduce <em>WeightME</em>, an unbiased gradient estimator based
  on model sampling. Under mild assumptions, WeightME approximates the gradient with
  probabilistic guarantees using a logarithmic number of calls to a SAT solver. Lastly,
  we evaluate the necessity of these guarantees on the gradient. Our experiments indicate
  that the existing biased approximations indeed struggle to optimize even when exact
  solving is still feasible.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: maene24a
month: 0
tex_title: On the Hardness of Probabilistic Neurosymbolic Learning
firstpage: 34203
lastpage: 34218
page: 34203-34218
order: 34203
cycles: false
bibtex_author: Maene, Jaron and Derkinderen, Vincent and De Raedt, Luc
author:
- given: Jaron
  family: Maene
- given: Vincent
  family: Derkinderen
- given: Luc
  family: De Raedt
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://raw.githubusercontent.com/mlresearch/v235/main/assets/maene24a/maene24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
