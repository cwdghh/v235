---
title: 'Medusa: Simple LLM Inference Acceleration Framework with Multiple Decoding
  Heads'
openreview: PEpbUobfJv
abstract: 'Large Language Models (LLMs) employ auto-regressive decoding that requires
  sequential computation, with each step reliant on the previous one’s output. This
  creates a bottleneck as each step necessitates moving the full model parameters
  from High-Bandwidth Memory (HBM) to the accelerator’s cache. While methods such
  as speculative decoding have been suggested to address this issue, their implementation
  is impeded by the challenges associated with acquiring and maintaining a separate
  draft model. In this paper, we present Medusa, an efficient method that augments
  LLM inference by adding extra decoding heads to predict multiple subsequent tokens
  in parallel. Using a tree-based attention mechanism, Medusa constructs multiple
  candidate continuations and verifies them simultaneously in each decoding step.
  By leveraging parallel processing, Medusa reduces the number of decoding steps required.
  We present two levels of fine-tuning procedures for Medusa to meet the needs of
  different use cases: Medusa-1: Medusa is directly fine-tuned on top of a frozen
  backbone LLM, enabling lossless inference acceleration. Medusa-2: Medusa is fine-tuned
  together with the backbone LLM, enabling better prediction accuracy of Medusa heads
  and higher speedup but needing a special training recipe that preserves the model’s
  capabilities. Moreover, we propose several extensions that improve or expand the
  utility of Medusa, including a self-distillation to handle situations where no training
  data is available and a typical acceptance scheme to boost the acceptance rate while
  maintaining generation quality. We evaluate Medusa on models of various sizes and
  training procedures. Our experiments demonstrate that Medusa-1 can achieve over
  2.2$\times$ speedup without compromising generation quality, while Medusa-2 further
  improves the speedup to 2.3-2.8$\times$.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: cai24b
month: 0
tex_title: 'Medusa: Simple {LLM} Inference Acceleration Framework with Multiple Decoding
  Heads'
firstpage: 5209
lastpage: 5235
page: 5209-5235
order: 5209
cycles: false
bibtex_author: Cai, Tianle and Li, Yuhong and Geng, Zhengyang and Peng, Hongwu and
  Lee, Jason D. and Chen, Deming and Dao, Tri
author:
- given: Tianle
  family: Cai
- given: Yuhong
  family: Li
- given: Zhengyang
  family: Geng
- given: Hongwu
  family: Peng
- given: Jason D.
  family: Lee
- given: Deming
  family: Chen
- given: Tri
  family: Dao
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/assets/cai24b/cai24b.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
