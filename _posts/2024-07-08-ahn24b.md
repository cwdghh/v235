---
title: 'Understanding Adam Optimizer via Online Learning of Updates: Adam is FTRL
  in Disguise'
openreview: iE2lMjeXRR
abstract: Despite the success of the Adam optimizer in practice, the theoretical understanding
  of its algorithmic components still remains limited. In particular, most existing
  analyses of Adam show the convergence rate that can be simply achieved by non-adative
  algorithms like SGD. In this work, we provide a different perspective based on online
  learning that underscores the importance of Adamâ€™s algorithmic components. Inspired
  by Cutkosky et al. (2023), we consider the framework called online learning of updates/increments,
  where we choose the updates/increments of an optimizer based on an online learner.
  With this framework, the design of a good optimizer is reduced to the design of
  a good online learner. Our main observation is that Adam corresponds to a principled
  online learning framework called Follow-the-Regularized-Leader (FTRL). Building
  on this observation, we study the benefits of its algorithmic components from the
  online learning perspective.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: ahn24b
month: 0
tex_title: 'Understanding {A}dam Optimizer via Online Learning of Updates: {A}dam
  is {FTRL} in Disguise'
firstpage: 619
lastpage: 640
page: 619-640
order: 619
cycles: false
bibtex_author: Ahn, Kwangjun and Zhang, Zhiyu and Kook, Yunbum and Dai, Yan
author:
- given: Kwangjun
  family: Ahn
- given: Zhiyu
  family: Zhang
- given: Yunbum
  family: Kook
- given: Yan
  family: Dai
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/assets/ahn24b/ahn24b.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
