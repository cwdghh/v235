---
title: 'SpikeZIP-TF: Conversion is All You Need for Transformer-based SNN'
openreview: NeotatlYOL
abstract: 'Spiking neural network (SNN) has attracted great attention due to its characteristic
  of high efficiency and accuracy. Currently, the ANN-to-SNN conversion methods can
  obtain ANN on-par accuracy SNN with ultra-low latency (8 time-steps) in CNN structure
  on computer vision (CV) tasks. However, as Transformer-based networks have achieved
  prevailing precision on both CV and natural language processing (NLP), the Transformer-based
  SNNs are still encounting the lower accuracy w.r.t the ANN counterparts. In this
  work, we introduce a novel ANN-to-SNN conversion method called SpikeZIP-TF, where
  ANN and SNN are exactly equivalent, thus incurring no accuracy degradation. SpikeZIP-TF
  achieves 83.82% accuracy on CV dataset (ImageNet) and 93.79% accuracy on NLP dataset
  (SST-2), which are higher than SOTA Transformer-based SNNs. The code is available
  in GitHub: https://github.com/Intelligent-Computing-Research-Group/SpikeZIP_transformer'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: you24b
month: 0
tex_title: "{S}pike{ZIP}-{TF}: Conversion is All You Need for Transformer-based {SNN}"
firstpage: 57367
lastpage: 57383
page: 57367-57383
order: 57367
cycles: false
bibtex_author: You, Kang and Xu, Zekai and Nie, Chen and Deng, Zhijie and Guo, Qinghai
  and Wang, Xiang and He, Zhezhi
author:
- given: Kang
  family: You
- given: Zekai
  family: Xu
- given: Chen
  family: Nie
- given: Zhijie
  family: Deng
- given: Qinghai
  family: Guo
- given: Xiang
  family: Wang
- given: Zhezhi
  family: He
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/you24b/you24b.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
