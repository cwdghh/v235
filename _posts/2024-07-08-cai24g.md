---
title: 'LoCoCo: Dropping In Convolutions for Long Context Compression'
openreview: NUlyqMyhO9
abstract: This paper tackles the memory hurdle of of processing long context sequences
  in Large Language Models (LLMs), by presenting a novel approach, Dropping In Convolutions
  for <b>Lo</b>ng <b>Co</b>ntext <b>Co</b>mpression (<b>LoCoCo</b>). LoCoCo employs
  only a fixed-size Key-Value (KV) cache, and can enhance efficiency in both inference
  and fine-tuning stages. Diverging from prior methods that selectively drop KV pairs
  based on heuristics, LoCoCo leverages a data-driven adaptive fusion technique, blending
  previous KV pairs with incoming tokens to minimize the loss of contextual information
  and ensure accurate attention modeling. This token integration is achieved through
  injecting one-dimensional convolutional kernels that dynamically calculate mixing
  weights for each KV cache slot. Designed for broad compatibility with existing LLM
  frameworks, LoCoCo allows for straightforward "drop-in" integration without needing
  architectural modifications, while incurring minimal tuning overhead. Experiments
  demonstrate that LoCoCo maintains consistently outstanding performance across various
  context lengths and can achieve a high context compression rate during both inference
  and fine-tuning phases. During inference, we successfully compressed up to $3482$
  tokens into a $128$-size KV cache, while retaining comparable performance to the
  full sequence - an accuracy improvement of up to $0.2791$ compared to baselines
  at the same cache size. During post-training tuning, we also effectively extended
  the context length from 4K to 32K using a KV cache of fixed size 512, achieving
  performance similar to fine-tuning with entire sequences.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: cai24g
month: 0
tex_title: "{L}o{C}o{C}o: Dropping In Convolutions for Long Context Compression"
firstpage: 5348
lastpage: 5359
page: 5348-5359
order: 5348
cycles: false
bibtex_author: Cai, Ruisi and Tian, Yuandong and Wang, Zhangyang and Chen, Beidi
author:
- given: Ruisi
  family: Cai
- given: Yuandong
  family: Tian
- given: Zhangyang
  family: Wang
- given: Beidi
  family: Chen
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/cai24g/cai24g.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
