---
title: Switching the Loss Reduces the Cost in Batch Reinforcement Learning
openreview: 7PXSc5fURu
abstract: We propose training fitted Q-iteration with log-loss (FQI-LOG) for batch
  reinforcement learning (RL). We show that the number of samples needed to learn
  a near-optimal policy with FQI-LOG scales with the accumulated cost of the optimal
  policy, which is zero in problems where acting optimally achieves the goal and incurs
  no cost. In doing so, we provide a general framework for proving small-cost bounds,
  i.e. bounds that scale with the optimal achievable cost, in batch RL. Moreover,
  we empirically verify that FQI-LOG uses fewer samples than FQI trained with squared
  loss on problems where the optimal policy reliably achieves the goal.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: ayoub24a
month: 0
tex_title: Switching the Loss Reduces the Cost in Batch Reinforcement Learning
firstpage: 2135
lastpage: 2158
page: 2135-2158
order: 2135
cycles: false
bibtex_author: Ayoub, Alex and Wang, Kaiwen and Liu, Vincent and Robertson, Samuel
  and Mcinerney, James and Liang, Dawen and Kallus, Nathan and Szepesvari, Csaba
author:
- given: Alex
  family: Ayoub
- given: Kaiwen
  family: Wang
- given: Vincent
  family: Liu
- given: Samuel
  family: Robertson
- given: James
  family: Mcinerney
- given: Dawen
  family: Liang
- given: Nathan
  family: Kallus
- given: Csaba
  family: Szepesvari
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/assets/ayoub24a/ayoub24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
