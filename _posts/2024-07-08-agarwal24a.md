---
title: 'CHAI: Clustered Head Attention for Efficient LLM Inference'
openreview: xcDRx8vzCa
abstract: Large Language Models (LLMs) with hundreds of billions of parameters have
  transformed the field of machine learning. However, serving these models at inference
  time is both compute and memory intensive, where a single request can require multiple
  GPUs and tens of Gigabytes of memory. Multi-head attention is one of the key components
  of LLMs, which can for over 50% of LLMs memory and compute requirement. We observe
  that there is a high amount of redundancy across heads on which tokens they pay
  attention to. Based on this insight, we propose Clustered HeadAttention ( CHAI ).
  CHAI combines heads with a high amount of correlation for self-attention at runtime,
  thus reducing both memory and compute. In our experiments, we show that CHAI is
  able to reduce the memory requirements for storing K,V cache by up to 21.4% and
  inference time latency by up to 1.73Ã— without any fine-tuning required. CHAI achieves
  this with a maximum 3.2% deviation in accuracy across 3 different models (i.e. OPT-66B,
  LLAMA-7B, LLAMA-33B) and 5 different evaluation datasets.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: agarwal24a
month: 0
tex_title: "{CHAI}: Clustered Head Attention for Efficient {LLM} Inference"
firstpage: 291
lastpage: 312
page: 291-312
order: 291
cycles: false
bibtex_author: Agarwal, Saurabh and Acun, Bilge and Hosmer, Basil and Elhoushi, Mostafa
  and Lee, Yejin and Venkataraman, Shivaram and Papailiopoulos, Dimitris and Wu, Carole-Jean
author:
- given: Saurabh
  family: Agarwal
- given: Bilge
  family: Acun
- given: Basil
  family: Hosmer
- given: Mostafa
  family: Elhoushi
- given: Yejin
  family: Lee
- given: Shivaram
  family: Venkataraman
- given: Dimitris
  family: Papailiopoulos
- given: Carole-Jean
  family: Wu
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/agarwal24a/agarwal24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
