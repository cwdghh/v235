---
title: 'LCA-on-the-Line: Benchmarking Out of Distribution Generalization with Class
  Taxonomies'
openreview: HPXRzM9BYZ
abstract: We tackle the challenge of predicting models’ Out-of-Distribution (OOD)
  performance using in-distribution (ID) measurements without requiring OOD data.
  Existing evaluations with “Effective robustness”, which use ID accuracy as an indicator
  of OOD accuracy, encounter limitations when models are trained with diverse supervision
  and distributions, such as class labels (<em>Vision Models, VMs, on ImageNet</em>)
  and textual descriptions (<em>Visual-Language Models, VLMs, on LAION</em>). VLMs
  often generalize better to OOD data than VMs despite having similar or lower ID
  performance. To improve the prediction of models’ OOD performance from ID measurements,
  we introduce the <em>Lowest Common Ancestor (LCA)-on-the-Line</em> framework. This
  approach revisits the established concept of LCA distance, which measures the hierarchical
  distance between labels and predictions within a predefined class hierarchy, such
  as WordNet. We assess 75 models using ImageNet as the ID dataset and five significantly
  shifted OOD variants, uncovering a strong linear correlation between ID LCA distance
  and OOD top-1 accuracy. Our method provides a compelling alternative for understanding
  why VLMs tend to generalize better. Additionally, we propose a technique to construct
  a taxonomic hierarchy on any dataset using $K$-means clustering, demonstrating that
  LCA distance is robust to the constructed taxonomic hierarchy. Moreover, we demonstrate
  that aligning model predictions with class taxonomies, through soft labels or prompt
  engineering, can enhance model generalization. Open source code in our Project Page.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: shi24c
month: 0
tex_title: "{LCA}-on-the-Line: Benchmarking Out of Distribution Generalization with
  Class Taxonomies"
firstpage: 44887
lastpage: 44908
page: 44887-44908
order: 44887
cycles: false
bibtex_author: Shi, Jia and Gare, Gautam Rajendrakumar and Tian, Jinjin and Chai,
  Siqi and Lin, Zhiqiu and Vasudevan, Arun Balajee and Feng, Di and Ferroni, Francesco
  and Kong, Shu
author:
- given: Jia
  family: Shi
- given: Gautam Rajendrakumar
  family: Gare
- given: Jinjin
  family: Tian
- given: Siqi
  family: Chai
- given: Zhiqiu
  family: Lin
- given: Arun Balajee
  family: Vasudevan
- given: Di
  family: Feng
- given: Francesco
  family: Ferroni
- given: Shu
  family: Kong
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/shi24c/shi24c.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
