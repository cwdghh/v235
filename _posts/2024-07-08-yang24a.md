---
title: Do Efficient Transformers Really Save Computation?
openreview: xLikRS9OhW
abstract: As transformer-based language models are trained on increasingly large datasets
  and with vast numbers of parameters, finding more efficient alternatives to the
  standard Transformer has become very valuable. While many efficient Transformers
  and Transformer alternatives have been proposed, none provide theoretical guarantees
  that they are a suitable replacement for the standard Transformer. This makes it
  challenging to identify when to use a specific model and what directions to prioritize
  for further investigation. In this paper, we aim to understand the capabilities
  and limitations of efficient Transformers, specifically the Sparse Transformer and
  the Linear Transformer. We focus on their reasoning capability as exhibited by Chain-of-Thought
  (CoT) prompts and follow previous works to model them as Dynamic Programming (DP)
  problems. Our results show that while these models are expressive enough to solve
  general DP tasks, contrary to expectations, they require a model size that scales
  with the problem size. Nonetheless, we identify a class of DP problems for which
  these models can be more efficient than the standard Transformer. We confirm our
  theoretical results through experiments on representative DP tasks, adding to the
  understanding of efficient Transformersâ€™ practical strengths and weaknesses.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: yang24a
month: 0
tex_title: Do Efficient Transformers Really Save Computation?
firstpage: 55928
lastpage: 55947
page: 55928-55947
order: 55928
cycles: false
bibtex_author: Yang, Kai and Ackermann, Jan and He, Zhenyu and Feng, Guhao and Zhang,
  Bohang and Feng, Yunzhen and Ye, Qiwei and He, Di and Wang, Liwei
author:
- given: Kai
  family: Yang
- given: Jan
  family: Ackermann
- given: Zhenyu
  family: He
- given: Guhao
  family: Feng
- given: Bohang
  family: Zhang
- given: Yunzhen
  family: Feng
- given: Qiwei
  family: Ye
- given: Di
  family: He
- given: Liwei
  family: Wang
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/assets/yang24a/yang24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
