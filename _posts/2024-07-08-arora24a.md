---
title: Simple linear attention language models balance the recall-throughput tradeoff
openreview: e93ffDcpH3
abstract: Recent work has shown that attention-based language models excel at "recall",
  the ability to ground generations in tokens previously seen in context. However,
  the efficiency of attention-based models is bottle-necked during inference by the
  KV-cache’s aggressive memory consumption. In this work, we explore whether we can
  improve language model efficiency (e.g. by reducing memory consumption) without
  compromising on recall. By applying experiments and theory to a broad set of architectures,
  we identify a key tradeoff between a model’s recurrent state size and recall ability.
  We show that efficient alternatives to attention (e.g. H3, Mamba, RWKV) maintain
  a fixed-size recurrent state, but struggle at recall. We propose BASED a simple
  architecture combining linear and sliding window attention. By varying BASED window
  size and linear attention feature dimension, we can dial the state size and traverse
  the Pareto frontier of the recall-memory tradeoff curve, recovering the full quality
  of attention on one end and the small state size of attention-alternatives on the
  other. We train language models up to $1.3$b parameters and show that BASED matches
  the strongest sub-quadratic models (e.g. Mamba) in perplexity and outperforms them
  on real-world recall-intensive tasks by 10.36 accuracy points. We further develop
  IO-aware algorithms that enable BASED to provide 24× higher throughput on language
  generation than FlashAttention-2, when generating 1024 tokens using 1.3b parameter
  models. Overall, BASED expands the Pareto frontier of the throughput-recall tradeoff
  space beyond prior architectures.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: arora24a
month: 0
tex_title: Simple linear attention language models balance the recall-throughput tradeoff
firstpage: 1763
lastpage: 1840
page: 1763-1840
order: 1763
cycles: false
bibtex_author: Arora, Simran and Eyuboglu, Sabri and Zhang, Michael and Timalsina,
  Aman and Alberti, Silas and Zou, James and Rudra, Atri and Re, Christopher
author:
- given: Simran
  family: Arora
- given: Sabri
  family: Eyuboglu
- given: Michael
  family: Zhang
- given: Aman
  family: Timalsina
- given: Silas
  family: Alberti
- given: James
  family: Zou
- given: Atri
  family: Rudra
- given: Christopher
  family: Re
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/assets/arora24a/arora24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
