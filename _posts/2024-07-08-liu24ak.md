---
title: Short-Long Convolutions Help Hardware-Efficient Linear Attention to Focus on
  Long Sequences
openreview: TRrXkVdhwi
abstract: 'To mitigate the computational complexity in the self-attention mechanism
  on long sequences, linear attention utilizes computation tricks to achieve linear
  complexity, while state space models (SSMs) popularize a favourable practice of
  using non-data-dependent memory pattern, <em>i.e.,</em> emphasize the near and neglect
  the distant, to processing sequences. Recent studies have shown the priorities by
  combining them as one. However, the efficiency of linear attention remains only
  at the theoretical level in a causal setting, and SSMs require various designed
  constraints to operate effectively on specific data. Therefore, in order to unveil
  the true power of the hybrid design, the following two issues need to be addressed:
  (1) hardware-efficient implementation for linear attention and (2) stabilization
  of SSMs. To achieve this, we leverage the thought of tiling and hierarchy to propose
  CHELA (short-long Convolutions with Hardware-Efficient Linear Attention), which
  replaces SSMs with short-long convolutions and implements linear attention in a
  divide-and-conquer manner. This approach enjoys global abstraction and data-dependent
  selection from stable SSM and linear attention while maintaining real linear complexity.
  Our comprehensive experiments on the Long Range Arena benchmark and language modeling
  tasks demonstrate the effectiveness of the proposed method.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: liu24ak
month: 0
tex_title: Short-Long Convolutions Help Hardware-Efficient Linear Attention to Focus
  on Long Sequences
firstpage: 31451
lastpage: 31463
page: 31451-31463
order: 31451
cycles: false
bibtex_author: Liu, Zicheng and Li, Siyuan and Wang, Li and Wang, Zedong and Liu,
  Yunfan and Li, Stan Z.
author:
- given: Zicheng
  family: Liu
- given: Siyuan
  family: Li
- given: Li
  family: Wang
- given: Zedong
  family: Wang
- given: Yunfan
  family: Liu
- given: Stan Z.
  family: Li
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/liu24ak/liu24ak.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
