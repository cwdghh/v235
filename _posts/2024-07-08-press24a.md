---
title: 'The Entropy Enigma: Success and Failure of Entropy Minimization'
openreview: 0bGsVoumFL
abstract: 'Entropy minimization (EM) is frequently used to increase the accuracy of
  classification models when they’re faced with new data at test time. EM is a self-supervised
  learning method that optimizes classifiers to assign even higher probabilities to
  their top predicted classes. In this paper, we analyze why EM works when adapting
  a model for a few steps and why it eventually fails after adapting for many steps.
  We show that, at first, EM causes the model to embed test images close to training
  images, thereby increasing model accuracy. After many steps of optimization, EM
  makes the model embed test images far away from the embeddings of training images,
  which results in a degradation of accuracy. Building upon our insights, we present
  a method for solving a practical problem: estimating a model’s accuracy on a given
  arbitrary dataset without having access to its labels. Our method estimates accuracy
  by looking at how the embeddings of input images change as the model is optimized
  to minimize entropy. Experiments on 23 challenging datasets show that our method
  sets the SoTA with a mean absolute error of 5.75%, an improvement of 29.62% over
  the previous SoTA on this task. Our code is available at: https://github.com/oripress/EntropyEnigma'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: press24a
month: 0
tex_title: 'The Entropy Enigma: Success and Failure of Entropy Minimization'
firstpage: 41064
lastpage: 41085
page: 41064-41085
order: 41064
cycles: false
bibtex_author: Press, Ori and Shwartz-Ziv, Ravid and Lecun, Yann and Bethge, Matthias
author:
- given: Ori
  family: Press
- given: Ravid
  family: Shwartz-Ziv
- given: Yann
  family: Lecun
- given: Matthias
  family: Bethge
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/assets/press24a/press24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
