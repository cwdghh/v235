---
title: Polynomial-based Self-Attention for Table Representation Learning
openreview: QZd3rvlP76
abstract: Structured data, which constitutes a significant portion of existing data
  types, has been a long-standing research topic in the field of machine learning.
  Various representation learning methods for tabular data have been proposed, ranging
  from encoder-decoder structures to Transformers. Among these, Transformer-based
  methods have achieved state-of-the-art performance not only in tabular data but
  also in various other fields, including computer vision and natural language processing.
  However, recent studies have revealed that self-attention, a key component of Transformers,
  can lead to an oversmoothing issue. We show that Transformers for tabular data also
  face this problem. To tackle the problem, we suggest a novel self-attention layer
  for tabular data, leveraging matrix polynomials. This proposed layer serves as a
  replacement for the original self-attention layer, contributing to the improvement
  of model scalability. In our experiments with three representative table learning
  models equipped with our proposed layer, we illustrate that the layer effectively
  mitigates the oversmoothing problem and enhances the representation performance
  of the existing methods, outperforming the state-of-the-art table representation
  methods.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: kim24ae
month: 0
tex_title: Polynomial-based Self-Attention for Table Representation Learning
firstpage: 24509
lastpage: 24526
page: 24509-24526
order: 24509
cycles: false
bibtex_author: Kim, Jayoung and Shin, Yehjin and Choi, Jeongwhan and Wi, Hyowon and
  Park, Noseong
author:
- given: Jayoung
  family: Kim
- given: Yehjin
  family: Shin
- given: Jeongwhan
  family: Choi
- given: Hyowon
  family: Wi
- given: Noseong
  family: Park
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/kim24ae/kim24ae.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
