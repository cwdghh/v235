---
title: GFlowNet Training by Policy Gradients
openreview: G1igwiBBUj
abstract: 'Generative Flow Networks (GFlowNets) have been shown effective to generate
  combinatorial objects with desired properties. We here propose a new GFlowNet training
  framework, with policy-dependent rewards, that bridges keeping flow balance of GFlowNets
  to optimizing the expected accumulated reward in traditional Reinforcement-Learning
  (RL). This enables the derivation of new policy-based GFlowNet training methods,
  in contrast to existing ones resembling value-based RL. It is known that the design
  of backward policies in GFlowNet training affects efficiency. We further develop
  a coupled training strategy that jointly solves GFlowNet forward policy training
  and backward policy design. Performance analysis is provided with a theoretical
  guarantee of our policy-based GFlowNet training. Experiments on both simulated and
  real-world datasets verify that our policy-based strategies provide advanced RL
  perspectives for robust gradient estimation to improve GFlowNet performance. Our
  code is available at: github.com/niupuhua1234/GFN-PG.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: niu24c
month: 0
tex_title: "{GF}low{N}et Training by Policy Gradients"
firstpage: 38344
lastpage: 38380
page: 38344-38380
order: 38344
cycles: false
bibtex_author: Niu, Puhua and Wu, Shili and Fan, Mingzhou and Qian, Xiaoning
author:
- given: Puhua
  family: Niu
- given: Shili
  family: Wu
- given: Mingzhou
  family: Fan
- given: Xiaoning
  family: Qian
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/assets/niu24c/niu24c.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
