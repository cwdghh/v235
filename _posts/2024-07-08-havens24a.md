---
title: Fine-grained Local Sensitivity Analysis of Standard Dot-Product Self-Attention
openreview: EEinDTdKr1
abstract: Self-attention has been widely used in various machine learning models,
  such as vision transformers. The standard dot-product self-attention is arguably
  the most popular structure, and there is a growing interest in understanding the
  mathematical properties of such attention mechanisms. This paper presents a fine-grained
  local sensitivity analysis of the standard dot-product self-attention, leading to
  new non-vacuous certified robustness results for vision transformers. Despite the
  well-known fact that dot-product self-attention is not (globally) Lipschitz, we
  develop new theoretical analysis of Local Fine-grained Attention Sensitivity (LoFAST)
  quantifying the effect of input feature perturbations on the attention output. Our
  analysis reveals that the local sensitivity of dot-product self-attention to $\ell_2$
  perturbations can actually be controlled by several key quantities associated with
  the attention weight matrices and the unperturbed input. We empirically validate
  our theoretical findings by computing non-vacuous certified $\ell_2$-robustness
  for vision transformers on CIFAR-10 and SVHN datasets. The code for LoFAST is available
  at https://github.com/AaronHavens/LoFAST.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: havens24a
month: 0
tex_title: Fine-grained Local Sensitivity Analysis of Standard Dot-Product Self-Attention
firstpage: 17680
lastpage: 17696
page: 17680-17696
order: 17680
cycles: false
bibtex_author: Havens, Aaron J and Araujo, Alexandre and Zhang, Huan and Hu, Bin
author:
- given: Aaron J
  family: Havens
- given: Alexandre
  family: Araujo
- given: Huan
  family: Zhang
- given: Bin
  family: Hu
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://raw.githubusercontent.com/mlresearch/v235/main/assets/havens24a/havens24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
