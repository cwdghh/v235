---
title: Model-based Reinforcement Learning for Confounded POMDPs
openreview: DlR8fWgJRl
abstract: We propose a model-based offline reinforcement learning (RL) algorithm for
  confounded partially observable Markov decision processes (POMDPs) under general
  function approximations and show it is provably efficient under some technical conditions
  such as the partial coverage imposed on the offline data distribution. Specifically,
  we first establish a novel model-based identification result for learning the effect
  of any action on the reward and future transitions in the confounded POMDP. Using
  this identification result, we then design a nonparametric two-stage estimation
  procedure to construct an estimator for off-policy evaluation (OPE), which permits
  general function approximations. Finally, we learn the optimal policy by performing
  a conservative policy optimization within the confidence regions based on the proposed
  estimation procedure for OPE. Under some mild conditions, we establish a finite-sample
  upper bound on the suboptimality of the learned policy in finding the optimal one,
  which depends on the sample size and the length of horizons polynomially.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: hong24d
month: 0
tex_title: Model-based Reinforcement Learning for Confounded {POMDP}s
firstpage: 18668
lastpage: 18710
page: 18668-18710
order: 18668
cycles: false
bibtex_author: Hong, Mao and Qi, Zhengling and Xu, Yanxun
author:
- given: Mao
  family: Hong
- given: Zhengling
  family: Qi
- given: Yanxun
  family: Xu
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/assets/hong24d/hong24d.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
