---
title: Investigating Pre-Training Objectives for Generalization in Vision-Based Reinforcement
  Learning
openreview: OiI12sNbgD
abstract: Recently, various pre-training methods have been introduced in vision-based
  Reinforcement Learning (RL). However, their generalization ability remains unclear
  due to evaluations being limited to in-distribution environments and non-unified
  experimental setups. To address this, we introduce the Atari Pre-training Benchmark
  (Atari-PB), which pre-trains a ResNet-50 model on 10 million transitions from 50
  Atari games and evaluates it across diverse environment distributions. Our experiments
  show that pre-training objectives focused on learning task-agnostic features (e.g.,
  identifying objects and understanding temporal dynamics) enhance generalization
  across different environments. In contrast, objectives focused on learning task-specific
  knowledge (e.g., identifying agents and fitting reward functions) improve performance
  in environments similar to the pre-training dataset but not in varied ones. We publicize
  our codes, datasets, and model checkpoints at https://github.com/dojeon-ai/Atari-PB.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: kim24u
month: 0
tex_title: Investigating Pre-Training Objectives for Generalization in Vision-Based
  Reinforcement Learning
firstpage: 24294
lastpage: 24326
page: 24294-24326
order: 24294
cycles: false
bibtex_author: Kim, Donghu and Lee, Hojoon and Lee, Kyungmin and Hwang, Dongyoon and
  Choo, Jaegul
author:
- given: Donghu
  family: Kim
- given: Hojoon
  family: Lee
- given: Kyungmin
  family: Lee
- given: Dongyoon
  family: Hwang
- given: Jaegul
  family: Choo
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/assets/kim24u/kim24u.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
