---
title: 'RLAIF vs. RLHF: Scaling Reinforcement Learning from Human Feedback with AI
  Feedback'
openreview: uydQ2W41KO
abstract: Reinforcement learning from human feedback (RLHF) has proven effective in
  aligning large language models (LLMs) with human preferences, but gathering high-quality
  preference labels is expensive. RL from AI Feedback (RLAIF), introduced in Bai et
  al. (2022b), offers a promising alternative that trains the reward model (RM) on
  preferences generated by an off-the-shelf LLM. Across the tasks of summarization,
  helpful dialogue generation, and harmless dialogue generation, we show that RLAIF
  achieves comparable performance to RLHF. Furthermore, we take a step towards "self-improvement"
  by demonstrating that RLAIF can outperform a supervised fine-tuned baseline even
  when the AI labeler is the same size as the policy, or even the exact same checkpoint
  as the initial policy. Finally, we introduce direct-RLAIF (d-RLAIF) - a technique
  that circumvents RM training by obtaining rewards directly from an off-the-shelf
  LLM during RL, which achieves superior performance to canonical RLAIF. Our results
  suggest that RLAIF can achieve performance on-par with using human feedback, offering
  a potential solution to the scalability limitations of RLHF.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: lee24t
month: 0
tex_title: "{RLAIF} vs. {RLHF}: Scaling Reinforcement Learning from Human Feedback
  with {AI} Feedback"
firstpage: 26874
lastpage: 26901
page: 26874-26901
order: 26874
cycles: false
bibtex_author: Lee, Harrison and Phatale, Samrat and Mansoor, Hassan and Mesnard,
  Thomas and Ferret, Johan and Lu, Kellie Ren and Bishop, Colton and Hall, Ethan and
  Carbune, Victor and Rastogi, Abhinav and Prakash, Sushant
author:
- given: Harrison
  family: Lee
- given: Samrat
  family: Phatale
- given: Hassan
  family: Mansoor
- given: Thomas
  family: Mesnard
- given: Johan
  family: Ferret
- given: Kellie Ren
  family: Lu
- given: Colton
  family: Bishop
- given: Ethan
  family: Hall
- given: Victor
  family: Carbune
- given: Abhinav
  family: Rastogi
- given: Sushant
  family: Prakash
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/assets/lee24t/lee24t.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
