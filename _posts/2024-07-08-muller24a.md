---
title: Is Kernel Prediction More Powerful than Gating in Convolutional Neural Networks?
openreview: UE79AkNg60
abstract: Neural networks whose weights are the output of a predictor (HyperNetworks)
  achieve excellent performance on many tasks. In ConvNets, kernel prediction layers
  are a popular type of HyperNetwork. Previous theoretical work has argued that a
  hierarchy of multiplicative interactions exists in which gating is at the bottom
  and full weight prediction, as in HyperNetworks, is at the top. In this paper, we
  constructively demonstrate an equivalence between gating combined with fixed weight
  layers and weight prediction, relativizing the notion of a hierarchy of multiplicative
  interactions. We further derive an equivalence between a restricted type of HyperNetwork
  and factorization machines. Finally, we find empirically that gating layers can
  learn to imitate weight prediction layers with an SGD variant and show a novel practical
  application in image denoising using kernel prediction networks. Our reformulation
  of predicted kernels, combining fixed layers and gating, reduces memory requirements.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: muller24a
month: 0
tex_title: Is Kernel Prediction More Powerful than Gating in Convolutional Neural
  Networks?
firstpage: 36591
lastpage: 36604
page: 36591-36604
order: 36591
cycles: false
bibtex_author: Muller, Lorenz K
author:
- given: Lorenz K
  family: Muller
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/assets/muller24a/muller24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
