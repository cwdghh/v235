---
title: Transformers, parallel computation, and logarithmic depth
openreview: QCZabhKQhB
abstract: We show that a constant number of self-attention layers can efficiently
  simulate—and be simulated by—a constant number of communication rounds of <em>Massively
  Parallel Computation</em>. As a consequence, we show that logarithmic-depth is sufficient
  for transformers to solve basic computational tasks that cannot be efficiently solved
  by several other neural sequence models and sub-quadratic transformer approximations.
  We thus establish parallelism as a key distinguishing property of transformers.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: sanford24a
month: 0
tex_title: Transformers, parallel computation, and logarithmic depth
firstpage: 43276
lastpage: 43327
page: 43276-43327
order: 43276
cycles: false
bibtex_author: Sanford, Clayton and Hsu, Daniel and Telgarsky, Matus
author:
- given: Clayton
  family: Sanford
- given: Daniel
  family: Hsu
- given: Matus
  family: Telgarsky
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/sanford24a/sanford24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
