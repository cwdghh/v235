---
title: Prototypical Transformer As Unified Motion Learners
openreview: JOrLz5d7OW
abstract: In this work, we introduce the Prototypical Transformer (ProtoFormer), a
  general and unified framework that approaches various motion tasks from a prototype
  perspective. ProtoFormer seamlessly integrates prototype learning with Transformer
  by thoughtfully considering motion dynamics, introducing two innovative designs.
  First, Cross-Attention Prototyping discovers prototypes based on signature motion
  patterns, providing transparency in understanding motion scenes. Second, Latent
  Synchronization guides feature representation learning via prototypes, effectively
  mitigating the problem of motion uncertainty. Empirical results demonstrate that
  our approach achieves competitive performance on popular motion tasks such as optical
  flow and scene depth. Furthermore, it exhibits generality across various downstream
  tasks, including object tracking and video stabilization.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: han24d
month: 0
tex_title: Prototypical Transformer As Unified Motion Learners
firstpage: 17416
lastpage: 17436
page: 17416-17436
order: 17416
cycles: false
bibtex_author: Han, Cheng and Lu, Yawen and Sun, Guohao and Liang, James Chenhao and
  Cao, Zhiwen and Wang, Qifan and Guan, Qiang and Dianat, Sohail and Rao, Raghuveer
  and Geng, Tong and Tao, Zhiqiang and Liu, Dongfang
author:
- given: Cheng
  family: Han
- given: Yawen
  family: Lu
- given: Guohao
  family: Sun
- given: James Chenhao
  family: Liang
- given: Zhiwen
  family: Cao
- given: Qifan
  family: Wang
- given: Qiang
  family: Guan
- given: Sohail
  family: Dianat
- given: Raghuveer
  family: Rao
- given: Tong
  family: Geng
- given: Zhiqiang
  family: Tao
- given: Dongfang
  family: Liu
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/han24d/han24d.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
