---
title: 'A Tale of Tails: Model Collapse as a Change of Scaling Laws'
openreview: KVvku47shW
abstract: 'As AI model size grows, neural <em>scaling laws</em> have become a crucial
  tool to predict the improvements of large models when increasing capacity and the
  size of original (human or natural) training data. Yet, the widespread use of popular
  models means that the ecosystem of online data and text will co-evolve to progressively
  contain increased amounts of synthesized data. In this paper we ask: <em>How will
  the scaling laws change in the inevitable regime where synthetic data makes its
  way into the training corpus?</em> Will future models, still improve, or be doomed
  to degenerate up to total <em>(model) collapse</em>? We develop a theoretical framework
  of model collapse through the lens of scaling laws. We discover a wide range of
  decay phenomena, analyzing loss of scaling, shifted scaling with number of generations,
  the ‚Äùun-learning" of skills, and grokking when mixing human and synthesized data.
  Our theory is validated by large-scale experiments with a transformer on an arithmetic
  task and text generation using the large language model Llama2.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: dohmatob24b
month: 0
tex_title: 'A Tale of Tails: Model Collapse as a Change of Scaling Laws'
firstpage: 11165
lastpage: 11197
page: 11165-11197
order: 11165
cycles: false
bibtex_author: Dohmatob, Elvis and Feng, Yunzhen and Yang, Pu and Charton, Francois
  and Kempe, Julia
author:
- given: Elvis
  family: Dohmatob
- given: Yunzhen
  family: Feng
- given: Pu
  family: Yang
- given: Francois
  family: Charton
- given: Julia
  family: Kempe
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://raw.githubusercontent.com/mlresearch/v235/main/assets/dohmatob24b/dohmatob24b.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
