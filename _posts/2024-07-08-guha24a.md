---
title: On the Diminishing Returns of Width for Continual Learning
openreview: Ld255Mbx9F
abstract: While deep neural networks have demonstrated groundbreaking performance
  in various settings, these models often suffer from <em>catastrophic forgetting</em>
  when trained on new tasks in sequence. Several works have empirically demonstrated
  that increasing the width of a neural network leads to a decrease in catastrophic
  forgetting but have yet to characterize the exact relationship between width and
  continual learning. We design one of the first frameworks to analyze Continual Learning
  Theory and prove that width is directly related to forgetting in Feed-Forward Networks
  (FFN), demonstrating that the diminishing returns of increasing widths to reduce
  forgetting. We empirically verify our claims at widths hitherto unexplored in prior
  studies where the diminishing returns are clearly observed as predicted by our theory.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: guha24a
month: 0
tex_title: On the Diminishing Returns of Width for Continual Learning
firstpage: 16706
lastpage: 16730
page: 16706-16730
order: 16706
cycles: false
bibtex_author: Guha, Etash Kumar and Lakshman, Vihan
author:
- given: Etash Kumar
  family: Guha
- given: Vihan
  family: Lakshman
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/assets/guha24a/guha24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
