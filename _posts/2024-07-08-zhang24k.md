---
title: Random Scaling and Momentum for Non-smooth Non-convex Optimization
openreview: NKirMgDsut
abstract: 'Training neural networks requires optimizing a loss function that may be
  highly irregular, and in particular neither convex nor smooth. Popular training
  algorithms are based on stochastic gradient descent with momentum (SGDM), for which
  classical analysis applies only if the loss is either convex or smooth. We show
  that a very small modification to SGDM closes this gap: simply scale the update
  at each time point by an exponentially distributed random scalar. The resulting
  algorithm achieves optimal convergence guarantees. Intriguingly, this result is
  not derived by a specific analysis of SGDM: instead, it falls naturally out of a
  more general framework for converting online convex optimization algorithms to non-convex
  optimization algorithms.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: zhang24k
month: 0
tex_title: Random Scaling and Momentum for Non-smooth Non-convex Optimization
firstpage: 58780
lastpage: 58799
page: 58780-58799
order: 58780
cycles: false
bibtex_author: Zhang, Qinzi and Cutkosky, Ashok
author:
- given: Qinzi
  family: Zhang
- given: Ashok
  family: Cutkosky
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/zhang24k/zhang24k.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
