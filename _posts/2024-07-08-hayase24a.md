---
title: Understanding MLP-Mixer as a wide and sparse MLP
openreview: 1dtYo5ywXZ
abstract: Multi-layer perceptron (MLP) is a fundamental component of deep learning,
  and recent MLP-based architectures, especially the MLP-Mixer, have achieved significant
  empirical success. Nevertheless, our understanding of why and how the MLP-Mixer
  outperforms conventional MLPs remains largely unexplored. In this work, we reveal
  that sparseness is a key mechanism underlying the MLP-Mixers. First, the Mixers
  have an effective expression as a wider MLP with Kronecker-product weights, clarifying
  that the Mixers efficiently embody several sparseness properties explored in deep
  learning. In the case of linear layers, the effective expression elucidates an implicit
  sparse regularization caused by the model architecture and a hidden relation to
  Monarch matrices, which is also known as another form of sparse parameterization.
  Next, for general cases, we empirically demonstrate quantitative similarities between
  the Mixer and the unstructured sparse-weight MLPs. Following a guiding principle
  proposed by Golubeva, Neyshabur and Gur-Ari (2021), which fixes the number of connections
  and increases the width and sparsity, the Mixers can demonstrate improved performance.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: hayase24a
month: 0
tex_title: Understanding {MLP}-Mixer as a wide and sparse {MLP}
firstpage: 17734
lastpage: 17758
page: 17734-17758
order: 17734
cycles: false
bibtex_author: Hayase, Tomohiro and Karakida, Ryo
author:
- given: Tomohiro
  family: Hayase
- given: Ryo
  family: Karakida
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://raw.githubusercontent.com/mlresearch/v235/main/assets/hayase24a/hayase24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
