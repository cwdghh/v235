---
title: 'FrameQuant: Flexible Low-Bit Quantization for Transformers'
openreview: xPypr0kufs
abstract: Transformers are the backbone of powerful foundation models for many Vision
  and Natural Language Processing tasks. But their compute and memory/storage footprint
  is large, and so, serving such models is expensive often requiring high-end hardware.
  To mitigate this difficulty, Post-Training Quantization seeks to modify a pre-trained
  model and quantize it to eight bits or lower, significantly boosting compute/memory/latency
  efficiency. Such models have been successfully quantized to four bits with some
  performance loss. In this work, we outline a simple scheme to quantize Transformer-based
  models to just two bits (plus some overhead) with only a small drop in accuracy.
  Key to our formulation is a concept borrowed from Harmonic analysis called Fusion
  Frames. Our main finding is that the quantization must take place not in the original
  weight space, but instead in the Fusion Frame representations. If quantization is
  interpreted as the addition of noise, our casting of the problem allows invoking
  an extensive body of known consistent recovery and noise robustness guarantees.
  Further, if desired, de-noising filters are known in closed form. We show empirically,
  via a variety of experiments, that (almost) two-bit quantization for Transformer
  models promises sizable efficiency gains. The code is available at https://github.com/vsingh-group/FrameQuant
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: adepu24a
month: 0
tex_title: "{F}rame{Q}uant: Flexible Low-Bit Quantization for Transformers"
firstpage: 203
lastpage: 227
page: 203-227
order: 203
cycles: false
bibtex_author: Adepu, Harshavardhan and Zeng, Zhanpeng and Zhang, Li and Singh, Vikas
author:
- given: Harshavardhan
  family: Adepu
- given: Zhanpeng
  family: Zeng
- given: Li
  family: Zhang
- given: Vikas
  family: Singh
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/adepu24a/adepu24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
