---
title: Double Momentum Method for Lower-Level Constrained Bilevel Optimization
openreview: 7OPHCeXcSS
abstract: Bilevel optimization (BO) has recently gained prominence in many machine
  learning applications due to its ability to capture the nested structure inherent
  in these problems. Recently, many hypergradient methods have been proposed as effective
  solutions for solving large-scale problems. However, current hypergradient methods
  for the lower-level constrained bilevel optimization (LCBO) problems need very restrictive
  assumptions, namely, where optimality conditions satisfy the differentiability and
  invertibility conditions, and lack a solid analysis of the convergence rate. Whatâ€™s
  worse, existing methods require either double-loop updates, which are sometimes
  less efficient. To solve this problem, in this paper, we propose a new hypergradient
  of LCBO leveraging the theory of nonsmooth implicit function theorem instead of
  using the restrive assumptions. In addition, we propose a <em>single-loop single-timescale</em>
  algorithm based on the double-momentum method and adaptive step size method and
  prove it can return a $(\delta, \epsilon)$-stationary point with $\tilde{\mathcal{O}}(d_2^2\epsilon^{-4})$
  iterations. Experiments on two applications demonstrate the effectiveness of our
  proposed method.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: shi24a
month: 0
tex_title: Double Momentum Method for Lower-Level Constrained Bilevel Optimization
firstpage: 44838
lastpage: 44864
page: 44838-44864
order: 44838
cycles: false
bibtex_author: Shi, Wanli and Chang, Yi and Gu, Bin
author:
- given: Wanli
  family: Shi
- given: Yi
  family: Chang
- given: Bin
  family: Gu
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://raw.githubusercontent.com/mlresearch/v235/main/assets/shi24a/shi24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
