---
title: 'Discounted Adaptive Online Learning: Towards Better Regularization'
openreview: ZoTIdyExx6
abstract: 'We study online learning in adversarial nonstationary environments. Since
  the future can be very different from the past, a critical challenge is to gracefully
  forget the history while new data comes in. To formalize this intuition, we revisit
  the discounted regret in online convex optimization, and propose an adaptive (i.e.,
  instance optimal), FTRL-based algorithm that improves the widespread non-adaptive
  baseline â€“ gradient descent with a constant learning rate. From a practical perspective,
  this refines the classical idea of regularization in lifelong learning: we show
  that designing better regularizers can be guided by the principled theory of adaptive
  online optimization. Complementing this result, we also consider the (Gibbs & Candes,
  2021)-style online conformal prediction problem, where the goal is to sequentially
  predict the uncertainty sets of a black-box machine learning model. We show that
  the FTRL nature of our algorithm can simplify the conventional gradient-descent-based
  analysis, leading to instance-dependent performance guarantees.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: zhang24e
month: 0
tex_title: 'Discounted Adaptive Online Learning: Towards Better Regularization'
firstpage: 58631
lastpage: 58661
page: 58631-58661
order: 58631
cycles: false
bibtex_author: Zhang, Zhiyu and Bombara, David and Yang, Heng
author:
- given: Zhiyu
  family: Zhang
- given: David
  family: Bombara
- given: Heng
  family: Yang
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/assets/zhang24e/zhang24e.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
