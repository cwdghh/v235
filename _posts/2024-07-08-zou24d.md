---
title: 'BiE: Bi-Exponent Block Floating-Point for Large Language Models Quantization'
openreview: DbyHDYslM7
abstract: Nowadays, Large Language Models (LLMs) mostly possess billions of parameters,
  bringing significant challenges to hardware platforms. Although quantization is
  an efficient approach to reduce computation and memory overhead for inference optimization,
  we stress the challenge that mainstream low-bit quantization approaches still suffer
  from either various data distribution outliers or a lack of hardware efficiency.
  We also find that low-bit data format has further potential expressiveness to cover
  the atypical language data distribution. In this paper, we propose a novel numerical
  representation, Bi-Exponent Block Floating Point (BiE), and a new quantization flow.
  BiE quantization shows accuracy superiority and hardware friendliness on various
  models and benchmarks.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: zou24d
month: 0
tex_title: "{B}i{E}: Bi-Exponent Block Floating-Point for Large Language Models Quantization"
firstpage: 62978
lastpage: 62992
page: 62978-62992
order: 62978
cycles: false
bibtex_author: Zou, Lancheng and Zhao, Wenqian and Yin, Shuo and Bai, Chen and Sun,
  Qi and Yu, Bei
author:
- given: Lancheng
  family: Zou
- given: Wenqian
  family: Zhao
- given: Shuo
  family: Yin
- given: Chen
  family: Bai
- given: Qi
  family: Sun
- given: Bei
  family: Yu
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/assets/zou24d/zou24d.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
