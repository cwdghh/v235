---
title: How Language Model Hallucinations Can Snowball
openreview: FPlaQyAGHu
abstract: 'A major risk of using language models in practical applications is their
  tendency to hallucinate incorrect statements. Hallucinations are often attributed
  to knowledge gaps in LMs, but we show that LMs sometimes produce hallucinations
  that they can separately recognize as incorrect. To do this, we construct three
  question-answering datasets where LMs often state an incorrect answer which is followed
  by an explanation with at least one incorrect claim. Crucially, we find that GPT-3.5,
  GPT-4, and LLaMA2-70B-chat can identify 67%, 87%, and 94% of these incorrect claims,
  respectively. We show that this phenomenon doesnâ€™t disappear under higher temperatures
  sampling, beam search, and zero-shot chain-of-thought prompting. These findings
  reveal that LM hallucinations can snowball: early mistakes by an LM can lead to
  more mistakes that otherwise would not be made.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: zhang24ay
month: 0
tex_title: How Language Model Hallucinations Can Snowball
firstpage: 59670
lastpage: 59684
page: 59670-59684
order: 59670
cycles: false
bibtex_author: Zhang, Muru and Press, Ofir and Merrill, William and Liu, Alisa and
  Smith, Noah A.
author:
- given: Muru
  family: Zhang
- given: Ofir
  family: Press
- given: William
  family: Merrill
- given: Alisa
  family: Liu
- given: Noah A.
  family: Smith
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/assets/zhang24ay/zhang24ay.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
