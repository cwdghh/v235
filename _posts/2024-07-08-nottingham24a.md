---
title: 'Skill Set Optimization: Reinforcing Language Model Behavior via Transferable
  Skills'
openreview: 9laB7ytoMp
abstract: Large language models (LLMs) have recently been used for sequential decision
  making in interactive environments. However, leveraging environment reward signals
  for continual LLM actor improvement is not straightforward. We propose Skill Set
  Optimization (SSO) for improving LLM actor performance through constructing and
  refining sets of transferable skills. SSO constructs skills by extracting common
  subtrajectories with high rewards and generating subgoals and instructions to represent
  each skill. These skills are provided to the LLM actor in-context to reinforce behaviors
  with high rewards. Then, SSO further refines the skill set by pruning skills that
  do not continue to result in high rewards. We evaluate our method in the classic
  videogame NetHack and the text environment ScienceWorld to demonstrate SSOâ€™s ability
  to optimize a set of skills and perform in-context policy improvement. SSO outperforms
  baselines by 40% in our custom NetHack task and outperforms the previous state-of-the-art
  in ScienceWorld by 35%.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: nottingham24a
month: 0
tex_title: 'Skill Set Optimization: Reinforcing Language Model Behavior via Transferable
  Skills'
firstpage: 38409
lastpage: 38425
page: 38409-38425
order: 38409
cycles: false
bibtex_author: Nottingham, Kolby and Majumder, Bodhisattwa Prasad and Dalvi Mishra,
  Bhavana and Singh, Sameer and Clark, Peter and Fox, Roy
author:
- given: Kolby
  family: Nottingham
- given: Bodhisattwa Prasad
  family: Majumder
- given: Bhavana
  family: Dalvi Mishra
- given: Sameer
  family: Singh
- given: Peter
  family: Clark
- given: Roy
  family: Fox
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://raw.githubusercontent.com/mlresearch/v235/main/assets/nottingham24a/nottingham24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
