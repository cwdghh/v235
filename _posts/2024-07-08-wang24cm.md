---
title: 'Calibration Bottleneck: Over-compressed Representations are Less Calibratable'
openreview: eRThYD9BGD
abstract: Although deep neural networks have achieved remarkable success, they often
  exhibit a significant deficiency in reliable uncertainty calibration. This paper
  focus on model calibratability, which assesses how amenable a model is to be well
  recalibrated post-hoc. We find that the widely used weight decay regularizer detrimentally
  affects model calibratability, subsequently leading to a decline in final calibration
  performance after post-hoc calibration. To identify the underlying causes leading
  to poor calibratability, we delve into the calibratability of intermediate features
  across the hidden layers. We observe a U-shaped trend in the calibratability of
  intermediate features from the bottom to the top layers, which indicates that over-compression
  of the top representation layers significantly hinders model calibratability. Based
  on the observations, this paper introduces a weak classifier hypothesis, i.e., given
  a weak classification head that has not been over-trained, the representation module
  can be better learned to produce more calibratable features. Consequently, we propose
  a progressively layer-peeled training (PLP) method to exploit this hypothesis, thereby
  enhancing model calibratability. Our comparative experiments show the effectiveness
  of our method, which improves model calibration and also yields competitive predictive
  performance.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: wang24cm
month: 0
tex_title: 'Calibration Bottleneck: Over-compressed Representations are Less Calibratable'
firstpage: 52156
lastpage: 52170
page: 52156-52170
order: 52156
cycles: false
bibtex_author: Wang, Deng-Bao and Zhang, Min-Ling
author:
- given: Deng-Bao
  family: Wang
- given: Min-Ling
  family: Zhang
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://raw.githubusercontent.com/mlresearch/v235/main/assets/wang24cm/wang24cm.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
