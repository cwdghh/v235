---
title: Rethinking Optimization and Architecture for Tiny Language Models
openreview: mHIEOZtDDF
abstract: The power of large language models (LLMs) has been demonstrated through
  numerous data and computing resources. However, the application of language models
  on mobile devices is facing huge challenge on the computation and memory costs,
  that is, tiny language models with high performance are urgently required. Limited
  by the highly complex training process, there are many details for optimizing language
  models that are seldom studied carefully. In this study, based on a tiny language
  model with 1B parameters, we carefully design a series of empirical study to analyze
  the effect of each component. Three perspectives are mainly discussed, i.e., neural
  architecture, parameter initialization, and optimization strategy. Several design
  formulas are empirically proved especially effective for tiny language models, including
  tokenizer compression, architecture tweaking, parameter inheritance and multiple-round
  training. Then we train PanGu-$\pi$-1B Pro and PanGu-$\pi$-1.5B Pro on 1.6T multilingual
  corpora, following the established formulas. Experimental results demonstrate the
  improved optimization and architecture yield a notable average improvement of 8.87
  on benchmark evaluation sets for PanGu-$\pi$-1B Pro. Besides, PanGu-$\pi$-1.5B Pro
  surpasses a range of SOTA models with larger model sizes, validating its superior
  performance. The code will be released soon. The code is available at https://github.com/YuchuanTian/RethinkTinyLM.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: tang24c
month: 0
tex_title: Rethinking Optimization and Architecture for Tiny Language Models
firstpage: 47743
lastpage: 47756
page: 47743-47756
order: 47743
cycles: false
bibtex_author: Tang, Yehui and Han, Kai and Liu, Fangcheng and Ni, Yunsheng and Tian,
  Yuchuan and Bai, Zheyuan and Hu, Yi-Qi and Liu, Sichao and Jui, Shangling and Wang,
  Yunhe
author:
- given: Yehui
  family: Tang
- given: Kai
  family: Han
- given: Fangcheng
  family: Liu
- given: Yunsheng
  family: Ni
- given: Yuchuan
  family: Tian
- given: Zheyuan
  family: Bai
- given: Yi-Qi
  family: Hu
- given: Sichao
  family: Liu
- given: Shangling
  family: Jui
- given: Yunhe
  family: Wang
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://raw.githubusercontent.com/mlresearch/v235/main/assets/tang24c/tang24c.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
