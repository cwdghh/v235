---
title: Differentiable Model Scaling using Differentiable Topk
openreview: bULHOW1RXM
abstract: Over the past few years, as large language models have ushered in an era
  of intelligence emergence, there has been an intensified focus on scaling networks.
  Although Neural Architecture Search (NAS) methods have been proposed to automate
  this process, they suffer from low search efficiency. This study introduces Differentiable
  Model Scaling (DMS), increasing the efficiency for searching optimal width and depth
  in networks. DMS can model both width and depth in a direct and fully differentiable
  way, making it easy to optimize. We have evaluated our DMS across diverse tasks,
  ranging from vision tasks to NLP tasks and various network architectures, including
  CNNs and Transformers. Results consistently indicate that our DMS can find improved
  structures and outperforms state-of-the-art NAS methods. Specifically, for image
  classification on ImageNet, our DMS improves the top-1 accuracy of EfficientNet-B0
  and Deit-Tiny by 1.4% and 0.6%, respectively, and outperforms the state-of-the-art
  zero-shot NAS method, ZiCo, by 1.3% while requiring only 0.4 GPU days for searching.
  For object detection on COCO, DMS improves the mAP of Yolo-v8-n by 2.0%. For language
  modeling, our pruned Llama-7B outperforms the prior method with lower perplexity
  and higher zero-shot classification accuracy. Our code is available at https://github.com/LKJacky/Differentiable-Model-Scaling.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: liu24bi
month: 0
tex_title: Differentiable Model Scaling using Differentiable Topk
firstpage: 31994
lastpage: 32014
page: 31994-32014
order: 31994
cycles: false
bibtex_author: Liu, Kai and Wang, Ruohui and Gao, Jianfei and Chen, Kai
author:
- given: Kai
  family: Liu
- given: Ruohui
  family: Wang
- given: Jianfei
  family: Gao
- given: Kai
  family: Chen
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/assets/liu24bi/liu24bi.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
