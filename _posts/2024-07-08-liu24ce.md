---
title: 'MobileLLM: Optimizing Sub-billion Parameter Language Models for On-Device
  Use Cases'
openreview: EIGbXbxcUQ
abstract: This paper addresses the growing need for efficient large language models
  (LLMs) on mobile devices, driven by increasing cloud costs and latency concerns.
  We focus on designing top-quality LLMs with fewer than a billion parameters, a practical
  choice for mobile deployment. Contrary to prevailing belief emphasizing the pivotal
  role of data and parameter quantity in determining model quality, our investigation
  underscores the significance of model architecture for sub-billion scale LLMs. Leveraging
  deep and thin architectures, coupled with embedding sharing and grouped-query attention
  mechanisms, we establish a strong baseline network denoted as MobileLLM, which attains
  a remarkable 2.7%/4.3% accuracy boost over preceding 125M/350M state-of-the-art
  models. Additionally, we propose an immediate block-wise weight-sharing approach
  with no increase in model size and only marginal latency overhead. The resultant
  models, denoted as MobileLLM-LS, demonstrate a further accuracy enhancement of 0.7%/0.8%
  than MobileLLM 125M/350M. Moreover, MobileLLM model family shows significant improvements
  compared to previous sub-billion models on chat benchmarks, and demonstrates close
  correctness to LLaMA-v2 7B in API calling tasks, highlighting the capability of
  small models for common on-device use cases.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: liu24ce
month: 0
tex_title: "{M}obile{LLM}: Optimizing Sub-billion Parameter Language Models for On-Device
  Use Cases"
firstpage: 32431
lastpage: 32454
page: 32431-32454
order: 32431
cycles: false
bibtex_author: Liu, Zechun and Zhao, Changsheng and Iandola, Forrest and Lai, Chen
  and Tian, Yuandong and Fedorov, Igor and Xiong, Yunyang and Chang, Ernie and Shi,
  Yangyang and Krishnamoorthi, Raghuraman and Lai, Liangzhen and Chandra, Vikas
author:
- given: Zechun
  family: Liu
- given: Changsheng
  family: Zhao
- given: Forrest
  family: Iandola
- given: Chen
  family: Lai
- given: Yuandong
  family: Tian
- given: Igor
  family: Fedorov
- given: Yunyang
  family: Xiong
- given: Ernie
  family: Chang
- given: Yangyang
  family: Shi
- given: Raghuraman
  family: Krishnamoorthi
- given: Liangzhen
  family: Lai
- given: Vikas
  family: Chandra
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://raw.githubusercontent.com/mlresearch/v235/main/assets/liu24ce/liu24ce.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
