---
title: 'Q-Probe: A Lightweight Approach to Reward Maximization for Language Models'
openreview: gxOQEMRbRa
abstract: 'We present an approach called Q-probing to adapt a pre-trained language
  model to maximize a task-specific reward function. At a high level, Q-probing sits
  between heavier approaches such as finetuning and lighter approaches such as few
  shot prompting, but can also be combined with either. The idea is to learn a simple
  linear function on a modelâ€™s embedding space that can be used to reweight candidate
  completions. We theoretically show that this sampling procedure is equivalent to
  a KL-constrained maximization of the Q-probe as the number of samples increases.
  To train the Q-probes we consider either reward modeling or a class of novel direct
  policy learning objectives based on importance-weighted policy gradients. With this
  technique, we see gains in domains with ground-truth rewards (code generation) as
  well as implicit rewards defined by preference data, even outperforming finetuning
  in data-limited regimes. Moreover, a Q-probe can be trained on top of an API since
  it only assumes access to sampling and embeddings. Code: https://github.com/likenneth/q_probe.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: li24ae
month: 0
tex_title: 'Q-Probe: A Lightweight Approach to Reward Maximization for Language Models'
firstpage: 27955
lastpage: 27968
page: 27955-27968
order: 27955
cycles: false
bibtex_author: Li, Kenneth and Jelassi, Samy and Zhang, Hugh and Kakade, Sham M. and
  Wattenberg, Martin and Brandfonbrener, David
author:
- given: Kenneth
  family: Li
- given: Samy
  family: Jelassi
- given: Hugh
  family: Zhang
- given: Sham M.
  family: Kakade
- given: Martin
  family: Wattenberg
- given: David
  family: Brandfonbrener
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/li24ae/li24ae.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
