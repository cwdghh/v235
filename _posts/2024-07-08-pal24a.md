---
title: Implicit Representations via Operator Learning
openreview: 2W3KUAaZgO
abstract: The idea of representing a signal as the weights of a neural network, called
  <em>Implicit Neural Representations</em> (INRs), has led to exciting implications
  for compression, view synthesis and 3D volumetric data understanding. One problem
  in this setting pertains to the use of INRs for downstream processing tasks. Despite
  some conceptual results, this remains challenging because the INR for a given image/signal
  often exists in isolation. What does the neighborhood around a given INR correspond
  to? Based on this question, we offer an operator theoretic reformulation of the
  INR model, which we call Operator INR (or O-INR). At a high level, instead of mapping
  positional encodings to a signal, O-INR maps one function space to another function
  space. A practical form of this general casting is obtained by appealing to Integral
  Transforms. The resultant model does not need multi-layer perceptrons (MLPs), used
  in most existing INR models â€“ we show that convolutions are sufficient and offer
  benefits including numerically stable behavior. We show that O-INR can easily handle
  most problem settings in the literature, and offers a similar performance profile
  as baselines. These benefits come with minimal, if any, compromise. Our code is
  available at https://github.com/vsingh-group/oinr.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: pal24a
month: 0
tex_title: Implicit Representations via Operator Learning
firstpage: 39022
lastpage: 39041
page: 39022-39041
order: 39022
cycles: false
bibtex_author: Pal, Sourav and Adepu, Harshavardhan and Wang, Clinton and Golland,
  Polina and Singh, Vikas
author:
- given: Sourav
  family: Pal
- given: Harshavardhan
  family: Adepu
- given: Clinton
  family: Wang
- given: Polina
  family: Golland
- given: Vikas
  family: Singh
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/pal24a/pal24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
