---
title: Decoupling Feature Extraction and Classification Layers for Calibrated Neural
  Networks
openreview: F2Tegvyqlo
abstract: Deep Neural Networks (DNN) have shown great promise in many classification
  applications, yet are widely known to have poorly calibrated predictions when they
  are over-parametrized. Improving DNN calibration without comprising on model accuracy
  is of extreme importance and interest in safety critical applications such as in
  the health-care sector. In this work, we show that decoupling the training of feature
  extraction layers and classification layers in over-parametrized DNN architectures
  such as Wide Residual Networks (WRN) and Vision Transformers (ViT) significantly
  improves model calibration whilst retaining accuracy, and at a low training cost.
  In addition, we show that placing a Gaussian prior on the last hidden layer outputs
  of a DNN, and training the model variationally in the classification training stage,
  even further improves calibration. We illustrate these methods improve calibration
  across ViT and WRN architectures for several image classification benchmark datasets.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: jordahn24a
month: 0
tex_title: Decoupling Feature Extraction and Classification Layers for Calibrated
  Neural Networks
firstpage: 22530
lastpage: 22550
page: 22530-22550
order: 22530
cycles: false
bibtex_author: Jordahn, Mikkel and Olmos, Pablo M.
author:
- given: Mikkel
  family: Jordahn
- given: Pablo M.
  family: Olmos
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/assets/jordahn24a/jordahn24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
