---
title: Multicalibration for Confidence Scoring in LLMs
openreview: 6Wauue8pWd
abstract: 'This paper proposes the use of "multicalibration": to yield interpretable
  and reliable confidence scores for outputs generated by large language models (LLMs).
  Multicalibration asks for calibration not just marginally, but simultaneously across
  various intersecting groupings of the data. We show how to form groupings for prompt/completion
  pairs that are correlated with the probability of correctness via two techniques:
  clustering within an embedding space, and "self-annotation" - querying the LLM by
  asking it various yes-or-no questions about the prompt. We also develop novel variants
  of multicalibration algorithms that offer performance improvements by reducing their
  tendency to overfit. Through systematic benchmarking across various question answering
  datasets and LLMs, we show how our techniques can yield confidence scores that provide
  substantial improvements in fine-grained measures of both calibration and accuracy
  compared to existing methods.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: detommaso24a
month: 0
tex_title: Multicalibration for Confidence Scoring in {LLM}s
firstpage: 10624
lastpage: 10641
page: 10624-10641
order: 10624
cycles: false
bibtex_author: Detommaso, Gianluca and Bertran, Martin Andres and Fogliato, Riccardo
  and Roth, Aaron
author:
- given: Gianluca
  family: Detommaso
- given: Martin Andres
  family: Bertran
- given: Riccardo
  family: Fogliato
- given: Aaron
  family: Roth
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/detommaso24a/detommaso24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
