---
title: Preference Fine-Tuning of LLMs Should Leverage Suboptimal, On-Policy Data
openreview: bWNPx6t0sF
abstract: 'Learning from preference labels plays a crucial role in fine-tuning large
  language models — this is done via supervised learning, on-policy reinforcement
  learning (RL), or contrastive learning. Different methods come with different implementation
  tradeoffs, and existing empirical findings present different conclusions, for instance,
  some results show that online RL is quite important to attain good fine-tuning results,
  while others find offline methods sufficient. This raises a question: <b>what kind
  of approaches are important for fine-tuning with preference data and why?</b> In
  this paper, we answer this question by performing a rigorous analysis of a number
  of fine-tuning techniques on didactic and full-scale LLM problems. Our main finding
  is that approaches that use on-policy sampling and attempt to push down the likelihood
  on certain responses (i.e., employ a ”negative gradient”) outperform offline and
  maximum likelihood objectives. We conceptualize our insights and unify methods that
  use on-policy sampling or negative gradient under a notion of mode-seeking objectives
  for categorical distributions. Mode-seeking objectives are able to alter probability
  mass on specific bins of a categorical distribution at a fast rate compared to maximum
  likelihood, allowing them to relocate masses across bins more effectively. Our analysis
  prescribes actionable insights for preference fine-tuning of LLMs and informs how
  data should be collected for maximal improvement.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: tajwar24a
month: 0
tex_title: Preference Fine-Tuning of {LLM}s Should Leverage Suboptimal, On-Policy
  Data
firstpage: 47441
lastpage: 47474
page: 47441-47474
order: 47441
cycles: false
bibtex_author: Tajwar, Fahim and Singh, Anikait and Sharma, Archit and Rafailov, Rafael
  and Schneider, Jeff and Xie, Tengyang and Ermon, Stefano and Finn, Chelsea and Kumar,
  Aviral
author:
- given: Fahim
  family: Tajwar
- given: Anikait
  family: Singh
- given: Archit
  family: Sharma
- given: Rafael
  family: Rafailov
- given: Jeff
  family: Schneider
- given: Tengyang
  family: Xie
- given: Stefano
  family: Ermon
- given: Chelsea
  family: Finn
- given: Aviral
  family: Kumar
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/tajwar24a/tajwar24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
