---
title: Rethinking Transformers in Solving POMDPs
openreview: SyY7ScNpGL
abstract: Sequential decision-making algorithms such as reinforcement learning (RL)
  in real-world scenarios inevitably face environments with partial observability.
  This paper scrutinizes the effectiveness of a popular architecture, namely Transformers,
  in Partially Observable Markov Decision Processes (POMDPs) and reveals its theoretical
  limitations. We establish that regular languages, which Transformers struggle to
  model, are reducible to POMDPs. This poses a significant challenge for Transformers
  in learning POMDP-specific inductive biases, due to their lack of inherent recurrence
  found in other models like RNNs. This paper casts doubt on the prevalent belief
  in Transformers as sequence models for RL and proposes to introduce a point-wise
  recurrent structure. The Deep Linear Recurrent Unit (LRU) emerges as a well-suited
  alternative for Partially Observable RL, with empirical results highlighting the
  sub-optimal performance of the Transformer and considerable strength of LRU.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: lu24h
month: 0
tex_title: Rethinking Transformers in Solving {POMDP}s
firstpage: 33089
lastpage: 33112
page: 33089-33112
order: 33089
cycles: false
bibtex_author: Lu, Chenhao and Shi, Ruizhe and Liu, Yuyao and Hu, Kaizhe and Du, Simon
  Shaolei and Xu, Huazhe
author:
- given: Chenhao
  family: Lu
- given: Ruizhe
  family: Shi
- given: Yuyao
  family: Liu
- given: Kaizhe
  family: Hu
- given: Simon Shaolei
  family: Du
- given: Huazhe
  family: Xu
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://raw.githubusercontent.com/mlresearch/v235/main/assets/lu24h/lu24h.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
