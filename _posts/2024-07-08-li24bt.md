---
title: 'EAGLE: Speculative Sampling Requires Rethinking Feature Uncertainty'
openreview: 1NdN7eXyb4
abstract: Autoregressive decoding makes the inference of Large Language Models (LLMs)
  time-consuming. In this paper, we reconsider speculative sampling and derive two
  key observations. Firstly, autoregression at the feature (second-to-top-layer) level
  is more straightforward than at the token level. Secondly, the inherent uncertainty
  in feature (second-to-top-layer) level autoregression constrains its performance.
  Based on these insights, we introduce EAGLE (Extrapolation Algorithm for Greater
  Language-model Efficiency), a simple yet highly efficient speculative sampling framework.
  By incorporating a token sequence advanced by one time step, EAGLE effectively resolves
  the uncertainty, enabling precise second-to-top-layer feature prediction with minimal
  overhead. We conducted comprehensive evaluations of EAGLE, including all models
  from the Vicuna and LLaMA2-Chat series, the MoE model Mixtral 8x7B Instruct, and
  tasks in dialogue, code generation, mathematical reasoning, and instruction following.
  For LLaMA2-Chat 70B, EAGLE achieved a latency speedup ratio of <b>2.7x-3.5x</b>,
  doubled throughput, while maintaining the distribution of the generated text.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: li24bt
month: 0
tex_title: "{EAGLE}: Speculative Sampling Requires Rethinking Feature Uncertainty"
firstpage: 28935
lastpage: 28948
page: 28935-28948
order: 28935
cycles: false
bibtex_author: Li, Yuhui and Wei, Fangyun and Zhang, Chao and Zhang, Hongyang
author:
- given: Yuhui
  family: Li
- given: Fangyun
  family: Wei
- given: Chao
  family: Zhang
- given: Hongyang
  family: Zhang
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://raw.githubusercontent.com/mlresearch/v235/main/assets/li24bt/li24bt.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
