---
title: Learning to Continually Learn with the Bayesian Principle
openreview: IpPnmhjw30
abstract: In the present era of deep learning, continual learning research is mainly
  focused on mitigating forgetting when training a neural network with stochastic
  gradient descent on a non-stationary stream of data. On the other hand, in the more
  classical literature of statistical machine learning, many models have sequential
  Bayesian update rules that yield the same learning outcome as the batch training,
  i.e., they are completely immune to catastrophic forgetting. However, they are often
  overly simple to model complex real-world data. In this work, we adopt the meta-learning
  paradigm to combine the strong representational power of neural networks and simple
  statistical modelsâ€™ robustness to forgetting. In our novel meta-continual learning
  framework, continual learning takes place only in statistical models via ideal sequential
  Bayesian update rules, while neural networks are meta-learned to bridge the raw
  data and the statistical models. Since the neural networks remain fixed during continual
  learning, they are protected from catastrophic forgetting. This approach not only
  achieves significantly improved performance but also exhibits excellent scalability.
  Since our approach is domain-agnostic and model-agnostic, it can be applied to a
  wide range of problems and easily integrated with existing model architectures.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: lee24j
month: 0
tex_title: Learning to Continually Learn with the {B}ayesian Principle
firstpage: 26621
lastpage: 26639
page: 26621-26639
order: 26621
cycles: false
bibtex_author: Lee, Soochan and Jeon, Hyeonseong and Son, Jaehyeon and Kim, Gunhee
author:
- given: Soochan
  family: Lee
- given: Hyeonseong
  family: Jeon
- given: Jaehyeon
  family: Son
- given: Gunhee
  family: Kim
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://raw.githubusercontent.com/mlresearch/v235/main/assets/lee24j/lee24j.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
