---
title: 'Collage: Light-Weight Low-Precision Strategy for LLM Training'
openreview: LkJ6qOMv77
abstract: Large models training is plagued by the intense compute cost and limited
  hardware memory. A practical solution is low-precision representation but is troubled
  by loss in numerical accuracy and unstable training rendering the model less useful.
  We argue that low-precision floating points can perform well provided the error
  is properly compensated at the critical locations in the training process. We propose
  Collage which utilizes multi-component float representation in low-precision to
  accurately perform operations with numerical errors accounted. To understand the
  impact of imprecision to training, we propose a simple and novel metric which tracks
  the lost information during training as well as differentiates various precision
  strategies. Our method works with commonly used low-precision such as half-precision
  ($16$-bit floating points) and can be naturally extended to work with even lower
  precision such as $8$-bit. Experimental results show that pre-training using Collage
  removes the requirement of using $32$-bit floating-point copies of the model and
  attains similar/better training performance compared to $(16, 32)$-bit mixed-precision
  strategy, with up to $3.7\times$ speedup and $\sim 15%$ to $23%$ less memory usage
  in practice. The code is available at https://github.com/amazon-science/collage.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: yu24d
month: 0
tex_title: 'Collage: Light-Weight Low-Precision Strategy for {LLM} Training'
firstpage: 57459
lastpage: 57479
page: 57459-57479
order: 57459
cycles: false
bibtex_author: Yu, Tao and Gupta, Gaurav and Gopalswamy, Karthick and Mamidala, Amith
  R and Zhou, Hao and Huynh, Jeffrey and Park, Youngsuk and Diamant, Ron and Deoras,
  Anoop and Huan, Luke
author:
- given: Tao
  family: Yu
- given: Gaurav
  family: Gupta
- given: Karthick
  family: Gopalswamy
- given: Amith R
  family: Mamidala
- given: Hao
  family: Zhou
- given: Jeffrey
  family: Huynh
- given: Youngsuk
  family: Park
- given: Ron
  family: Diamant
- given: Anoop
  family: Deoras
- given: Luke
  family: Huan
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/yu24d/yu24d.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
