---
title: Break the Sequential Dependency of LLM Inference Using Lookahead Decoding
openreview: eDjvSFOkXw
abstract: Autoregressive decoding of large language models (LLMs) is memory bandwidth
  bounded, resulting in high latency and significant wastes of the parallel processing
  power of modern accelerators. Existing methods for accelerating LLM decoding often
  require a draft model (e.g., speculative decoding), which is nontrivial to obtain
  and unable to generalize. In this paper, we introduce Lookahead decoding, an exact,
  parallel decoding algorithm that accelerates LLM decoding without needing auxiliary
  models or data stores. It allows trading per-step log(FLOPs) to reduce the number
  of total decoding steps, is more parallelizable on single or multiple modern accelerators,
  and is compatible with concurrent memory-efficient attention (e.g., FlashAttention).
  Our implementation of Lookahead decoding can speed up autoregressive decoding by
  up to 1.8x on MT-bench and 4x with strong scaling on multiple GPUs in code completion
  tasks. Our code is avialable at https://github.com/hao-ai-lab/LookaheadDecoding
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: fu24a
month: 0
tex_title: Break the Sequential Dependency of {LLM} Inference Using Lookahead Decoding
firstpage: 14060
lastpage: 14079
page: 14060-14079
order: 14060
cycles: false
bibtex_author: Fu, Yichao and Bailis, Peter and Stoica, Ion and Zhang, Hao
author:
- given: Yichao
  family: Fu
- given: Peter
  family: Bailis
- given: Ion
  family: Stoica
- given: Hao
  family: Zhang
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://raw.githubusercontent.com/mlresearch/v235/main/assets/fu24a/fu24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
