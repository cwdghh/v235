---
title: Near-Optimal Reinforcement Learning with Self-Play under Adaptivity Constraints
openreview: pPNMhdYMaz
abstract: We study the problem of multi-agent reinforcement learning (MARL) with adaptivity
  constraints â€” a new problem motivated by real-world applications where deployments
  of new policies are costly and the number of policy updates must be minimized. For
  two-player zero-sum Markov Games, we design a (policy) elimination based algorithm
  that achieves a regret of $\widetilde{O}(\sqrt{H^3 S^2 ABK})$, while the batch complexity
  is only $O(H+\log\log K)$. In the above, $S$ denotes the number of states, $A,B$
  are the number of actions for the two players respectively, $H$ is the horizon and
  $K$ is the number of episodes. Furthermore, we prove a batch complexity lower bound
  $\Omega(\frac{H}{\log_{A}K}+\log\log K)$ for all algorithms with $\widetilde{O}(\sqrt{K})$
  regret bound, which matches our upper bound up to logarithmic factors. As a byproduct,
  our techniques naturally extend to learning bandit games and reward-free MARL within
  near optimal batch complexity. To the best of our knowledge, these are the first
  line of results towards understanding MARL with low adaptivity.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: qiao24b
month: 0
tex_title: Near-Optimal Reinforcement Learning with Self-Play under Adaptivity Constraints
firstpage: 41430
lastpage: 41455
page: 41430-41455
order: 41430
cycles: false
bibtex_author: Qiao, Dan and Wang, Yu-Xiang
author:
- given: Dan
  family: Qiao
- given: Yu-Xiang
  family: Wang
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/qiao24b/qiao24b.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
