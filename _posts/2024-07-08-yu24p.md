---
title: 'Language Models are Super Mario: Absorbing Abilities from Homologous Models
  as a Free Lunch'
openreview: fq0NaiU8Ex
abstract: 'In this paper, we unveil that Language Models (LMs) can acquire new capabilities
  by assimilating parameters from homologous models without retraining or GPUs. We
  first introduce DARE to set most delta parameters (i.e., the disparity between fine-tuned
  and pre-trained parameters) to zeros without affecting the abilities of Supervised
  Fine-Tuning (SFT) LMs, which randomly <b>D</b>rops delta parameters with a ratio
  $p$ <b>A</b>nd <b>RE</b>scales the remaining ones by $1 / (1 - p)$ to approximate
  the original embeddings. Then, we use DARE as a versatile plug-in to sparsify delta
  parameters of multiple SFT homologous models for mitigating parameter interference
  and merge them into a single model by parameter fusing. We experiment with encoder-
  and decoder-based LMs, showing that: (1) SFT delta parameter value ranges are typically
  small (within 0.002) with extreme redundancy, and DARE can effortlessly eliminate
  90% or even 99% of them; (2) DARE can merge multiple task-specific LMs into one
  LM with diverse capabilities. Notably, this phenomenon is more pronounced in large-scale
  LMs, where the merged LM reveals the potential to surpass the performance of any
  source LM, providing a new discovery. We also utilize DARE to create a merged LM
  that ranks first among models with 7 billion parameters on the Open LLM Leaderboard.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: yu24p
month: 0
tex_title: 'Language Models are Super Mario: Absorbing Abilities from Homologous Models
  as a Free Lunch'
firstpage: 57755
lastpage: 57775
page: 57755-57775
order: 57755
cycles: false
bibtex_author: Yu, Le and Yu, Bowen and Yu, Haiyang and Huang, Fei and Li, Yongbin
author:
- given: Le
  family: Yu
- given: Bowen
  family: Yu
- given: Haiyang
  family: Yu
- given: Fei
  family: Huang
- given: Yongbin
  family: Li
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/assets/yu24p/yu24p.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
