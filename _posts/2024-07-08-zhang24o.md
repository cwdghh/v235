---
title: 'Watermarks in the Sand: Impossibility of Strong Watermarking for Language
  Models'
openreview: bM2s12t4hR
abstract: 'Watermarking generative models consists of planting a statistical signal
  (watermark) in a modelâ€™s output so that it can be later verified that the output
  was generated by the given model. A strong watermarking scheme satisfies the property
  that a computationally bounded attacker cannot erase the watermark without causing
  significant quality degradation. In this paper, we study the (im)possibility of
  strong watermarking schemes. We prove that, under well-specified and natural assumptions,
  strong watermarking is impossible to achieve. This holds even in the private detection
  algorithm setting, where the watermark insertion and detection algorithms share
  a secret key, unknown to the attacker. To prove this result, we introduce a generic
  efficient watermark attack; the attacker is not required to know the private key
  of the scheme or even which scheme is used. Our attack is based on two assumptions:
  (1) The attacker has access to a "quality oracle" that can evaluate whether a candidate
  output is a high-quality response to a prompt, and (2) The attacker has access to
  a "perturbation oracle" which can modify an output with a nontrivial probability
  of maintaining quality, and which induces an efficiently mixing random walk on high-quality
  outputs. We argue that both assumptions can be satisfied in practice by an attacker
  with weaker computational capabilities than the watermarked model itself, to which
  the attacker has only black-box access. Furthermore, our assumptions will likely
  only be easier to satisfy over time as models grow in capabilities and modalities.
  We demonstrate the feasibility of our attack by instantiating it to attack three
  existing watermarking schemes for large language models: Kirchenbauer et al. (2023),
  Kuditipudi et al. (2023), and Zhao et al. (2023), and include preliminary results
  on vision-language models. The same attack successfully removes the watermarks planted
  by all schemes, with only minor quality degradation.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: zhang24o
month: 0
tex_title: 'Watermarks in the Sand: Impossibility of Strong Watermarking for Language
  Models'
firstpage: 58851
lastpage: 58880
page: 58851-58880
order: 58851
cycles: false
bibtex_author: Zhang, Hanlin and Edelman, Benjamin L. and Francati, Danilo and Venturi,
  Daniele and Ateniese, Giuseppe and Barak, Boaz
author:
- given: Hanlin
  family: Zhang
- given: Benjamin L.
  family: Edelman
- given: Danilo
  family: Francati
- given: Daniele
  family: Venturi
- given: Giuseppe
  family: Ateniese
- given: Boaz
  family: Barak
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/zhang24o/zhang24o.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
