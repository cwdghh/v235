---
title: 'CRUXEval: A Benchmark for Code Reasoning, Understanding and Execution'
openreview: Ffpg52swvg
abstract: 'We present Code Reasoning, Understanding, and eXecution Evaluation, a benchmark
  consisting of 800 Python functions (3-13 lines). Each function comes with an input-output
  pair, leading to two natural tasks: input prediction and output prediction. First,
  we propose a general recipe for generating our execution benchmark by sampling from
  a model, which can be used for more challenging versions of the benchmark if needed.
  Second, we evaluate twenty code models on our benchmark and discover that many recent
  high-scoring models on HumanEval show no improvements on our benchmark. Third, we
  show that simple CoT and fine-tuning schemes can improve performance on our benchmark
  but remain far from solving it. The best setup, GPT-4 with chain of thought (CoT),
  achieves a pass@1 of 75% and 81% on input and output prediction, respectively. In
  contrast, Code Llama 34B achieves a pass@1 of 50% and 46% on input and output prediction.
  When it comes to reasoning about code, GPT-4 has a huge edge over other models but
  still fails consistently on some surprisingly simple Python programs.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: gu24c
month: 0
tex_title: "{CRUXE}val: A Benchmark for Code Reasoning, Understanding and Execution"
firstpage: 16568
lastpage: 16621
page: 16568-16621
order: 16568
cycles: false
bibtex_author: Gu, Alex and Roziere, Baptiste and Leather, Hugh James and Solar-Lezama,
  Armando and Synnaeve, Gabriel and Wang, Sida
author:
- given: Alex
  family: Gu
- given: Baptiste
  family: Roziere
- given: Hugh James
  family: Leather
- given: Armando
  family: Solar-Lezama
- given: Gabriel
  family: Synnaeve
- given: Sida
  family: Wang
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/assets/gu24c/gu24c.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
