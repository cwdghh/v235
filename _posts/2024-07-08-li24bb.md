---
title: Evaluating Quantized Large Language Models
openreview: DKKg5EFAFr
abstract: 'Post-training quantization (PTQ) has emerged as a promising technique to
  reduce the cost of large language models (LLMs). Specifically, PTQ can effectively
  mitigate memory consumption and reduce computational overhead in LLMs. To meet the
  requirements of both high efficiency and performance across diverse scenarios, a
  comprehensive evaluation of quantized LLMs is essential to guide the selection of
  quantization methods. This paper presents a thorough evaluation of these factors
  by evaluating the effect of PTQ on Weight, Activation, and KV Cache on 11 model
  families, including OPT, LLaMA2, Falcon, Bloomz, Mistral, ChatGLM, Vicuna, LongChat,
  StableLM, Gemma, and Mamba, with parameters ranging from 125M to 180B. The evaluation
  encompasses five types of tasks: basic NLP, emergent ability, trustworthiness, dialogue,
  and long-context tasks. Moreover, we also evaluate the state-of-the-art (SOTA) quantization
  methods to demonstrate their applicability. Based on the extensive experiments,
  we systematically summarize the effect of quantization, provide recommendations
  to apply quantization techniques, and point out future directions. The code can
  be found in https://github.com/thu-nics/qllm-eval.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: li24bb
month: 0
tex_title: Evaluating Quantized Large Language Models
firstpage: 28480
lastpage: 28524
page: 28480-28524
order: 28480
cycles: false
bibtex_author: Li, Shiyao and Ning, Xuefei and Wang, Luning and Liu, Tengxuan and
  Shi, Xiangsheng and Yan, Shengen and Dai, Guohao and Yang, Huazhong and Wang, Yu
author:
- given: Shiyao
  family: Li
- given: Xuefei
  family: Ning
- given: Luning
  family: Wang
- given: Tengxuan
  family: Liu
- given: Xiangsheng
  family: Shi
- given: Shengen
  family: Yan
- given: Guohao
  family: Dai
- given: Huazhong
  family: Yang
- given: Yu
  family: Wang
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/assets/li24bb/li24bb.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
