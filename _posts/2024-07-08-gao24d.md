---
title: Stochastic Weakly Convex Optimization beyond Lipschitz Continuity
openreview: pAyX8q1IIn
abstract: 'This paper considers stochastic weakly convex optimization without the
  standard Lipschitz continuity assumption. Based on new adaptive regularization (stepsize)
  strategies, we show that a wide class of stochastic algorithms, including the stochastic
  subgradient method, preserve the $\mathcal{O} ( 1 / \sqrt{K})$ convergence rate
  with constant failure rate. Our analyses rest on rather weak assumptions: the Lipschitz
  parameter can be either bounded by a general growth function of $\\|x\\|$ or locally
  estimated through independent random samples. Numerical experiments demonstrate
  the efficiency and robustness of our proposed stepsize policies.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: gao24d
month: 0
tex_title: Stochastic Weakly Convex Optimization beyond {L}ipschitz Continuity
firstpage: 14651
lastpage: 14680
page: 14651-14680
order: 14651
cycles: false
bibtex_author: Gao, Wenzhi and Deng, Qi
author:
- given: Wenzhi
  family: Gao
- given: Qi
  family: Deng
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/assets/gao24d/gao24d.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
