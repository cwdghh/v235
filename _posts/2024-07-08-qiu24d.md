---
title: Transferring Knowledge From Large Foundation Models to Small Downstream Models
openreview: XtDJaSe8jE
abstract: How do we transfer the relevant knowledge from ever larger foundation models
  into small, task-specific downstream models that can run at much lower costs? Standard
  transfer learning using pre-trained weights as the initialization transfers limited
  information and commits us to often massive pre-trained architectures. This procedure
  also precludes combining multiple pre-trained models that learn complementary information.
  To address these shortcomings, we introduce Adaptive Feature Transfer (AFT). Instead
  of transferring weights, AFT operates purely on features, thereby decoupling the
  choice of the pre-trained model from the smaller downstream model. Rather than indiscriminately
  compressing all pre-trained features, AFT adaptively transfers pre-trained features
  that are most useful for performing the downstream task, using a simple regularization
  that adds minimal overhead. Across multiple vision, language, and multi-modal datasets,
  AFT achieves significantly better downstream performance compared to alternatives
  with a similar computational cost. Furthermore, AFT reliably translates improvement
  in pre-trained models into improvement in downstream performance, even if the downstream
  model is over $50\times$ smaller, and can effectively transfer complementary information
  learned by multiple pre-trained models.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: qiu24d
month: 0
tex_title: Transferring Knowledge From Large Foundation Models to Small Downstream
  Models
firstpage: 41644
lastpage: 41657
page: 41644-41657
order: 41644
cycles: false
bibtex_author: Qiu, Shikai and Han, Boran and Maddix, Danielle C. and Zhang, Shuai
  and Wang, Bernie and Wilson, Andrew Gordon
author:
- given: Shikai
  family: Qiu
- given: Boran
  family: Han
- given: Danielle C.
  family: Maddix
- given: Shuai
  family: Zhang
- given: Bernie
  family: Wang
- given: Andrew Gordon
  family: Wilson
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://raw.githubusercontent.com/mlresearch/v235/main/assets/qiu24d/qiu24d.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
