---
title: Large Language Models Can Automatically Engineer Features for Few-Shot Tabular
  Learning
openreview: fRG45xL1WT
abstract: Large Language Models (LLMs), with their remarkable ability to tackle challenging
  and unseen reasoning problems, hold immense potential for tabular learning, that
  is vital for many real-world applications. In this paper, we propose a novel in-context
  learning framework, FeatLLM, which employs LLMs as feature engineers to produce
  an input data set that is optimally suited for tabular predictions. The generated
  features are used to infer class likelihood with a simple downstream machine learning
  model, such as linear regression and yields high performance few-shot learning.
  The proposed FeatLLM framework only uses this simple predictive model with the discovered
  features at inference time. Compared to existing LLM-based approaches, FeatLLM eliminates
  the need to send queries to the LLM for each sample at inference time. Moreover,
  it merely requires API-level access to LLMs, and overcomes prompt size limitations.
  As demonstrated across numerous tabular datasets from a wide range of domains, FeatLLM
  generates high-quality rules, significantly (10% on average) outperforming alternatives
  such as TabLLM and STUNT.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: han24f
month: 0
tex_title: Large Language Models Can Automatically Engineer Features for Few-Shot
  Tabular Learning
firstpage: 17454
lastpage: 17479
page: 17454-17479
order: 17454
cycles: false
bibtex_author: Han, Sungwon and Yoon, Jinsung and Arik, Sercan O and Pfister, Tomas
author:
- given: Sungwon
  family: Han
- given: Jinsung
  family: Yoon
- given: Sercan O
  family: Arik
- given: Tomas
  family: Pfister
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/assets/han24f/han24f.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
