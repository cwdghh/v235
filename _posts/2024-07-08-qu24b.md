---
title: 'Connect Later: Improving Fine-tuning for Robustness with Targeted Augmentations'
openreview: Uz4Qr40Y3C
abstract: Models trained on a labeled source domain often generalize poorly when deployed
  on an out-of-distribution (OOD) target domain. In the domain adaptation setting
  where unlabeled target data is available, self-supervised pretraining (e.g., contrastive
  learning or masked autoencoding) is a promising method to mitigate this performance
  drop. Pretraining depends on generic data augmentations (e.g., cropping or masking)
  to learn representations that generalize across domains, which may not work for
  all distribution shifts. In this paper, we show on real-world tasks that standard
  fine-tuning after pretraining does not consistently improve OOD error over simply
  training from scratch on labeled source data. To better leverage pretraining for
  distribution shifts, we propose the Connect Later framework, which fine-tunes the
  model with targeted augmentations designed with knowledge of the shift. Intuitively,
  pretraining learns good representations within the source and target domains, while
  fine-tuning with targeted augmentations improves generalization across domains.
  Connect Later achieves state-of-the-art OOD accuracy while maintaining comparable
  or better in-distribution accuracy on 4 real-world tasks in wildlife identification
  (iWildCam-WILDS), tumor detection (Camelyon17-WILDS), and astronomy (AstroClassification,
  Redshifts).
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: qu24b
month: 0
tex_title: 'Connect Later: Improving Fine-tuning for Robustness with Targeted Augmentations'
firstpage: 41769
lastpage: 41786
page: 41769-41786
order: 41769
cycles: false
bibtex_author: Qu, Helen and Xie, Sang Michael
author:
- given: Helen
  family: Qu
- given: Sang Michael
  family: Xie
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/assets/qu24b/qu24b.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
