---
title: Understanding Unimodal Bias in Multimodal Deep Linear Networks
openreview: CTEMHDSwIj
abstract: 'Using multiple input streams simultaneously to train multimodal neural
  networks is intuitively advantageous but practically challenging. A key challenge
  is unimodal bias, where a network overly relies on one modality and ignores others
  during joint training. We develop a theory of unimodal bias with multimodal deep
  linear networks to understand how architecture and data statistics influence this
  bias. This is the first work to calculate the duration of the unimodal phase in
  learning as a function of the depth at which modalities are fused within the network,
  dataset statistics, and initialization. We show that the deeper the layer at which
  fusion occurs, the longer the unimodal phase. A long unimodal phase can lead to
  a generalization deficit and permanent unimodal bias in the overparametrized regime.
  Our results, derived for multimodal linear networks, extend to nonlinear networks
  in certain settings. Taken together, this work illuminates pathologies of multimodal
  learning under joint training, showing that late and intermediate fusion architectures
  can give rise to long unimodal phases and permanent unimodal bias. Our code is available
  at: https://yedizhang.github.io/unimodal-bias.html.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: zhang24aa
month: 0
tex_title: Understanding Unimodal Bias in Multimodal Deep Linear Networks
firstpage: 59100
lastpage: 59125
page: 59100-59125
order: 59100
cycles: false
bibtex_author: Zhang, Yedi and Latham, Peter E. and Saxe, Andrew M
author:
- given: Yedi
  family: Zhang
- given: Peter E.
  family: Latham
- given: Andrew M
  family: Saxe
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/zhang24aa/zhang24aa.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
