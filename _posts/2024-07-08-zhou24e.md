---
title: On the Emergence of Cross-Task Linearity in Pretraining-Finetuning Paradigm
openreview: qg6AlnpEQH
abstract: The pretraining-finetuning paradigm has become the prevailing trend in modern
  deep learning. In this work, we discover an intriguing linear phenomenon in models
  that are initialized from a common pretrained checkpoint and finetuned on different
  tasks, termed as Cross-Task Linearity (CTL). Specifically, we show that if we linearly
  interpolate the weights of two finetuned models, the features in the weight-interpolated
  model are often approximately equal to the linear interpolation of features in two
  finetuned models at each layer. We provide comprehensive empirical evidence supporting
  that CTL consistently occurs for finetuned models that start from the same pretrained
  checkpoint. We conjecture that in the pretraining-finetuning paradigm, neural networks
  approximately function as linear maps, mapping from the parameter space to the feature
  space. Based on this viewpoint, our study unveils novel insights into explaining
  model merging/editing, particularly by translating operations from the parameter
  space to the feature space. Furthermore, we delve deeper into the root cause for
  the emergence of CTL, highlighting the role of pretraining.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: zhou24e
month: 0
tex_title: On the Emergence of Cross-Task Linearity in Pretraining-Finetuning Paradigm
firstpage: 61854
lastpage: 61884
page: 61854-61884
order: 61854
cycles: false
bibtex_author: Zhou, Zhanpeng and Chen, Zijun and Chen, Yilan and Zhang, Bo and Yan,
  Junchi
author:
- given: Zhanpeng
  family: Zhou
- given: Zijun
  family: Chen
- given: Yilan
  family: Chen
- given: Bo
  family: Zhang
- given: Junchi
  family: Yan
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/zhou24e/zhou24e.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
