---
title: 'Transformers are SSMs: Generalized Models and Efficient Algorithms Through
  Structured State Space Duality'
openreview: ztn8FCR1td
abstract: While Transformers have been the main architecture behind deep learning’s
  success in language modeling, state-space models (SSMs) such as Mamba have recently
  been shown to match or outperform Transformers at small to medium scale. We show
  that these families of models are actually quite closely related, and develop a
  rich framework of theoretical connections between SSMs and variants of attention,
  connected through various decompositions of a well-studied class of structured <em>semiseparable
  matrices</em>. Our state space duality (SSD) framework allows us to design a new
  architecture (<b>Mamba-2</b>) whose core layer is an a refinement of Mamba’s selective
  SSM that is 2-8$\times$ faster, while continuing to be competitive with Transformers
  on language modeling.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: dao24a
month: 0
tex_title: 'Transformers are {SSM}s: Generalized Models and Efficient Algorithms Through
  Structured State Space Duality'
firstpage: 10041
lastpage: 10071
page: 10041-10071
order: 10041
cycles: false
bibtex_author: Dao, Tri and Gu, Albert
author:
- given: Tri
  family: Dao
- given: Albert
  family: Gu
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/dao24a/dao24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
