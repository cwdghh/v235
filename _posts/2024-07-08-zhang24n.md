---
title: 'CaM: Cache Merging for Memory-efficient LLMs Inference'
openreview: LCTmppB165
abstract: Despite the exceptional performance of Large Language Models (LLMs), the
  substantial volume of key-value (KV) pairs cached during inference presents a barrier
  to their efficient deployment. To ameliorate this, recent works have aimed to selectively
  eliminate these caches, informed by the attention scores of associated tokens. However,
  such cache eviction invariably leads to output perturbation, regardless of the token
  choice. This perturbation escalates with the compression ratio, which can precipitate
  a marked deterioration in LLM inference performance. This paper introduces Cache
  Merging (CaM) as a solution to mitigate this challenge. CaM adaptively merges to-be-evicted
  caches into the remaining ones, employing a novel sampling strategy governed by
  the prominence of attention scores within discarded locations. In this manner, CaM
  enables memory-efficient LLMs to preserve critical token information, even obviating
  the need to maintain their corresponding caches. Extensive experiments utilizing
  LLaMA, OPT, and GPT-NeoX across various benchmarks corroborate CaMâ€™s proficiency
  in bolstering the performance of memory-efficient LLMs. Code is released at https://github.com/zyxxmu/cam.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: zhang24n
month: 0
tex_title: "{C}a{M}: Cache Merging for Memory-efficient {LLM}s Inference"
firstpage: 58840
lastpage: 58850
page: 58840-58850
order: 58840
cycles: false
bibtex_author: Zhang, Yuxin and Du, Yuxuan and Luo, Gen and Zhong, Yunshan and Zhang,
  Zhenyu and Liu, Shiwei and Ji, Rongrong
author:
- given: Yuxin
  family: Zhang
- given: Yuxuan
  family: Du
- given: Gen
  family: Luo
- given: Yunshan
  family: Zhong
- given: Zhenyu
  family: Zhang
- given: Shiwei
  family: Liu
- given: Rongrong
  family: Ji
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/zhang24n/zhang24n.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
