---
title: 'WARM: On the Benefits of Weight Averaged Reward Models'
openreview: s7RDnNUJy6
abstract: 'Aligning large language models (LLMs) with human preferences through reinforcement
  learning (RLHF) can lead to reward hacking, where LLMs exploit failures in the reward
  model (RM) to achieve seemingly high rewards without meeting the underlying objectives.
  We identify two primary challenges when designing RMs to mitigate reward hacking:
  distribution shifts during the RL process and inconsistencies in human preferences.
  As a solution, we propose Weight Averaged Reward Models (WARM), first fine-tuning
  multiple RMs, then averaging them in the weight space. This strategy follows the
  observation that fine-tuned weights remain linearly mode connected when sharing
  the same pre-training. By averaging weights, WARM improves efficiency compared to
  the traditional ensembling of predictions, while improving reliability under distribution
  shifts and robustness to preference inconsistencies. Our experiments on summarization
  tasks, using best-of-N and RL methods, shows that WARM improves the overall quality
  and alignment of LLM predictions; for example, a policy RL fine-tuned with WARM
  has a 79.4% win rate against a policy RL fine-tuned with a single RM.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: rame24a
month: 0
tex_title: "{WARM}: On the Benefits of Weight Averaged Reward Models"
firstpage: 42048
lastpage: 42073
page: 42048-42073
order: 42048
cycles: false
bibtex_author: Rame, Alexandre and Vieillard, Nino and Hussenot, Leonard and Dadashi-Tazehozi,
  Robert and Cideron, Geoffrey and Bachem, Olivier and Ferret, Johan
author:
- given: Alexandre
  family: Rame
- given: Nino
  family: Vieillard
- given: Leonard
  family: Hussenot
- given: Robert
  family: Dadashi-Tazehozi
- given: Geoffrey
  family: Cideron
- given: Olivier
  family: Bachem
- given: Johan
  family: Ferret
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/assets/rame24a/rame24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
