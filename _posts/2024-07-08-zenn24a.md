---
title: Differentiable Annealed Importance Sampling Minimizes The Jensen-Shannon Divergence
  Between Initial and Target Distribution
openreview: rvaN2P1rvC
abstract: Differentiable annealed importance sampling (DAIS), proposed by Geffner
  & Domke (2021) and Zhang et al. (2021), allows optimizing, among others, over the
  initial distribution of AIS. In this paper, we show that, in the limit of many transitions,
  DAIS minimizes the symmetrized KL divergence (Jensen-Shannon divergence) between
  the initial and target distribution. Thus, DAIS can be seen as a form of variational
  inference (VI) in that its initial distribution is a parametric fit to an intractable
  target distribution. We empirically evaluate the usefulness of the initial distribution
  as a variational distribution on synthetic and real-world data, observing that it
  often provides more accurate uncertainty estimates than standard VI (optimizing
  the reverse KL divergence), importance weighted VI, and Markovian score climbing
  (optimizing the forward KL divergence).
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: zenn24a
month: 0
tex_title: Differentiable Annealed Importance Sampling Minimizes The Jensen-Shannon
  Divergence Between Initial and Target Distribution
firstpage: 58448
lastpage: 58469
page: 58448-58469
order: 58448
cycles: false
bibtex_author: Zenn, Johannes and Bamler, Robert
author:
- given: Johannes
  family: Zenn
- given: Robert
  family: Bamler
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://raw.githubusercontent.com/mlresearch/v235/main/assets/zenn24a/zenn24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
