---
title: 'Iterative Data Smoothing: Mitigating Reward Overfitting and Overoptimization
  in RLHF'
openreview: WXg6MJo1FH
abstract: Reinforcement Learning from Human Feedback (RLHF) is a pivotal technique
  that aligns language models closely with human-centric values. The initial phase
  of RLHF involves learning human values using a reward model from ranking data. It
  is observed that the performance of the reward model degrades after one epoch of
  training, and optimizing too much against the learned reward model eventually hinders
  the true objective. This paper analyzes potential reasons behind the issues, and
  designs improved reward learning algorithm termed ’Iterative Data Smoothing’ (IDS).
  The core idea is that during each training epoch, we not only update the model with
  the data, but also update the date using the model, replacing hard labels with soft
  labels. Our empirical findings highlight the superior performance of this approach
  over the traditional methods.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: zhu24e
month: 0
tex_title: 'Iterative Data Smoothing: Mitigating Reward Overfitting and Overoptimization
  in {RLHF}'
firstpage: 62405
lastpage: 62428
page: 62405-62428
order: 62405
cycles: false
bibtex_author: Zhu, Banghua and Jordan, Michael and Jiao, Jiantao
author:
- given: Banghua
  family: Zhu
- given: Michael
  family: Jordan
- given: Jiantao
  family: Jiao
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/assets/zhu24e/zhu24e.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
