---
title: On the Last-Iterate Convergence of Shuffling Gradient Methods
openreview: Xdy9bjwHDu
abstract: 'Shuffling gradient methods are widely used in modern machine learning tasks
  and include three popular implementations: Random Reshuffle (RR), Shuffle Once (SO),
  and Incremental Gradient (IG). Compared to the empirical success, the theoretical
  guarantee of shuffling gradient methods was not well-understood for a long time.
  Until recently, the convergence rates had just been established for the average
  iterate for convex functions and the last iterate for strongly convex problems (using
  squared distance as the metric). However, when using the function value gap as the
  convergence criterion, existing theories cannot interpret the good performance of
  the last iterate in different settings (e.g., constrained optimization). To bridge
  this gap between practice and theory, we prove the first last-iterate convergence
  rates for shuffling gradient methods with respect to the objective value even without
  strong convexity. Our new results either (nearly) match the existing last-iterate
  lower bounds or are as fast as the previous best upper bounds for the average iterate.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: liu24cg
month: 0
tex_title: On the Last-Iterate Convergence of Shuffling Gradient Methods
firstpage: 32471
lastpage: 32508
page: 32471-32508
order: 32471
cycles: false
bibtex_author: Liu, Zijian and Zhou, Zhengyuan
author:
- given: Zijian
  family: Liu
- given: Zhengyuan
  family: Zhou
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/liu24cg/liu24cg.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
