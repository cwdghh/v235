---
title: Optimally Improving Cooperative Learning in a Social Setting
openreview: Sz9mAYuqlE
abstract: 'We consider a cooperative learning scenario where a collection of networked
  agents with individually owned classifiers dynamically update their predictions,
  for the same classification task, through communication or observations of each
  otherâ€™s predictions. Clearly if highly influential vertices use erroneous classifiers,
  there will be a negative effect on the accuracy of all the agents in the network.
  We ask the following question: how can we optimally fix the prediction of a few
  classifiers so as maximize the overall accuracy in the entire network. To this end
  we consider an aggregate and an egalitarian objective function. We show a polynomial
  time algorithm for optimizing the aggregate objective function, and show that optimizing
  the egalitarian objective function is NP-hard. Furthermore, we develop approximation
  algorithms for the egalitarian improvement. The performance of all of our algorithms
  are guaranteed by mathematical analysis and backed by experiments on synthetic and
  real data.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: haddadan24a
month: 0
tex_title: Optimally Improving Cooperative Learning in a Social Setting
firstpage: 17148
lastpage: 17188
page: 17148-17188
order: 17148
cycles: false
bibtex_author: Haddadan, Shahrzad and Xin, Cheng and Gao, Jie
author:
- given: Shahrzad
  family: Haddadan
- given: Cheng
  family: Xin
- given: Jie
  family: Gao
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/assets/haddadan24a/haddadan24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
