---
title: Mixtures of Experts Unlock Parameter Scaling for Deep RL
openreview: X9VMhfFxwn
abstract: 'The recent rapid progress in (self) supervised learning models is in large
  part predicted by empirical scaling laws: a modelâ€™s performance scales proportionally
  to its size. Analogous scaling laws remain elusive for reinforcement learning domains,
  however, where increasing the parameter count of a model often hurts its final performance.
  In this paper, we demonstrate that incorporating Mixture-of-Expert (MoE) modules,
  and in particular Soft MoEs (Puigcerver et al., 2023), into value-based networks
  results in more parameter-scalable models, evidenced by substantial performance
  increases across a variety of training regimes and model sizes. This work thus provides
  strong empirical evidence towards developing scaling laws for reinforcement learning.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: obando-ceron24b
month: 0
tex_title: Mixtures of Experts Unlock Parameter Scaling for Deep {RL}
firstpage: 38520
lastpage: 38540
page: 38520-38540
order: 38520
cycles: false
bibtex_author: Obando Ceron, Johan Samir and Sokar, Ghada and Willi, Timon and Lyle,
  Clare and Farebrother, Jesse and Foerster, Jakob Nicolaus and Dziugaite, Gintare
  Karolina and Precup, Doina and Castro, Pablo Samuel
author:
- given: Johan Samir
  family: Obando Ceron
- given: Ghada
  family: Sokar
- given: Timon
  family: Willi
- given: Clare
  family: Lyle
- given: Jesse
  family: Farebrother
- given: Jakob Nicolaus
  family: Foerster
- given: Gintare Karolina
  family: Dziugaite
- given: Doina
  family: Precup
- given: Pablo Samuel
  family: Castro
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/assets/obando-ceron24b/obando-ceron24b.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
