---
title: ReLUs Are Sufficient for Learning Implicit Neural Representations
openreview: srejp9uOx7
abstract: Motivated by the growing theoretical understanding of neural networks that
  employ the Rectified Linear Unit (ReLU) as their activation function, we revisit
  the use of ReLU activation functions for learning implicit neural representations
  (INRs). Inspired by second order B-spline wavelets, we incorporate a set of simple
  constraints to the ReLU neurons in each layer of a deep neural network (DNN) to
  remedy the spectral bias. This in turn enables its use for various INR tasks. Empirically,
  we demonstrate that, contrary to popular belief, one <em>can learn</em> state-of-the-art
  INRs based on a DNN composed of only ReLU neurons. Next, by leveraging recent theoretical
  works which characterize the kinds of functions ReLU neural networks learn, we provide
  a way to quantify the regularity of the learned function. This offers a principled
  approach to selecting the hyperparameters in INR architectures. We substantiate
  our claims through experiments in signal representation, super resolution, and computed
  tomography, demonstrating the versatility and effectiveness of our method. The code
  for all experiments can be found at https://github.com/joeshenouda/relu-inrs.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: shenouda24a
month: 0
tex_title: "{R}e{LU}s Are Sufficient for Learning Implicit Neural Representations"
firstpage: 44800
lastpage: 44814
page: 44800-44814
order: 44800
cycles: false
bibtex_author: Shenouda, Joseph and Zhou, Yamin and Nowak, Robert D
author:
- given: Joseph
  family: Shenouda
- given: Yamin
  family: Zhou
- given: Robert D
  family: Nowak
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://raw.githubusercontent.com/mlresearch/v235/main/assets/shenouda24a/shenouda24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
