---
title: Understanding Finetuning for Factual Knowledge Extraction
openreview: cPsn9AcOYh
abstract: In this work, we study the impact of QA fine-tuning data on downstream factuality.
  We show that fine-tuning on lesser-known facts that are poorly stored during pretraining
  yields significantly worse factuality than fine-tuning on well-known facts, even
  when all facts are seen during pretraining. We prove this phenomenon theoretically,
  showing that training on lesser-known facts can lead the model to ignore subject
  entity names and instead output a generic plausible response even when the relevant
  factual knowledge is encoded in the model. On three question answering benchmarks
  (PopQA, Entity Questions, and MMLU) and two language models (Llama-2-7B and Mistral-7B),
  we find that (i) finetuning on a completely factual but lesser-known subset of the
  data deteriorates downstream factuality (5-10%) and (ii) finetuning on a subset
  of better-known examples matches or outperforms finetuning on the entire dataset.
  Ultimately, our results shed light on the interaction between pretrained knowledge
  and finetuning data and demonstrate the importance of taking into account how facts
  are stored in the pretrained model when fine-tuning for knowledge-intensive tasks.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: ghosal24a
month: 0
tex_title: Understanding Finetuning for Factual Knowledge Extraction
firstpage: 15540
lastpage: 15558
page: 15540-15558
order: 15540
cycles: false
bibtex_author: Ghosal, Gaurav Rohit and Hashimoto, Tatsunori and Raghunathan, Aditi
author:
- given: Gaurav Rohit
  family: Ghosal
- given: Tatsunori
  family: Hashimoto
- given: Aditi
  family: Raghunathan
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://raw.githubusercontent.com/mlresearch/v235/main/assets/ghosal24a/ghosal24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
