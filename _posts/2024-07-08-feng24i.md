---
title: Memory Efficient Neural Processes via Constant Memory Attention Block
openreview: xtwCf7iAs2
abstract: Neural Processes (NPs) are popular meta-learning methods for efficiently
  modelling predictive uncertainty. Recent state-of-the-art methods, however, leverage
  expensive attention mechanisms, limiting their applications, particularly in low-resource
  settings. In this work, we propose Constant Memory Attentive Neural Processes (CMANPs),
  an NP variant that only requires <b>constant</b> memory. To do so, we first propose
  an efficient update operation for Cross Attention. Leveraging the update operation,
  we propose Constant Memory Attention Block (CMAB), a novel attention block that
  (i) is permutation invariant, (ii) computes its output in constant memory, and (iii)
  performs constant computation updates. Finally, building on CMAB, we detail Constant
  Memory Attentive Neural Processes. Empirically, we show CMANPs achieve state-of-the-art
  results on popular NP benchmarks while being significantly more memory efficient
  than prior methods.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: feng24i
month: 0
tex_title: Memory Efficient Neural Processes via Constant Memory Attention Block
firstpage: 13365
lastpage: 13386
page: 13365-13386
order: 13365
cycles: false
bibtex_author: Feng, Leo and Tung, Frederick and Hajimirsadeghi, Hossein and Bengio,
  Yoshua and Ahmed, Mohamed Osama
author:
- given: Leo
  family: Feng
- given: Frederick
  family: Tung
- given: Hossein
  family: Hajimirsadeghi
- given: Yoshua
  family: Bengio
- given: Mohamed Osama
  family: Ahmed
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/feng24i/feng24i.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
