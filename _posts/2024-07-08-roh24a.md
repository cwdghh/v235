---
title: 'LEVI: Generalizable Fine-tuning via Layer-wise Ensemble of Different Views'
openreview: 4ZrppmS42b
abstract: Fine-tuning is becoming widely used for leveraging the power of pre-trained
  foundation models in new downstream tasks. While there are many successes of fine-tuning
  on various tasks, recent studies have observed challenges in the generalization
  of fine-tuned models to unseen distributions (i.e., out-of-distribution; OOD). To
  improve OOD generalization, some previous studies identify the limitations of fine-tuning
  data and regulate fine-tuning to preserve the general representation learned from
  pre-training data. However, potential limitations in the pre-training data and models
  are often ignored. In this paper, we contend that overly relying on the pre-trained
  representation may hinder fine-tuning from learning essential representations for
  downstream tasks and thus hurt its OOD generalization. It can be especially catastrophic
  when new tasks are from different (sub)domains compared to pre-training data. To
  address the issues in both pre-training and fine-tuning data, we propose a novel
  generalizable fine-tuning method LEVI (<b>L</b>ayer-wise <b>E</b>nsemble of different
  <b>VI</b>ews), where the pre-trained model is adaptively ensembled layer-wise with
  a small task-specific model, while preserving its efficiencies. By combining two
  complementing models, LEVI effectively suppresses problematic features in both the
  fine-tuning data and pre-trained model and preserves useful features for new tasks.
  Broad experiments with large language and vision models show that LEVI greatly improves
  fine-tuning generalization via emphasizing different views from fine-tuning data
  and pre-trained features.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: roh24a
month: 0
tex_title: "{LEVI}: Generalizable Fine-tuning via Layer-wise Ensemble of Different
  Views"
firstpage: 42666
lastpage: 42690
page: 42666-42690
order: 42666
cycles: false
bibtex_author: Roh, Yuji and Liu, Qingyun and Gui, Huan and Yuan, Zhe and Tang, Yujin
  and Whang, Steven Euijong and Liu, Liang and Bi, Shuchao and Hong, Lichan and Chi,
  Ed H. and Zhao, Zhe
author:
- given: Yuji
  family: Roh
- given: Qingyun
  family: Liu
- given: Huan
  family: Gui
- given: Zhe
  family: Yuan
- given: Yujin
  family: Tang
- given: Steven Euijong
  family: Whang
- given: Liang
  family: Liu
- given: Shuchao
  family: Bi
- given: Lichan
  family: Hong
- given: Ed H.
  family: Chi
- given: Zhe
  family: Zhao
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://raw.githubusercontent.com/mlresearch/v235/main/assets/roh24a/roh24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
