---
title: Exploring the Benefit of Activation Sparsity in Pre-training
openreview: KfXXPCcobh
abstract: Pre-trained Transformers inherently possess the characteristic of sparse
  activation, where only a small fraction of the neurons are activated for each token.
  While sparse activation has been explored through post-training methods, its potential
  in pre-training remains untapped. In this work, we first study how activation properties
  change during pre-training. Our examination reveals that Transformers exhibit sparse
  activation throughout the majority of the pre-training process while the activation
  correlation keeps evolving as training progresses. Leveraging this observation,
  we propose Switchable Sparse-Dense Learning (SSD). SSD adaptively switches between
  the Mixtures-of-Experts (MoE) based sparse training and the conventional dense training
  during the pre-training process, leveraging the efficiency of sparse training and
  avoiding the static activation correlation of sparse training. Compared to dense
  training, SSD achieves comparable performance with identical model size and reduces
  pre-training costs. Moreover, the models trained with SSD can be directly used as
  MoE models for sparse inference and achieve the same performance as dense models
  with up to $2\times$ faster inference speed. Codes are available at https://github.com/thunlp/moefication.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: zhang24bq
month: 0
tex_title: Exploring the Benefit of Activation Sparsity in Pre-training
firstpage: 60040
lastpage: 60056
page: 60040-60056
order: 60040
cycles: false
bibtex_author: Zhang, Zhengyan and Xiao, Chaojun and Qin, Qiujieli and Lin, Yankai
  and Zeng, Zhiyuan and Han, Xu and Liu, Zhiyuan and Xie, Ruobing and Sun, Maosong
  and Zhou, Jie
author:
- given: Zhengyan
  family: Zhang
- given: Chaojun
  family: Xiao
- given: Qiujieli
  family: Qin
- given: Yankai
  family: Lin
- given: Zhiyuan
  family: Zeng
- given: Xu
  family: Han
- given: Zhiyuan
  family: Liu
- given: Ruobing
  family: Xie
- given: Maosong
  family: Sun
- given: Jie
  family: Zhou
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/assets/zhang24bq/zhang24bq.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
