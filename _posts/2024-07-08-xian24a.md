---
title: Delving into the Convergence of Generalized Smooth Minimax Optimization
openreview: QPsEPI9bvp
abstract: Minimax optimization is fundamental and important for enormous machine learning
  applications such as generative adversarial network, adversarial training, and robust
  optimization. Recently, a variety of minimax algorithms with theoretical guarantees
  based on Lipschitz smoothness have been proposed. However, these algorithms could
  fail to converge in practice because the requisite Lipschitz smooth condition may
  not hold even in some classic minimax problems. We will present some counterexamples
  to reveal this divergence issue. Thus, to fill this gap, we are motivated to delve
  into the convergence analysis of minimax algorithms under a relaxed Lipschitz smoothness
  condition, <em>i.e.</em>, generalized smoothness. We prove that variants of basic
  minimax optimization algorithms GDA, SGDA, GDmax and SGDmax can still converge in
  generalized smooth problems, and hence their theoretical guarantees can be extended
  to a wider range of applications. We also conduct a numerical experiment to validate
  the performance of our proposed algorithms.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: xian24a
month: 0
tex_title: Delving into the Convergence of Generalized Smooth Minimax Optimization
firstpage: 54191
lastpage: 54211
page: 54191-54211
order: 54191
cycles: false
bibtex_author: Xian, Wenhan and Chen, Ziyi and Huang, Heng
author:
- given: Wenhan
  family: Xian
- given: Ziyi
  family: Chen
- given: Heng
  family: Huang
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://raw.githubusercontent.com/mlresearch/v235/main/assets/xian24a/xian24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
