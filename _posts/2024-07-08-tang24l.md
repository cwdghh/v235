---
title: 'QUEST: Query-Aware Sparsity for Efficient Long-Context LLM Inference'
openreview: KzACYw0MTV
abstract: As the demand for long-context large language models (LLMs) increases, models
  with context windows of up to 128K or 1M tokens are becoming increasingly prevalent.
  However, long-context LLM inference is challenging since the inference speed decreases
  significantly as the sequence length grows. This slowdown is primarily caused by
  loading a large KV cache during self-attention. Previous works have shown that a
  small portion of critical tokens will dominate the attention outcomes. However,
  we observe the criticality of a token highly depends on the query. To this end,
  we propose Quest, a query-aware KV cache selection algorithm. Quest keeps track
  of the minimal and maximal Key values in KV cache pages and estimates the criticality
  of a given page using Query vectors. By only loading the Top-K critical KV cache
  pages for attention, Quest significantly speeds up self-attention without sacrificing
  accuracy. We show that Quest can achieve up to 2.23x self-attention speedup, which
  reduces inference latency by 7.03x while performing well on tasks with long dependencies
  with negligible accuracy loss. Code is available at https://github.com/mit-han-lab/quest.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: tang24l
month: 0
tex_title: "{QUEST}: Query-Aware Sparsity for Efficient Long-Context {LLM} Inference"
firstpage: 47901
lastpage: 47911
page: 47901-47911
order: 47901
cycles: false
bibtex_author: Tang, Jiaming and Zhao, Yilong and Zhu, Kan and Xiao, Guangxuan and
  Kasikci, Baris and Han, Song
author:
- given: Jiaming
  family: Tang
- given: Yilong
  family: Zhao
- given: Kan
  family: Zhu
- given: Guangxuan
  family: Xiao
- given: Baris
  family: Kasikci
- given: Song
  family: Han
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/assets/tang24l/tang24l.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
