---
title: LLM-Empowered State Representation for Reinforcement Learning
openreview: xJMZbdiQnf
abstract: Conventional state representations in reinforcement learning often omit
  critical task-related details, presenting a significant challenge for value networks
  in establishing accurate mappings from states to task rewards. Traditional methods
  typically depend on extensive sample learning to enrich state representations with
  task-specific information, which leads to low sample efficiency and high time costs.
  Recently, surging knowledgeable large language models (LLM) have provided promising
  substitutes for prior injection with minimal human intervention. Motivated by this,
  we propose LLM-Empowered State Representation (LESR), a novel approach that utilizes
  LLM to autonomously generate task-related state representation codes which help
  to enhance the continuity of network mappings and facilitate efficient training.
  Experimental results demonstrate LESR exhibits high sample efficiency and outperforms
  state-of-the-art baselines by an average of <b>29%</b> in accumulated reward in
  Mujoco tasks and <b>30%</b> in success rates in Gym-Robotics tasks. Codes of LESR
  are accessible at https://github.com/thu-rllab/LESR.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: wang24bh
month: 0
tex_title: "{LLM}-Empowered State Representation for Reinforcement Learning"
firstpage: 51348
lastpage: 51375
page: 51348-51375
order: 51348
cycles: false
bibtex_author: Wang, Boyuan and Qu, Yun and Jiang, Yuhang and Shao, Jianzhun and Liu,
  Chang and Yang, Wenming and Ji, Xiangyang
author:
- given: Boyuan
  family: Wang
- given: Yun
  family: Qu
- given: Yuhang
  family: Jiang
- given: Jianzhun
  family: Shao
- given: Chang
  family: Liu
- given: Wenming
  family: Yang
- given: Xiangyang
  family: Ji
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/assets/wang24bh/wang24bh.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
