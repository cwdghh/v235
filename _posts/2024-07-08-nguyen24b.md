---
title: A General Theory for Softmax Gating Multinomial Logistic Mixture of Experts
openreview: 2Sl0lPF6ka
abstract: Mixture-of-experts (MoE) model incorporates the power of multiple submodels
  via gating functions to achieve greater performance in numerous regression and classification
  applications. From a theoretical perspective, while there have been previous attempts
  to comprehend the behavior of that model under the regression settings through the
  convergence analysis of maximum likelihood estimation in the Gaussian MoE model,
  such analysis under the setting of a classification problem has remained missing
  in the literature. We close this gap by establishing the convergence rates of density
  estimation and parameter estimation in the softmax gating multinomial logistic MoE
  model. Notably, when part of the expert parameters vanish, these rates are shown
  to be slower than polynomial rates owing to an inherent interaction between the
  softmax gating and expert functions via partial differential equations. To address
  this issue, we propose using a novel class of modified softmax gating functions
  which transform the input before delivering them to the gating functions. As a result,
  the previous interaction disappears and the parameter estimation rates are significantly
  improved.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: nguyen24b
month: 0
tex_title: A General Theory for Softmax Gating Multinomial Logistic Mixture of Experts
firstpage: 37617
lastpage: 37648
page: 37617-37648
order: 37617
cycles: false
bibtex_author: Nguyen, Huy and Akbarian, Pedram and Nguyen, Trungtin and Ho, Nhat
author:
- given: Huy
  family: Nguyen
- given: Pedram
  family: Akbarian
- given: Trungtin
  family: Nguyen
- given: Nhat
  family: Ho
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/nguyen24b/nguyen24b.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
