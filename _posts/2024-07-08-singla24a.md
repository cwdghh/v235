---
title: 'SAPG: Split and Aggregate Policy Gradients'
openreview: 4dOJAfXhNV
abstract: Despite extreme sample inefficiency, on-policy reinforcement learning, aka
  policy gradients, has become a fundamental tool in decision-making problems. With
  the recent advances in GPU-driven simulation, the ability to collect large amounts
  of data for RL training has scaled exponentially. However, we show that current
  RL methods, e.g. PPO, fail to ingest the benefit of parallelized environments beyond
  a certain point and their performance saturates. To address this, we propose a new
  on-policy RL algorithm that can effectively leverage large-scale environments by
  splitting them into chunks and fusing them back together via importance sampling.
  Our algorithm, termed SAPG, shows significantly higher performance across a variety
  of challenging environments where vanilla PPO and other strong baselines fail to
  achieve high performance. Webpage at https://sapg-rl.github.io/.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: singla24a
month: 0
tex_title: "{SAPG}: Split and Aggregate Policy Gradients"
firstpage: 45759
lastpage: 45772
page: 45759-45772
order: 45759
cycles: false
bibtex_author: Singla, Jayesh and Agarwal, Ananye and Pathak, Deepak
author:
- given: Jayesh
  family: Singla
- given: Ananye
  family: Agarwal
- given: Deepak
  family: Pathak
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/singla24a/singla24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
