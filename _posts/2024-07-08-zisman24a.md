---
title: Emergence of In-Context Reinforcement Learning from Noise Distillation
openreview: Y8KsHT1kTV
abstract: Recently, extensive studies in Reinforcement Learning have been carried
  out on the ability of transformers to adapt in-context to various environments and
  tasks. Current in-context RL methods are limited by their strict requirements for
  data, which needs to be generated by RL agents or labeled with actions from an optimal
  policy. In order to address this prevalent problem, we propose AD$^\varepsilon$,
  a new data acquisition approach that enables in-context Reinforcement Learning from
  noise-induced curriculum. We show that it is viable to construct a synthetic noise
  injection curriculum which helps to obtain learning histories. Moreover, we experimentally
  demonstrate that it is possible to alleviate the need for generation using optimal
  policies, with in-context RL still able to outperform the best suboptimal policy
  in a learning dataset by a 2x margin.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: zisman24a
month: 0
tex_title: Emergence of In-Context Reinforcement Learning from Noise Distillation
firstpage: 62832
lastpage: 62846
page: 62832-62846
order: 62832
cycles: false
bibtex_author: Zisman, Ilya and Kurenkov, Vladislav and Nikulin, Alexander and Sinii,
  Viacheslav and Kolesnikov, Sergey
author:
- given: Ilya
  family: Zisman
- given: Vladislav
  family: Kurenkov
- given: Alexander
  family: Nikulin
- given: Viacheslav
  family: Sinii
- given: Sergey
  family: Kolesnikov
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/assets/zisman24a/zisman24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
