---
title: Better Locally Private Sparse Estimation Given Multiple Samples Per User
openreview: wlBtHP8KqS
abstract: Previous studies yielded discouraging results for item-level locally differentially
  private linear regression with $s$-sparsity assumption, where the minimax rate for
  $nm$ samples is $\mathcal{O}(sd / nm\varepsilon^2)$. This can be challenging for
  high-dimensional data, where the dimension $d$ is extremely large. In this work,
  we investigate user-level locally differentially private sparse linear regression.
  We show that with $n$ users each contributing $m$ samples, the linear dependency
  of dimension $d$ can be eliminated, yielding an error upper bound of $\mathcal{O}(s/
  nm\varepsilon^2)$. We propose a framework that first selects candidate variables
  and then conducts estimation in the narrowed low-dimensional space, which is extendable
  to general sparse estimation problems with tight error bounds. Experiments on both
  synthetic and real datasets demonstrate the superiority of the proposed methods.
  Both the theoretical and empirical results suggest that, with the same number of
  samples, locally private sparse estimation is better conducted when multiple samples
  per user are available.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: ma24c
month: 0
tex_title: Better Locally Private Sparse Estimation Given Multiple Samples Per User
firstpage: 33746
lastpage: 33776
page: 33746-33776
order: 33746
cycles: false
bibtex_author: Ma, Yuheng and Jia, Ke and Yang, Hanfang
author:
- given: Yuheng
  family: Ma
- given: Ke
  family: Jia
- given: Hanfang
  family: Yang
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/ma24c/ma24c.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
