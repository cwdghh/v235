---
title: Speech Self-Supervised Learning Using Diffusion Model Synthetic Data
openreview: ecnpYYHjt9
abstract: While self-supervised learning (SSL) in speech has greatly reduced the reliance
  of speech processing systems on annotated corpora, the success of SSL still hinges
  on the availability of a large-scale unannotated corpus, which is still often impractical
  for many low-resource languages or under privacy concerns. Some existing work seeks
  to alleviate the problem by data augmentation, but most works are confined to introducing
  perturbations to real speech and do not introduce new variations in speech prosody,
  speakers, and speech content, which are important for SSL. Motivated by the recent
  finding that diffusion models have superior capabilities for modeling data distributions,
  we propose DiffS4L, a pretraining scheme that augments the limited unannotated data
  with synthetic data with different levels of variations, generated by a diffusion
  model trained on the limited unannotated data. Finally, an SSL model is pre-trained
  on the real and the synthetic speech. Our experiments show that DiffS4L can significantly
  improve the performance of SSL models, such as reducing the WER of the HuBERT pretrained
  model by 6.26 percentage points in the English ASR task. Notably, we find that the
  synthetic speech with all levels of variations, i.e. new prosody, new speakers,
  and even new content (despite the new content being mostly babble), accounts for
  significant performance improvement. The code is available at github.com/Hertin/DiffS4L.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: gao24j
month: 0
tex_title: Speech Self-Supervised Learning Using Diffusion Model Synthetic Data
firstpage: 14790
lastpage: 14810
page: 14790-14810
order: 14790
cycles: false
bibtex_author: Gao, Heting and Qian, Kaizhi and Ni, Junrui and Gan, Chuang and Hasegawa-Johnson,
  Mark A. and Chang, Shiyu and Zhang, Yang
author:
- given: Heting
  family: Gao
- given: Kaizhi
  family: Qian
- given: Junrui
  family: Ni
- given: Chuang
  family: Gan
- given: Mark A.
  family: Hasegawa-Johnson
- given: Shiyu
  family: Chang
- given: Yang
  family: Zhang
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/assets/gao24j/gao24j.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
