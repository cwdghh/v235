---
title: 'Case-Based or Rule-Based: How Do Transformers Do the Math?'
openreview: 4Vqr8SRfyX
abstract: Despite the impressive performance in a variety of complex tasks, modern
  large language models (LLMs) still have trouble dealing with some math problems
  that are simple and intuitive for humans, such as addition. While we can easily
  learn basic <em>rules</em> of addition and apply them to new problems of any length,
  LLMs struggle to do the same. Instead, they may rely on similar <em>cases</em> seen
  in the training corpus for help. We define these two different reasoning mechanisms
  as "<em>rule-based reasoning</em>" and "<em>case-based reasoning</em>". Since rule-based
  reasoning is essential for acquiring systematic generalization ability, we aim to
  explore exactly whether transformers use rule-based or case-based reasoning for
  math problems. Through carefully designed intervention experiments on five math
  tasks, we confirm that transformers are performing case-based reasoning, no matter
  whether scratchpad is used, which aligns with the previous observations that transformers
  use subgraph matching/shortcut learning to reason. To mitigate such problems, we
  propose a Rule-Following Fine-Tuning (RFFT) technique to teach transformers to perform
  rule-based reasoning. Specifically, we provide explicit rules in the input and then
  instruct transformers to recite and follow the rules step by step. Through RFFT,
  we successfully enable LLMs fine-tuned on 1-5 digit addition to generalize to up
  to 12-digit addition with over 95% accuracy, which is over 40% higher than scratchpad.
  The significant improvement demonstrates that teaching LLMs to use rules explicitly
  helps them learn rule-based reasoning and generalize better in length. Code is available
  at https://github.com/GraphPKU/Case_or_Rule.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: hu24n
month: 0
tex_title: 'Case-Based or Rule-Based: How Do Transformers Do the Math?'
firstpage: 19438
lastpage: 19474
page: 19438-19474
order: 19438
cycles: false
bibtex_author: Hu, Yi and Tang, Xiaojuan and Yang, Haotong and Zhang, Muhan
author:
- given: Yi
  family: Hu
- given: Xiaojuan
  family: Tang
- given: Haotong
  family: Yang
- given: Muhan
  family: Zhang
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/assets/hu24n/hu24n.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
