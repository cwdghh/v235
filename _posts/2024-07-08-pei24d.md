---
title: Data-free Neural Representation Compression with Riemannian Neural Dynamics
openreview: LTifAl5bKb
abstract: Neural models are equivalent to dynamic systems from a physics-inspired
  view, implying that computation on neural networks can be interpreted as the dynamical
  interactions between neurons. However, existing work models neuronal interaction
  as a weight-based linear transformation, and the nonlinearity comes from the nonlinear
  activation functions, which leads to limited nonlinearity and data-fitting ability
  of the whole neural model. Inspired by Riemannian geometry, we interpret neural
  structures by projecting neurons onto the Riemannian neuronal state space and model
  neuronal interaction with Riemannian metric (${\it RieM}$), which provides a more
  efficient neural representation with higher parameter efficiency. With ${\it RieM}$,
  we further design a novel data-free neural compression mechanism that does not require
  additional fine-tuning with real data. Using backbones like ResNet and Vision Transformer,
  we conduct extensive experiments on datasets such as MNIST, CIFAR-100, ImageNet-1k,
  and COCO object detection. Empirical results show that, under equal compression
  rates and computational complexity, models compressed with ${\it RieM}$ achieve
  superior inference accuracy compared to existing data-free compression methods.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: pei24d
month: 0
tex_title: Data-free Neural Representation Compression with {R}iemannian Neural Dynamics
firstpage: 40129
lastpage: 40144
page: 40129-40144
order: 40129
cycles: false
bibtex_author: Pei, Zhengqi and Zhang, Anran and Wang, Shuhui and Ji, Xiangyang and
  Huang, Qingming
author:
- given: Zhengqi
  family: Pei
- given: Anran
  family: Zhang
- given: Shuhui
  family: Wang
- given: Xiangyang
  family: Ji
- given: Qingming
  family: Huang
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://raw.githubusercontent.com/mlresearch/v235/main/assets/pei24d/pei24d.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
