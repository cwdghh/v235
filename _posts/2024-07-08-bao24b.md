---
title: Self-attention Networks Localize When QK-eigenspectrum Concentrates
openreview: aRZjRj41WQ
abstract: The self-attention mechanism prevails in modern machine learning. It has
  an interesting functionality of adaptively selecting tokens from an input sequence
  by modulating the degree of attention localization, which many researchers speculate
  is the basis of the powerful model performance but complicates the underlying mechanism
  of the learning dynamics. In recent years, mainly two arguments have connected attention
  localization to the model performances. One is the rank collapse, where the embedded
  tokens by a self-attention block become very similar across different tokens, leading
  to a less expressive network. The other is the entropy collapse, where the attention
  probability approaches non-uniform and entails low entropy, making the learning
  dynamics more likely to be trapped in plateaus. These two failure modes may apparently
  contradict each other because the rank and entropy collapses are relevant to uniform
  and non-uniform attention, respectively. To this end, we characterize the notion
  of attention localization by the eigenspectrum of query-key parameter matrices and
  reveal that a small eigenspectrum variance leads attention to be localized. Interestingly,
  the small eigenspectrum variance prevents both rank and entropy collapse, leading
  to better model expressivity and trainability.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: bao24b
month: 0
tex_title: Self-attention Networks Localize When {QK}-eigenspectrum Concentrates
firstpage: 2903
lastpage: 2922
page: 2903-2922
order: 2903
cycles: false
bibtex_author: Bao, Han and Hataya, Ryuichiro and Karakida, Ryo
author:
- given: Han
  family: Bao
- given: Ryuichiro
  family: Hataya
- given: Ryo
  family: Karakida
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/bao24b/bao24b.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
