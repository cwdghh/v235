---
title: 'APT: Adaptive Pruning and Tuning Pretrained Language Models for Efficient
  Training and Inference'
openreview: sb81Xl50JG
abstract: Fine-tuning and inference with large Language Models (LM) are generally
  known to be expensive. Parameter-efficient fine-tuning over pretrained LMs reduces
  training memory by updating a small number of LM parameters but does not improve
  inference efficiency. Structured pruning improves LM inference efficiency by removing
  consistent parameter blocks, yet often increases training memory and time. To improve
  both training and inference efficiency, we introduce APT that adaptively <em>prunes</em>
  and <em>tunes</em> parameters for the LMs. At the early stage of fine-tuning, APT
  dynamically adds <em>salient</em> tuning parameters for fast and accurate convergence
  while discarding unimportant parameters for efficiency. Compared to baselines, our
  experiments show that APT maintains up to 98% task performance when pruning RoBERTa
  and T5 models with 40% parameters left while keeping 86.4% LLaMA models’ performance
  with 70% parameters remaining. Furthermore, APT speeds up LMs’ fine-tuning by up
  to 8$\times$ and reduces large LMs’ memory training footprint by up to 70%. Our
  code and models are publicly available at https://github.com/ROIM1998/APT.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: zhao24g
month: 0
tex_title: "{APT}: Adaptive Pruning and Tuning Pretrained Language Models for Efficient
  Training and Inference"
firstpage: 60812
lastpage: 60831
page: 60812-60831
order: 60812
cycles: false
bibtex_author: Zhao, Bowen and Hajishirzi, Hannaneh and Cao, Qingqing
author:
- given: Bowen
  family: Zhao
- given: Hannaneh
  family: Hajishirzi
- given: Qingqing
  family: Cao
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/zhao24g/zhao24g.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
