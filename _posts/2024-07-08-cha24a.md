---
title: Regularizing with Pseudo-Negatives for Continual Self-Supervised Learning
openreview: 9jXS07TIBH
abstract: We introduce a novel Pseudo-Negative Regularization (PNR) framework for
  effective continual self-supervised learning (CSSL). Our PNR leverages pseudo-negatives
  obtained through model-based augmentation in a way that newly learned representations
  may not contradict what has been learned in the past. Specifically, for the InfoNCE-based
  contrastive learning methods, we define symmetric pseudo-negatives obtained from
  current and previous models and use them in both main and regularization loss terms.
  Furthermore, we extend this idea to non-contrastive learning methods which do not
  inherently rely on negatives. For these methods, a pseudo-negative is defined as
  the output from the previous model for a differently augmented version of the anchor
  sample and is asymmetrically applied to the regularization term. Extensive experimental
  results demonstrate that our PNR framework achieves state-of-the-art performance
  in representation learning during CSSL by effectively balancing the trade-off between
  plasticity and stability.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: cha24a
month: 0
tex_title: Regularizing with Pseudo-Negatives for Continual Self-Supervised Learning
firstpage: 6048
lastpage: 6065
page: 6048-6065
order: 6048
cycles: false
bibtex_author: Cha, Sungmin and Cho, Kyunghyun and Moon, Taesup
author:
- given: Sungmin
  family: Cha
- given: Kyunghyun
  family: Cho
- given: Taesup
  family: Moon
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/assets/cha24a/cha24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
