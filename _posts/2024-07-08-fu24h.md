---
title: Breaking through the learning plateaus of in-context learning in Transformer
openreview: 2K87GFLYWz
abstract: In-context learning, i.e., learning from context examples, is an impressive
  ability of Transformer. Training Transformers to possess this in-context learning
  skill is computationally intensive due to the occurrence of <em>learning plateaus</em>,
  which are periods within the training process where there is minimal or no enhancement
  in the model’s in-context learning capability. To study the mechanism behind the
  learning plateaus, we conceptually separate a component within the model’s internal
  representation that is exclusively affected by the model’s weights. We call this
  the “weights component”, and the remainder is identified as the “context component”.
  By conducting meticulous and controlled experiments on synthetic tasks, we note
  that the persistence of learning plateaus correlates with compromised functionality
  of the weights component. Recognizing the impaired performance of the weights component
  as a fundamental behavior that drives learning plateaus, we have developed three
  strategies to expedite the learning of Transformers. The effectiveness of these
  strategies is further confirmed in natural language processing tasks. In conclusion,
  our research demonstrates the feasibility of cultivating a powerful in-context learning
  ability within AI systems in an eco-friendly manner.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: fu24h
month: 0
tex_title: Breaking through the learning plateaus of in-context learning in Transformer
firstpage: 14207
lastpage: 14227
page: 14207-14227
order: 14207
cycles: false
bibtex_author: Fu, Jingwen and Yang, Tao and Wang, Yuwang and Lu, Yan and Zheng, Nanning
author:
- given: Jingwen
  family: Fu
- given: Tao
  family: Yang
- given: Yuwang
  family: Wang
- given: Yan
  family: Lu
- given: Nanning
  family: Zheng
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://raw.githubusercontent.com/mlresearch/v235/main/assets/fu24h/fu24h.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
