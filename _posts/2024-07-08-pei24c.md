---
title: Modeling Language Tokens as Functionals of Semantic Fields
openreview: EEO4Iktfjp
abstract: Recent advances in natural language processing have relied heavily on using
  Transformer-based language models. However, Transformers often require large parameter
  sizes and model depth. Existing Transformer-free approaches using state-space models
  demonstrate superiority over Transformers, yet they still lack a neuro-biologically
  connection to the human brain. This paper proposes ${\it LasF}$, representing ${\bf
  L}$anguage tokens ${\bf as}$ ${\bf F}$unctionals of semantic fields, to simulate
  the neuronal behaviors for better language modeling. The ${\it LasF}$ module is
  equivalent to a nonlinear approximator tailored for sequential data. By replacing
  the final layers of pre-trained language models with the ${\it LasF}$ module, we
  obtain ${\it LasF}$-based models. Experiments conducted for standard reading comprehension
  and question-answering tasks demonstrate that the ${\it LasF}$-based models consistently
  improve accuracy with fewer parameters. Besides, we use CommonsenseQAâ€™s blind test
  set to evaluate a full-parameter tuned ${\it LasF}$-based model, which outperforms
  the prior best ensemble and single models by $0.4%$ and $3.1%$, respectively. Furthermore,
  our ${\it LasF}$-only language model trained from scratch outperforms existing parameter-efficient
  language models on standard datasets such as WikiText103 and PennTreebank.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: pei24c
month: 0
tex_title: Modeling Language Tokens as Functionals of Semantic Fields
firstpage: 40114
lastpage: 40128
page: 40114-40128
order: 40114
cycles: false
bibtex_author: Pei, Zhengqi and Zhang, Anran and Wang, Shuhui and Huang, Qingming
author:
- given: Zhengqi
  family: Pei
- given: Anran
  family: Zhang
- given: Shuhui
  family: Wang
- given: Qingming
  family: Huang
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://raw.githubusercontent.com/mlresearch/v235/main/assets/pei24c/pei24c.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
