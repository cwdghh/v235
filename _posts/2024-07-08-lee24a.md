---
title: 'A Mechanistic Understanding of Alignment Algorithms: A Case Study on DPO and
  Toxicity'
openreview: dBqHGZPGZI
abstract: While alignment algorithms are commonly used to tune pre-trained language
  models towards user preferences, we lack explanations for the underlying mechanisms
  in which models become “aligned”, thus making it difficult to explain phenomena
  like jailbreaks. In this work we study a popular algorithm, direct preference optimization
  (DPO), and the mechanisms by which it reduces toxicity. Namely, we first study how
  toxicity is represented and elicited in pre-trained language models (GPT2-medium,
  Llama2-7b). We then apply DPO with a carefully crafted pairwise dataset to reduce
  toxicity. We examine how the resulting models avert toxic outputs, and find that
  capabilities learned from pre-training are not removed, but rather bypassed. We
  use this insight to demonstrate a simple method to un-align the models, reverting
  them back to their toxic behavior.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: lee24a
month: 0
tex_title: 'A Mechanistic Understanding of Alignment Algorithms: A Case Study on {DPO}
  and Toxicity'
firstpage: 26361
lastpage: 26378
page: 26361-26378
order: 26361
cycles: false
bibtex_author: Lee, Andrew and Bai, Xiaoyan and Pres, Itamar and Wattenberg, Martin
  and Kummerfeld, Jonathan K. and Mihalcea, Rada
author:
- given: Andrew
  family: Lee
- given: Xiaoyan
  family: Bai
- given: Itamar
  family: Pres
- given: Martin
  family: Wattenberg
- given: Jonathan K.
  family: Kummerfeld
- given: Rada
  family: Mihalcea
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/lee24a/lee24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
