---
title: Fewer Truncations Improve Language Modeling
openreview: kRxCDDFNpp
abstract: In large language model training, input documents are typically concatenated
  together and then split into sequences of equal length to avoid padding tokens.
  Despite its efficiency, the concatenation approach compromises data integrityâ€”it
  inevitably breaks many documents into incomplete pieces, leading to excessive truncations
  that hinder the model from learning to compose logically coherent and factually
  consistent content that is grounded on the complete context. To address the issue,
  we propose Best-fit Packing, a scalable and efficient method that packs documents
  into training sequences through length-aware combinatorial optimization. Our method
  completely eliminates unnecessary truncations while retaining the same training
  efficiency as concatenation. Empirical results from both text and code pre-training
  show that our method achieves superior performance (e.g., +4.7% on reading comprehension;
  +16.8% in context following; and +9.2% on program synthesis), and reduces closed-domain
  hallucination effectively by up to 58.3%.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: ding24f
month: 0
tex_title: Fewer Truncations Improve Language Modeling
firstpage: 11030
lastpage: 11048
page: 11030-11048
order: 11030
cycles: false
bibtex_author: Ding, Hantian and Wang, Zijian and Paolini, Giovanni and Kumar, Varun
  and Deoras, Anoop and Roth, Dan and Soatto, Stefano
author:
- given: Hantian
  family: Ding
- given: Zijian
  family: Wang
- given: Giovanni
  family: Paolini
- given: Varun
  family: Kumar
- given: Anoop
  family: Deoras
- given: Dan
  family: Roth
- given: Stefano
  family: Soatto
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/ding24f/ding24f.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
