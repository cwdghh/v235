---
title: 'Don’t Label Twice: Quantity Beats Quality when Comparing Binary Classifiers
  on a Budget'
openreview: zkcya47Sq5
abstract: We study how to best spend a budget of noisy labels to compare the accuracy
  of two binary classifiers. It’s common practice to collect and aggregate multiple
  noisy labels for a given data point into a less noisy label via a majority vote.
  We prove a theorem that runs counter to conventional wisdom. If the goal is to identify
  the better of two classifiers, we show it’s best to spend the budget on collecting
  a single label for more samples. Our result follows from a non-trivial application
  of Cramér’s theorem, a staple in the theory of large deviations. We discuss the
  implications of our work for the design of machine learning benchmarks, where they
  overturn some time-honored recommendations. In addition, our results provide sample
  size bounds superior to what follows from Hoeffding’s bound.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: dorner24a
month: 0
tex_title: 'Don’t Label Twice: Quantity Beats Quality when Comparing Binary Classifiers
  on a Budget'
firstpage: 11544
lastpage: 11572
page: 11544-11572
order: 11544
cycles: false
bibtex_author: Dorner, Florian E. and Hardt, Moritz
author:
- given: Florian E.
  family: Dorner
- given: Moritz
  family: Hardt
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/dorner24a/dorner24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
