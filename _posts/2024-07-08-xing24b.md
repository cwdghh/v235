---
title: 'Less is More: on the Over-Globalizing Problem in Graph Transformers'
openreview: uKmcyyrZae
abstract: 'Graph Transformer, due to its global attention mechanism, has emerged as
  a new tool in dealing with graph-structured data. It is well recognized that the
  global attention mechanism considers a wider receptive field in a fully connected
  graph, leading many to believe that useful information can be extracted from all
  the nodes. In this paper, we challenge this belief: does the globalizing property
  always benefit Graph Transformers? We reveal the over-globalizing problem in Graph
  Transformer by presenting both empirical evidence and theoretical analysis, i.e.,
  the current attention mechanism overly focuses on those distant nodes, while the
  near nodes, which actually contain most of the useful information, are relatively
  weakened. Then we propose a novel Bi-Level Global Graph Transformer with Collaborative
  Training (CoBFormer), including the inter-cluster and intra-cluster Transformers,
  to prevent the over-globalizing problem while keeping the ability to extract valuable
  information from distant nodes. Moreover, the collaborative training is proposed
  to improve the modelâ€™s generalization ability with a theoretical guarantee. Extensive
  experiments on various graphs well validate the effectiveness of our proposed CoBFormer.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: xing24b
month: 0
tex_title: 'Less is More: on the Over-Globalizing Problem in Graph Transformers'
firstpage: 54656
lastpage: 54672
page: 54656-54672
order: 54656
cycles: false
bibtex_author: Xing, Yujie and Wang, Xiao and Li, Yibo and Huang, Hai and Shi, Chuan
author:
- given: Yujie
  family: Xing
- given: Xiao
  family: Wang
- given: Yibo
  family: Li
- given: Hai
  family: Huang
- given: Chuan
  family: Shi
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/xing24b/xing24b.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
