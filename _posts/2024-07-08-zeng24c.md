---
title: Token-level Direct Preference Optimization
openreview: 1RZKuvqYCR
abstract: Fine-tuning pre-trained Large Language Models (LLMs) is essential to align
  them with human values and intentions. This process often utilizes methods like
  pairwise comparisons and KL divergence against a reference LLM, focusing on the
  evaluation of full answers generated by the models. However, the generation of these
  responses occurs in a token level, following a sequential, auto-regressive fashion.
  In this paper, we introduce Token-level Direct Preference Optimization (TDPO), a
  novel approach to align LLMs with human preferences by optimizing policy at the
  token level. Unlike previous methods, which face challenges in divergence efficiency,
  TDPO integrates forward KL divergence constraints for each token, improving alignment
  and diversity. Utilizing the Bradley-Terry model for a token-based reward system,
  our method enhances the regulation of KL divergence, while preserving simplicity
  without the need for explicit reward modeling. Experimental results across various
  text tasks demonstrate TDPOâ€™s superior performance in balancing alignment with generation
  diversity. Notably, fine-tuning with TDPO strikes a better balance than DPO in the
  controlled sentiment generation and single-turn dialogue datasets, and significantly
  improves the quality of generated responses compared to both DPO and PPO-based RLHF
  methods.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: zeng24c
month: 0
tex_title: Token-level Direct Preference Optimization
firstpage: 58348
lastpage: 58365
page: 58348-58365
order: 58348
cycles: false
bibtex_author: Zeng, Yongcheng and Liu, Guoqing and Ma, Weiyu and Yang, Ning and Zhang,
  Haifeng and Wang, Jun
author:
- given: Yongcheng
  family: Zeng
- given: Guoqing
  family: Liu
- given: Weiyu
  family: Ma
- given: Ning
  family: Yang
- given: Haifeng
  family: Zhang
- given: Jun
  family: Wang
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://raw.githubusercontent.com/mlresearch/v235/main/assets/zeng24c/zeng24c.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
