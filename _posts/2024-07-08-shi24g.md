---
title: 'Long-Tail Learning with Foundation Model: Heavy Fine-Tuning Hurts'
openreview: ccSSKTz9LX
abstract: The fine-tuning paradigm in addressing long-tail learning tasks has sparked
  significant interest since the emergence of foundation models. Nonetheless, how
  fine-tuning impacts performance in long-tail learning was not explicitly quantified.
  In this paper, we disclose that heavy fine-tuning may even lead to non-negligible
  performance deterioration on tail classes, and lightweight fine-tuning is more effective.
  The reason is attributed to inconsistent class conditions caused by heavy fine-tuning.
  With the observation above, we develop a low-complexity and accurate long-tail learning
  algorithms LIFT with the goal of facilitating fast prediction and compact models
  by adaptive lightweight fine-tuning. Experiments clearly verify that both the training
  time and the learned parameters are significantly reduced with more accurate predictive
  performance compared with state-of-the-art approaches. The implementation code is
  available at https://github.com/shijxcs/LIFT.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: shi24g
month: 0
tex_title: 'Long-Tail Learning with Foundation Model: Heavy Fine-Tuning Hurts'
firstpage: 45014
lastpage: 45039
page: 45014-45039
order: 45014
cycles: false
bibtex_author: Shi, Jiang-Xin and Wei, Tong and Zhou, Zhi and Shao, Jie-Jing and Han,
  Xin-Yan and Li, Yu-Feng
author:
- given: Jiang-Xin
  family: Shi
- given: Tong
  family: Wei
- given: Zhi
  family: Zhou
- given: Jie-Jing
  family: Shao
- given: Xin-Yan
  family: Han
- given: Yu-Feng
  family: Li
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/assets/shi24g/shi24g.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
