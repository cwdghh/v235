---
title: 'DPZero: Private Fine-Tuning of Language Models without Backpropagation'
openreview: QJkG8Mln72
abstract: The widespread practice of fine-tuning large language models (LLMs) on domain-specific
  data faces two major challenges in memory and privacy. First, as the size of LLMs
  continues to grow, the memory demands of gradient-based training methods via backpropagation
  become prohibitively high. Second, given the tendency of LLMs to memorize training
  data, it is important to protect potentially sensitive information in the fine-tuning
  data from being regurgitated. Zeroth-order methods, which rely solely on forward
  passes, substantially reduce memory consumption during training. However, directly
  combining them with standard differentially private gradient descent suffers more
  as model size grows. To bridge this gap, we introduce DPZero, a novel private zeroth-order
  algorithm with nearly dimension-independent rates. The memory efficiency of DPZero
  is demonstrated in privately fine-tuning RoBERTa and OPT on several downstream tasks.
  Our code is available at https://github.com/Liang137/DPZero.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: zhang24af
month: 0
tex_title: "{DPZ}ero: Private Fine-Tuning of Language Models without Backpropagation"
firstpage: 59210
lastpage: 59246
page: 59210-59246
order: 59210
cycles: false
bibtex_author: Zhang, Liang and Li, Bingcong and Thekumparampil, Kiran Koshy and Oh,
  Sewoong and He, Niao
author:
- given: Liang
  family: Zhang
- given: Bingcong
  family: Li
- given: Kiran Koshy
  family: Thekumparampil
- given: Sewoong
  family: Oh
- given: Niao
  family: He
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/zhang24af/zhang24af.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
