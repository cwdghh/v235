---
title: 'Flora: Low-Rank Adapters Are Secretly Gradient Compressors'
openreview: uubBZKM99Y
abstract: Despite large neural networks demonstrating remarkable abilities to complete
  different tasks, they require excessive memory usage to store the optimization states
  for training. To alleviate this, the low-rank adaptation (LoRA) is proposed to reduce
  the optimization states by training fewer parameters. However, LoRA restricts overall
  weight update matrices to be low-rank, limiting the model performance. In this work,
  we investigate the dynamics of LoRA and identify that it can be approximated by
  a random projection. Based on this observation, we propose Flora, which is able
  to achieve high-rank updates by resampling the projection matrices while enjoying
  the sublinear space complexity of optimization states. We conduct experiments across
  different tasks and model architectures to verify the effectiveness of our approach.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: hao24a
month: 0
tex_title: 'Flora: Low-Rank Adapters Are Secretly Gradient Compressors'
firstpage: 17554
lastpage: 17571
page: 17554-17571
order: 17554
cycles: false
bibtex_author: Hao, Yongchang and Cao, Yanshuai and Mou, Lili
author:
- given: Yongchang
  family: Hao
- given: Yanshuai
  family: Cao
- given: Lili
  family: Mou
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://raw.githubusercontent.com/mlresearch/v235/main/assets/hao24a/hao24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
