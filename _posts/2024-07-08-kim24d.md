---
title: 'CARTE: Pretraining and Transfer for Tabular Learning'
openreview: 9kArQnKLDp
abstract: 'Pretrained deep-learning models are the go-to solution for images or text.
  However, for tabular data the standard is still to train tree-based models. Indeed,
  transfer learning on tables hits the challenge of <em>data integration</em>: finding
  correspondences, correspondences in the entries (<em>entity matching</em>) where
  different words may denote the same entity, correspondences across columns (<em>schema
  matching</em>), which may come in different orders, names... We propose a neural
  architecture that does not need such correspondences. As a result, we can pretrain
  it on background data that has not been matched. The architecture –CARTE for Context
  Aware Representation of Table Entries– uses a graph representation of tabular (or
  relational) data to process tables with different columns, string embedding of entries
  and columns names to model an open vocabulary, and a graph-attentional network to
  contextualize entries with column names and neighboring entries. An extensive benchmark
  shows that CARTE facilitates learning, outperforming a solid set of baselines including
  the best tree-based models. CARTE also enables joint learning across tables with
  unmatched columns, enhancing a small table with bigger ones. CARTE opens the door
  to large pretrained models for tabular data.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: kim24d
month: 0
tex_title: "{CARTE}: Pretraining and Transfer for Tabular Learning"
firstpage: 23843
lastpage: 23866
page: 23843-23866
order: 23843
cycles: false
bibtex_author: Kim, Myung Jun and Grinsztajn, Leo and Varoquaux, Gael
author:
- given: Myung Jun
  family: Kim
- given: Leo
  family: Grinsztajn
- given: Gael
  family: Varoquaux
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/assets/kim24d/kim24d.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
