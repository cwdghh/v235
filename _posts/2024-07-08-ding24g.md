---
title: Delving into Differentially Private Transformer
openreview: FzyMdAm2fZ
abstract: 'Deep learning with differential privacy (DP) has garnered significant attention
  over the past years, leading to the development of numerous methods aimed at enhancing
  model accuracy and training efficiency. This paper delves into the problem of training
  Transformer models with differential privacy. Our treatment is modular: the logic
  is to ’reduce’ the problem of training DP Transformer to the more basic problem
  of training DP vanilla neural nets. The latter is better understood and amenable
  to many model-agnostic methods. Such ’reduction’ is done by first identifying the
  hardness unique to DP Transformer training: the attention distraction phenomenon
  and a lack of compatibility with existing techniques for efficient gradient clipping.
  To deal with these two issues, we propose the Re-Attention Mechanism and Phantom
  Clipping, respectively. We believe that our work not only casts new light on training
  DP Transformers but also promotes a modular treatment to advance research in the
  field of differentially private deep learning.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: ding24g
month: 0
tex_title: Delving into Differentially Private Transformer
firstpage: 11049
lastpage: 11071
page: 11049-11071
order: 11049
cycles: false
bibtex_author: Ding, Youlong and Wu, Xueyang and Meng, Yining and Luo, Yonggang and
  Wang, Hao and Pan, Weike
author:
- given: Youlong
  family: Ding
- given: Xueyang
  family: Wu
- given: Yining
  family: Meng
- given: Yonggang
  family: Luo
- given: Hao
  family: Wang
- given: Weike
  family: Pan
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://raw.githubusercontent.com/mlresearch/v235/main/assets/ding24g/ding24g.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
