---
title: 'GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection'
openreview: hYHsrKDiX7
abstract: Training Large Language Models (LLMs) presents significant memory challenges,
  predominantly due to the growing size of weights and optimizer states. Common memory-reduction
  approaches, such as low-rank adaptation (LoRA), add a trainable low-rank matrix
  to the frozen pre-trained weight in each layer, reducing trainable parameters and
  optimizer states. However, such approaches typically underperform training with
  full-rank weights in both pre-training and fine-tuning stages since they limit the
  parameter search to a low-rank subspace and alter the training dynamics, and further,
  may require full-rank warm start. In this work, we propose Gradient Low-Rank Projection
  (GaLore), a training strategy that allows full-parameter learning but is more memory-efficient
  than common low-rank adaptation methods such as LoRA. Our approach reduces memory
  usage by up to 65.5% in optimizer states while maintaining both efficiency and performance
  for pre-training on LLaMA 1B and 7B architectures with C4 dataset with up to 19.7B
  tokens, and on fine-tuning RoBERTa on GLUE tasks. Our 8-bit GaLore further reduces
  optimizer memory by up to 82.5% and total training memory by 63.3%, compared to
  a BF16 baseline. Notably, we demonstrate, for the first time, the feasibility of
  pre-training a 7B model on consumer GPUs with 24GB memory (e.g., NVIDIA RTX 4090)
  without model parallel, checkpointing, or offloading strategies.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: zhao24s
month: 0
tex_title: "{G}a{L}ore: Memory-Efficient {LLM} Training by Gradient Low-Rank Projection"
firstpage: 61121
lastpage: 61143
page: 61121-61143
order: 61121
cycles: false
bibtex_author: Zhao, Jiawei and Zhang, Zhenyu and Chen, Beidi and Wang, Zhangyang
  and Anandkumar, Anima and Tian, Yuandong
author:
- given: Jiawei
  family: Zhao
- given: Zhenyu
  family: Zhang
- given: Beidi
  family: Chen
- given: Zhangyang
  family: Wang
- given: Anima
  family: Anandkumar
- given: Yuandong
  family: Tian
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://raw.githubusercontent.com/mlresearch/v235/main/assets/zhao24s/zhao24s.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
