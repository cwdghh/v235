---
title: Can Looped Transformers Learn to Implement Multi-step Gradient Descent for
  In-context Learning?
openreview: o8AaRKbP9K
abstract: Transformers to do reasoning and few-shot learning, without any fine-tuning,
  is widely conjectured to stem from their ability to implicitly simulate a multi-step
  algorithms – such as gradient descent – with their weights in a single forward pass.
  Recently, there has been progress in understanding this complex phenomenon from
  an expressivity point of view, by demonstrating that Transformers can express such
  multi-step algorithms. However, our knowledge about the more fundamental aspect
  of its learnability, beyond single layer models, is very limited. In particular,
  <em>can training Transformers enable convergence to algorithmic solutions</em>?
  In this work we resolve this for in context linear regression with linear looped
  Transformers – a multi-layer model with weight sharing that is conjectured to have
  an inductive bias to learn fix-point iterative algorithms. More specifically, for
  this setting we show that the global minimizer of the population training loss implements
  multi-step preconditioned gradient descent, with a preconditioner that adapts to
  the data distribution. Furthermore, we show a fast convergence for gradient flow
  on the regression loss, despite the non-convexity of the landscape, by proving a
  novel gradient dominance condition. To our knowledge, this is the first theoretical
  analysis for multi-layer Transformer in this setting. We further validate our theoretical
  findings through synthetic experiments.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: gatmiry24b
month: 0
tex_title: Can Looped Transformers Learn to Implement Multi-step Gradient Descent
  for In-context Learning?
firstpage: 15130
lastpage: 15152
page: 15130-15152
order: 15130
cycles: false
bibtex_author: Gatmiry, Khashayar and Saunshi, Nikunj and J. Reddi, Sashank and Jegelka,
  Stefanie and Kumar, Sanjiv
author:
- given: Khashayar
  family: Gatmiry
- given: Nikunj
  family: Saunshi
- given: Sashank
  family: J. Reddi
- given: Stefanie
  family: Jegelka
- given: Sanjiv
  family: Kumar
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/assets/gatmiry24b/gatmiry24b.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
