---
title: Mitigating Privacy Risk in Membership Inference by Convex-Concave Loss
openreview: tdomF3PW6A
abstract: Machine learning models are susceptible to membership inference attacks
  (MIAs), which aim to infer whether a sample is in the training set. Existing work
  utilizes gradient ascent to enlarge the loss variance of training data, alleviating
  the privacy risk. However, optimizing toward a reverse direction may cause the model
  parameters to oscillate near local minima, leading to instability and suboptimal
  performance. In this work, we propose a novel method â€“ Convex Concave Loss (CCL),
  which enables a high variance of training loss distribution by gradient descent.
  Our method is motivated by the theoretical analysis that convex losses tend to decrease
  the loss variance during training. Thus, our key idea behind CCL is to reduce the
  convexity of loss functions with a concave term. Trained with CCL, neural networks
  produce losses with high variance for training data, reinforcing the defense against
  MIAs. Extensive experiments demonstrate the superiority of CCL, achieving a state-of-the-art
  balance in the privacy-utility trade-off.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: liu24q
month: 0
tex_title: Mitigating Privacy Risk in Membership Inference by Convex-Concave Loss
firstpage: 30998
lastpage: 31014
page: 30998-31014
order: 30998
cycles: false
bibtex_author: Liu, Zhenlong and Feng, Lei and Zhuang, Huiping and Cao, Xiaofeng and
  Wei, Hongxin
author:
- given: Zhenlong
  family: Liu
- given: Lei
  family: Feng
- given: Huiping
  family: Zhuang
- given: Xiaofeng
  family: Cao
- given: Hongxin
  family: Wei
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://raw.githubusercontent.com/mlresearch/v235/main/assets/liu24q/liu24q.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
