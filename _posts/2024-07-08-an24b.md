---
title: Training-Free Long-Context Scaling of Large Language Models
openreview: If4xW9vF7U
abstract: The ability of Large Language Models (LLMs) to process and generate coherent
  text is markedly weakened when the number of input tokens exceeds their pretraining
  length. Given the expensive overhead of finetuning large-scale models with longer
  sequences, we propose a training-free approach named Dual Chunk Attention (DCA),
  which enables Llama2 70B to support context windows of up to 100k tokens. By decomposing
  the attention computation for long sequences into chunk-based modules, DCA manages
  to effectively capture the relative positional information of tokens within the
  same chunk (Intra-Chunk) and across distinct chunks (Inter-Chunk), as well as integrates
  seamlessly with Flash Attention. In addition to its impressive extrapolation capability,
  DCA achieves performance on practical long-context tasks that is comparable to or
  even better than that of models built through continual training. All code and data
  used in this work are released at https://github.com/HKUNLP/ChunkLlama.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: an24b
month: 0
tex_title: Training-Free Long-Context Scaling of Large Language Models
firstpage: 1493
lastpage: 1510
page: 1493-1510
order: 1493
cycles: false
bibtex_author: An, Chenxin and Huang, Fei and Zhang, Jun and Gong, Shansan and Qiu,
  Xipeng and Zhou, Chang and Kong, Lingpeng
author:
- given: Chenxin
  family: An
- given: Fei
  family: Huang
- given: Jun
  family: Zhang
- given: Shansan
  family: Gong
- given: Xipeng
  family: Qiu
- given: Chang
  family: Zhou
- given: Lingpeng
  family: Kong
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/assets/an24b/an24b.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
