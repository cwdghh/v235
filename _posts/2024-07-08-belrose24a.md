---
title: Neural Networks Learn Statistics of Increasing Complexity
openreview: IGdpKP0N6w
abstract: The <em>distributional simplicity bias</em> (DSB) posits that neural networks
  learn low-order moments of the data distribution first, before moving on to higher-order
  correlations. In this work, we present compelling new evidence for the DSB by showing
  that networks automatically learn to perform well on maximum-entropy distributions
  whose low-order statistics match those of the training set early in training, then
  lose this ability later. We also extend the DSB to discrete domains by proving an
  equivalence between token $n$-gram frequencies and the moments of embedding vectors,
  and by finding empirical evidence for the bias in LLMs. Finally we use optimal transport
  methods to surgically edit the low-order statistics of one class to match those
  of another, and show that early-training networks treat the edited samples as if
  they were drawn from the target class. Code is available at https://github.com/EleutherAI/features-across-time.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: belrose24a
month: 0
tex_title: Neural Networks Learn Statistics of Increasing Complexity
firstpage: 3382
lastpage: 3409
page: 3382-3409
order: 3382
cycles: false
bibtex_author: Belrose, Nora and Pope, Quintin and Quirke, Lucia and Mallen, Alex
  Troy and Fern, Xiaoli
author:
- given: Nora
  family: Belrose
- given: Quintin
  family: Pope
- given: Lucia
  family: Quirke
- given: Alex Troy
  family: Mallen
- given: Xiaoli
  family: Fern
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/belrose24a/belrose24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
