---
title: 'InstructZero: Efficient Instruction Optimization for Black-Box Large Language
  Models'
openreview: rADFNrIss3
abstract: Large language models (LLMs) are instruction followers but the performance
  varies under different instructions. It is challenging to create the best instruction,
  especially for black-box LLMs on which backpropagation is forbidden. Instead of
  directly optimizing the discrete instruction, we optimize a low-dimensional soft
  prompt applied to an open-source LLM to generate the instruction for the black-box
  LLM. In each optimization step of the proposed method InstructZero, a soft prompt
  is converted into an instruction by the open-source LLM, which is then submitted
  to the black-box LLM for zero-shot evaluation, whose result is sent to Bayesian
  optimization to produce new soft prompts improving the zero-shot performance. We
  evaluate InstructZero on different combinations of open-source LLMs and APIs including
  Vicuna and ChatGPT. InstructZero outperforms SOTA auto-instruction methods across
  a variety of downstream tasks.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: chen24e
month: 0
tex_title: "{I}nstruct{Z}ero: Efficient Instruction Optimization for Black-Box Large
  Language Models"
firstpage: 6503
lastpage: 6518
page: 6503-6518
order: 6503
cycles: false
bibtex_author: Chen, Lichang and Chen, Jiuhai and Goldstein, Tom and Huang, Heng and
  Zhou, Tianyi
author:
- given: Lichang
  family: Chen
- given: Jiuhai
  family: Chen
- given: Tom
  family: Goldstein
- given: Heng
  family: Huang
- given: Tianyi
  family: Zhou
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/assets/chen24e/chen24e.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
