---
title: Recovering the Pre-Fine-Tuning Weights of Generative Models
openreview: 761UxjOTHB
abstract: 'The dominant paradigm in generative modeling consists of two steps: i)
  pre-training on a large-scale but unsafe dataset, ii) aligning the pre-trained model
  with human values via fine-tuning. This practice is considered safe, as no current
  method can recover the unsafe, <em>pre-fine-tuning</em> model weights. In this paper,
  we demonstrate that this assumption is often false. Concretely, we present <em>Spectral
  DeTuning</em>, a method that can recover the weights of the pre-fine-tuning model
  using a few low-rank (LoRA) fine-tuned models. In contrast to previous attacks that
  attempt to recover pre-fine-tuning capabilities, our method aims to recover the
  exact pre-fine-tuning weights. Our approach exploits this new vulnerability against
  large-scale models such as a personalized Stable Diffusion and an aligned Mistral.
  The code is available at https://vision.huji.ac.il/spectral_detuning/.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: horwitz24a
month: 0
tex_title: Recovering the Pre-Fine-Tuning Weights of Generative Models
firstpage: 18882
lastpage: 18904
page: 18882-18904
order: 18882
cycles: false
bibtex_author: Horwitz, Eliahu and Kahana, Jonathan and Hoshen, Yedid
author:
- given: Eliahu
  family: Horwitz
- given: Jonathan
  family: Kahana
- given: Yedid
  family: Hoshen
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://raw.githubusercontent.com/mlresearch/v235/main/assets/horwitz24a/horwitz24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
