---
title: Soft Prompt Recovers Compressed LLMs, Transferably
openreview: muBJPCIqZT
abstract: Model compression is one of the most popular approaches to improve the accessibility
  of Large Language Models (LLMs) by reducing their memory footprint. However, the
  gaining of such efficiency benefits often simultaneously demands extensive engineering
  efforts and intricate designs to mitigate the performance decline. In this work,
  we leverage <em>(Soft) Prompt Tuning</em> in its most vanilla form and discover
  such conventionally learned soft prompts can recover the performance of compressed
  LLMs. More surprisingly, we observe such recovery effect to be transferable among
  different tasks and models (albeit natural tokenizer and dimensionality limitations),
  resulting in further overhead reduction and yet, subverting the common belief that
  learned soft prompts are task-specific. Our work is fully orthogonal and compatible
  with model compression frameworks such as pruning and quantization, where we enable
  up to $8\times$ compressed LLM (with a joint 4-bit quantization and 50% weight pruning
  compression) to match its uncompressed counterparts on popular benchmarks. We note
  that we are the first to reveal vanilla Parameter-Efficient Fine-Tuning (PEFT) techniques
  have the potential to be utilized under a compression recovery context, opening
  a new line of opportunities for model accessibility advancement while freeing our
  fellow researchers from the previously present engineering burdens and constraints.
  The code is available at https://github.com/zirui-ray-liu/compress-then-prompt.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: xu24s
month: 0
tex_title: Soft Prompt Recovers Compressed {LLM}s, Transferably
firstpage: 55186
lastpage: 55203
page: 55186-55203
order: 55186
cycles: false
bibtex_author: Xu, Zhaozhuo and Liu, Zirui and Chen, Beidi and Zhong, Shaochen and
  Tang, Yuxin and Wang, Jue and Zhou, Kaixiong and Hu, Xia and Shrivastava, Anshumali
author:
- given: Zhaozhuo
  family: Xu
- given: Zirui
  family: Liu
- given: Beidi
  family: Chen
- given: Shaochen
  family: Zhong
- given: Yuxin
  family: Tang
- given: Jue
  family: Wang
- given: Kaixiong
  family: Zhou
- given: Xia
  family: Hu
- given: Anshumali
  family: Shrivastava
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://raw.githubusercontent.com/mlresearch/v235/main/assets/xu24s/xu24s.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
