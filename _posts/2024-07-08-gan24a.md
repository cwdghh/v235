---
title: 'Erasing the Bias: Fine-Tuning Foundation Models for Semi-Supervised Learning'
openreview: f47ZK6gy3I
abstract: Semi-supervised learning (SSL) has witnessed remarkable progress, resulting
  in the emergence of numerous method variations. However, practitioners often encounter
  challenges when attempting to deploy these methods due to their subpar performance.
  In this paper, we present a novel SSL approach named FineSSL that significantly
  addresses this limitation by adapting pre-trained foundation models. We identify
  the aggregated biases and cognitive deviation problems inherent in foundation models,
  and propose a simple yet effective solution by imposing balanced margin softmax
  and decoupled label smoothing. Through extensive experiments, we demonstrate that
  FineSSL sets a new state of the art for SSL on multiple benchmark datasets, reduces
  the training cost by over six times, and can seamlessly integrate various fine-tuning
  and modern SSL algorithms. The source code is available at https://github.com/Gank0078/FineSSL.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: gan24a
month: 0
tex_title: 'Erasing the Bias: Fine-Tuning Foundation Models for Semi-Supervised Learning'
firstpage: 14453
lastpage: 14470
page: 14453-14470
order: 14453
cycles: false
bibtex_author: Gan, Kai and Wei, Tong
author:
- given: Kai
  family: Gan
- given: Tong
  family: Wei
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/assets/gan24a/gan24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
