---
title: 'Mobile Attention: Mobile-Friendly Linear-Attention for Vision Transformers'
openreview: VHtIDVaOKC
abstract: Vision Transformers (ViTs) excel in computer vision tasks due to their ability
  to capture global context among tokens. However, their quadratic complexity $\mathcal{O}(N^2D)$
  in terms of token number $N$ and feature dimension $D$ limits practical use on mobile
  devices, necessitating more mobile-friendly ViTs with reduced latency. Multi-head
  linear-attention is emerging as a promising alternative with linear complexity $\mathcal{O}(NDd)$,
  where $d$ is the per-head dimension. Still, more compute is needed as $d$ gets large
  for model accuracy. Reducing $d$ improves mobile friendliness at the expense of
  excessive small heads weak at learning valuable subspaces, ultimately impeding model
  capability. To overcome this efficiency-capability dilemma, we propose a novel Mobile-Attention
  design with a head-competition mechanism empowered by information flow, which prevents
  overemphasis on less important subspaces upon trivial heads while preserving essential
  subspaces to ensure Transformerâ€™s capability. It enables linear-time complexity
  on mobile devices by supporting a small per-head dimension $d$ for mobile efficiency.
  By replacing the standard attention of ViTs with Mobile-Attention, our optimized
  ViTs achieved enhanced model capacity and competitive performance in a range of
  computer vision tasks. Specifically, we have achieved remarkable reductions in latency
  on the iPhone 12. Code is available at https://github.com/thuml/MobileAttention.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: yao24c
month: 0
tex_title: 'Mobile Attention: Mobile-Friendly Linear-Attention for Vision Transformers'
firstpage: 56914
lastpage: 56926
page: 56914-56926
order: 56914
cycles: false
bibtex_author: Yao, Zhiyu and Wang, Jian and Wu, Haixu and Wang, Jingdong and Long,
  Mingsheng
author:
- given: Zhiyu
  family: Yao
- given: Jian
  family: Wang
- given: Haixu
  family: Wu
- given: Jingdong
  family: Wang
- given: Mingsheng
  family: Long
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/yao24c/yao24c.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
