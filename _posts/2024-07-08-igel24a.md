---
title: Smooth Min-Max Monotonic Networks
openreview: m8t1yzfBsJ
abstract: Monotonicity constraints are powerful regularizers in statistical modelling.
  They can support fairness in computer-aided decision making and increase plausibility
  in data-driven scientific models. The seminal min-max (MM) neural network architecture
  ensures monotonicity, but often gets stuck in undesired local optima during training
  because of partial derivatives being zero when computing extrema. We propose a simple
  modification of the MM network using strictly-increasing smooth minimum and maximum
  functions that alleviates this problem. The resulting smooth min-max (SMM) network
  module inherits the asymptotic approximation properties from the MM architecture.
  It can be used within larger deep learning systems trained end-to-end. The SMM module
  is conceptually simple and computationally less demanding than state-of-the-art
  neural networks for monotonic modelling. Our experiments show that this does not
  come with a loss in generalization performance compared to alternative neural and
  non-neural approaches.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: igel24a
month: 0
tex_title: Smooth Min-Max Monotonic Networks
firstpage: 20908
lastpage: 20923
page: 20908-20923
order: 20908
cycles: false
bibtex_author: Igel, Christian
author:
- given: Christian
  family: Igel
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/assets/igel24a/igel24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
