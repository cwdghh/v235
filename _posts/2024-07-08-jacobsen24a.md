---
title: Online Linear Regression in Dynamic Environments via Discounting
openreview: 9iRGs3wBTy
abstract: We develop algorithms for online linear regression which achieve optimal
  static and dynamic regret guarantees <em>even in the complete absence of prior knowledge</em>.
  We present a novel analysis showing that a discounted variant of the Vovk-Azoury-Warmuth
  forecaster achieves dynamic regret of the form $R_{T}(\vec{u})\le O\Big(d\log(T)\vee
  \sqrt{dP_{T}^{\gamma}(\vec{u})T}\Big)$, where $P_{T}^{\gamma}(\vec{u})$ is a measure
  of variability of the comparator sequence, and show that the discount factor achieving
  this result can be learned on-the-fly. We show that this result is optimal by providing
  a matching lower bound. We also extend our results to <em>strongly-adaptive</em>
  guarantees which hold over every sub-interval $[a,b]\subseteq[1,T]$ simultaneously.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: jacobsen24a
month: 0
tex_title: Online Linear Regression in Dynamic Environments via Discounting
firstpage: 21083
lastpage: 21120
page: 21083-21120
order: 21083
cycles: false
bibtex_author: Jacobsen, Andrew and Cutkosky, Ashok
author:
- given: Andrew
  family: Jacobsen
- given: Ashok
  family: Cutkosky
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://raw.githubusercontent.com/mlresearch/v235/main/assets/jacobsen24a/jacobsen24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
