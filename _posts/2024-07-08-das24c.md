---
title: A decoder-only foundation model for time-series forecasting
openreview: jn2iTJas6h
abstract: Motivated by recent advances in large language models for Natural Language
  Processing (NLP), we design a time-series foundation model for forecasting whose
  out-of-the-box zero-shot performance on a variety of public datasets comes close
  to the accuracy of state-of-the-art supervised forecasting models for each individual
  dataset. Our model is based on pretraining a decoder style attention model with
  input patching, using a large time-series corpus comprising both real-world and
  synthetic datasets. Experiments on a diverse set of previously unseen forecasting
  datasets suggests that the model can yield accurate zero-shot forecasts across different
  domains, forecasting horizons and temporal granularities.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: das24c
month: 0
tex_title: A decoder-only foundation model for time-series forecasting
firstpage: 10148
lastpage: 10167
page: 10148-10167
order: 10148
cycles: false
bibtex_author: Das, Abhimanyu and Kong, Weihao and Sen, Rajat and Zhou, Yichen
author:
- given: Abhimanyu
  family: Das
- given: Weihao
  family: Kong
- given: Rajat
  family: Sen
- given: Yichen
  family: Zhou
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://raw.githubusercontent.com/mlresearch/v235/main/assets/das24c/das24c.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
