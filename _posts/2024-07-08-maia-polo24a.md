---
title: 'tinyBenchmarks: evaluating LLMs with fewer examples'
openreview: qAml3FpfhG
abstract: 'The versatility of large language models (LLMs) led to the creation of
  diverse benchmarks that thoroughly test a variety of language modelsâ€™ abilities.
  These benchmarks consist of tens of thousands of examples making evaluation of LLMs
  very expensive. In this paper, we investigate strategies to reduce the number of
  evaluations needed to assess the performance of an LLM on several key benchmarks.
  For example, we show that to accurately estimate the performance of an LLM on MMLU,
  a popular multiple-choice QA benchmark consisting of 14K examples, it is sufficient
  to evaluate this LLM on 100 curated examples. We release evaluation tools and tiny
  versions of popular benchmarks: Open LLM Leaderboard, MMLU, HELM, and AlpacaEval
  2.0. Our empirical analysis demonstrates that these tools and tiny benchmarks are
  sufficient to reliably and efficiently reproduce the original evaluation results.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: maia-polo24a
month: 0
tex_title: 'tiny{B}enchmarks: evaluating {LLM}s with fewer examples'
firstpage: 34303
lastpage: 34326
page: 34303-34326
order: 34303
cycles: false
bibtex_author: Maia Polo, Felipe and Weber, Lucas and Choshen, Leshem and Sun, Yuekai
  and Xu, Gongjun and Yurochkin, Mikhail
author:
- given: Felipe
  family: Maia Polo
- given: Lucas
  family: Weber
- given: Leshem
  family: Choshen
- given: Yuekai
  family: Sun
- given: Gongjun
  family: Xu
- given: Mikhail
  family: Yurochkin
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://raw.githubusercontent.com/mlresearch/v235/main/assets/maia-polo24a/maia-polo24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
