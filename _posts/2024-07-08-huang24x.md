---
title: 'Position: TrustLLM: Trustworthiness in Large Language Models'
openreview: bWUU0LwwMp
abstract: Large language models (LLMs) have gained considerable attention for their
  excellent natural language processing capabilities. Nonetheless, these LLMs present
  many challenges, particularly in the realm of trustworthiness. This paper introduces
  TrustLLM, a comprehensive study of trustworthiness in LLMs, including principles
  for different dimensions of trustworthiness, established benchmark, evaluation,
  and analysis of trustworthiness for mainstream LLMs, and discussion of open challenges
  and future directions. Specifically, we first propose a set of principles for trustworthy
  LLMs that span eight different dimensions. Based on these principles, we further
  establish a benchmark across six dimensions including truthfulness, safety, fairness,
  robustness, privacy, and machine ethics. We then present a study evaluating 16 mainstream
  LLMs in TrustLLM, consisting of over 30 datasets. Our findings firstly show that
  in general trustworthiness and capability (i.e., functional effectiveness) are positively
  related. Secondly, our observations reveal that proprietary LLMs generally outperform
  most open-source counterparts in terms of trustworthiness, raising concerns about
  the potential risks of widely accessible open-source LLMs. However, a few open-source
  LLMs come very close to proprietary ones, suggesting that open-source models can
  achieve high levels of trustworthiness without additional mechanisms like <em>moderator</em>,
  offering valuable insights for developers in this field. Thirdly, it is important
  to note that some LLMs may be overly calibrated towards exhibiting trustworthiness,
  to the extent that they compromise their utility by mistakenly treating benign prompts
  as harmful and consequently not responding. Besides these observations, weâ€™ve uncovered
  key insights into the multifaceted trustworthiness in LLMs. We emphasize the importance
  of ensuring transparency not only in the models themselves but also in the technologies
  that underpin trustworthiness. We advocate that the establishment of an AI alliance
  between industry, academia, the open-source community to foster collaboration is
  imperative to advance the trustworthiness of LLMs.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: huang24x
month: 0
tex_title: 'Position: {T}rust{LLM}: Trustworthiness in Large Language Models'
firstpage: 20166
lastpage: 20270
page: 20166-20270
order: 20166
cycles: false
bibtex_author: Huang, Yue and Sun, Lichao and Wang, Haoran and Wu, Siyuan and Zhang,
  Qihui and Li, Yuan and Gao, Chujie and Huang, Yixin and Lyu, Wenhan and Zhang, Yixuan
  and Li, Xiner and Sun, Hanchi and Liu, Zhengliang and Liu, Yixin and Wang, Yijue
  and Zhang, Zhikun and Vidgen, Bertie and Kailkhura, Bhavya and Xiong, Caiming and
  Xiao, Chaowei and Li, Chunyuan and Xing, Eric P. and Huang, Furong and Liu, Hao
  and Ji, Heng and Wang, Hongyi and Zhang, Huan and Yao, Huaxiu and Kellis, Manolis
  and Zitnik, Marinka and Jiang, Meng and Bansal, Mohit and Zou, James and Pei, Jian
  and Liu, Jian and Gao, Jianfeng and Han, Jiawei and Zhao, Jieyu and Tang, Jiliang
  and Wang, Jindong and Vanschoren, Joaquin and Mitchell, John and Shu, Kai and Xu,
  Kaidi and Chang, Kai-Wei and He, Lifang and Huang, Lifu and Backes, Michael and
  Gong, Neil Zhenqiang and Yu, Philip S. and Chen, Pin-Yu and Gu, Quanquan and Xu,
  Ran and Ying, Rex and Ji, Shuiwang and Jana, Suman and Chen, Tianlong and Liu, Tianming
  and Zhou, Tianyi and Wang, William Yang and Li, Xiang and Zhang, Xiangliang and
  Wang, Xiao and Xie, Xing and Chen, Xun and Wang, Xuyu and Liu, Yan and Ye, Yanfang
  and Cao, Yinzhi and Chen, Yong and Zhao, Yue
author:
- given: Yue
  family: Huang
- given: Lichao
  family: Sun
- given: Haoran
  family: Wang
- given: Siyuan
  family: Wu
- given: Qihui
  family: Zhang
- given: Yuan
  family: Li
- given: Chujie
  family: Gao
- given: Yixin
  family: Huang
- given: Wenhan
  family: Lyu
- given: Yixuan
  family: Zhang
- given: Xiner
  family: Li
- given: Hanchi
  family: Sun
- given: Zhengliang
  family: Liu
- given: Yixin
  family: Liu
- given: Yijue
  family: Wang
- given: Zhikun
  family: Zhang
- given: Bertie
  family: Vidgen
- given: Bhavya
  family: Kailkhura
- given: Caiming
  family: Xiong
- given: Chaowei
  family: Xiao
- given: Chunyuan
  family: Li
- given: Eric P.
  family: Xing
- given: Furong
  family: Huang
- given: Hao
  family: Liu
- given: Heng
  family: Ji
- given: Hongyi
  family: Wang
- given: Huan
  family: Zhang
- given: Huaxiu
  family: Yao
- given: Manolis
  family: Kellis
- given: Marinka
  family: Zitnik
- given: Meng
  family: Jiang
- given: Mohit
  family: Bansal
- given: James
  family: Zou
- given: Jian
  family: Pei
- given: Jian
  family: Liu
- given: Jianfeng
  family: Gao
- given: Jiawei
  family: Han
- given: Jieyu
  family: Zhao
- given: Jiliang
  family: Tang
- given: Jindong
  family: Wang
- given: Joaquin
  family: Vanschoren
- given: John
  family: Mitchell
- given: Kai
  family: Shu
- given: Kaidi
  family: Xu
- given: Kai-Wei
  family: Chang
- given: Lifang
  family: He
- given: Lifu
  family: Huang
- given: Michael
  family: Backes
- given: Neil Zhenqiang
  family: Gong
- given: Philip S.
  family: Yu
- given: Pin-Yu
  family: Chen
- given: Quanquan
  family: Gu
- given: Ran
  family: Xu
- given: Rex
  family: Ying
- given: Shuiwang
  family: Ji
- given: Suman
  family: Jana
- given: Tianlong
  family: Chen
- given: Tianming
  family: Liu
- given: Tianyi
  family: Zhou
- given: William Yang
  family: Wang
- given: Xiang
  family: Li
- given: Xiangliang
  family: Zhang
- given: Xiao
  family: Wang
- given: Xing
  family: Xie
- given: Xun
  family: Chen
- given: Xuyu
  family: Wang
- given: Yan
  family: Liu
- given: Yanfang
  family: Ye
- given: Yinzhi
  family: Cao
- given: Yong
  family: Chen
- given: Yue
  family: Zhao
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://raw.githubusercontent.com/mlresearch/v235/main/assets/huang24x/huang24x.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
