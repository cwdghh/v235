---
title: I/O Complexity of Attention, or How Optimal is FlashAttention?
openreview: MdPBVWTfwG
abstract: 'Attention is at the heart of the popular Transformer architecture, yet
  suffers from quadratic time and memory complexity. In a recent significant development,
  FlashAttention shows that the I/O complexity of attention is the true bottleneck
  in scaling Transformers. Given two levels of memory hierarchy, a fast cache (e.g.
  GPU on-chip SRAM) where computation happens and a slow memory (e.g. GPU high-bandwidth
  memory) where the data resides, the I/O complexity measures the number of accesses
  to the slow memory. FlashAttention is an I/O-aware algorithm for self-attention
  that requires $\frac{N^2d^2}{M}$ I/O operations where $N$ is the dimension of the
  attention matrix, $d$ is the head-dimension and $M$ is the size of cache. Naturally,
  to further reduce the computational costs of Attention, the authors ask the question:
  is FlashAttentionâ€™s I/O complexity optimal for every value of $M$? We resolve the
  above question in its full generality by showing an I/O complexity lower bound that
  matches the upper bound provided by FlashAttention for any values of $M \geq d^2$
  within any constant factors. Moreover, our lower bounds do not rely on using combinatorial
  matrix multiplication for computing the attention matrix: even if one uses fast
  matrix multiplication, the above I/O complexity bounds cannot be improved. Further,
  we give a better algorithm with lower I/O complexity for $M < d^2$, and show that
  it is optimal for combinatorial algorithms. We do so by introducing a new communication
  complexity protocol for matrix compression, and connecting communication complexity
  to I/O complexity. We believe this connection could be of independent interest and
  will find more applications in proving I/O complexity lower bounds in future.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: saha24a
month: 0
tex_title: "{I}/{O} Complexity of Attention, or How Optimal is {F}lash{A}ttention?"
firstpage: 43024
lastpage: 43042
page: 43024-43042
order: 43024
cycles: false
bibtex_author: Saha, Barna and Ye, Christopher
author:
- given: Barna
  family: Saha
- given: Christopher
  family: Ye
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/saha24a/saha24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
