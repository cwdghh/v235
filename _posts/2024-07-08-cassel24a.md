---
title: Near-Optimal Regret in Linear MDPs with Aggregate Bandit Feedback
openreview: hXQOO6VsxH
abstract: 'In many real-world applications, it is hard to provide a reward signal
  in each step of a Reinforcement Learning (RL) process and more natural to give feedback
  when an episode ends. To this end, we study the recently proposed model of RL with
  Aggregate Bandit Feedback (RL-ABF), where the agent only observes the sum of rewards
  at the end of an episode instead of each reward individually. Prior work studied
  RL-ABF only in tabular settings, where the number of states is assumed to be small.
  In this paper, we extend ABF to linear function approximation and develop two efficient
  algorithms with near-optimal regret guarantees: a value-based optimistic algorithm
  built on a new randomization technique with a Q-functions ensemble, and a policy
  optimization algorithm that uses a novel hedging scheme over the ensemble.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: cassel24a
month: 0
tex_title: Near-Optimal Regret in Linear {MDP}s with Aggregate Bandit Feedback
firstpage: 5757
lastpage: 5791
page: 5757-5791
order: 5757
cycles: false
bibtex_author: Cassel, Asaf and Luo, Haipeng and Rosenberg, Aviv and Sotnikov, Dmitry
author:
- given: Asaf
  family: Cassel
- given: Haipeng
  family: Luo
- given: Aviv
  family: Rosenberg
- given: Dmitry
  family: Sotnikov
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/cassel24a/cassel24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
