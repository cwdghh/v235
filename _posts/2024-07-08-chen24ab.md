---
title: 'DiJiang: Efficient Large Language Models through Compact Kernelization'
openreview: 0uUHfhXdnH
abstract: In an effort to reduce the computational load of Transformers, research
  on linear attention has gained significant momentum. However, the improvement strategies
  for attention mechanisms typically necessitate extensive retraining, which is impractical
  for large language models with a vast array of parameters. In this paper, we present
  DiJiang, a novel Frequency Domain Kernelization approach that enables the transformation
  of a pre-trained vanilla Transformer into a linear complexity model with little
  training costs. By employing a weighted Quasi-Monte Carlo method for sampling, the
  proposed approach theoretically offers superior approximation efficiency. To further
  reduce the training computational complexity, our kernelization is based on Discrete
  Cosine Transform (DCT) operations. Extensive experiments demonstrate that the proposed
  method achieves comparable performance to the original Transformer, but with significantly
  reduced training costs and much faster inference speeds. Our DiJiang-7B achieves
  comparable performance with LLaMA2-7B on various benchmark while requires only about
  1/50 training cost. Code is available at https://github.com/YuchuanTian/DiJiang.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: chen24ab
month: 0
tex_title: "{D}i{J}iang: Efficient Large Language Models through Compact Kernelization"
firstpage: 7103
lastpage: 7117
page: 7103-7117
order: 7103
cycles: false
bibtex_author: Chen, Hanting and Zhicheng, Liu and Wang, Xutao and Tian, Yuchuan and
  Wang, Yunhe
author:
- given: Hanting
  family: Chen
- given: Liu
  family: Zhicheng
- given: Xutao
  family: Wang
- given: Yuchuan
  family: Tian
- given: Yunhe
  family: Wang
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://raw.githubusercontent.com/mlresearch/v235/main/assets/chen24ab/chen24ab.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
