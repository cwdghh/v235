---
title: 'COPAL: Continual Pruning in Large Language Generative Models'
openreview: Lt8Lk7IQ5b
abstract: 'Adapting pre-trained large language models to different domains in natural
  language processing requires two key considerations: high computational demands
  and model’s inability to continual adaptation. To simultaneously address both issues,
  this paper presents COPAL (<b>CO</b>ntinual <b>P</b>runing in <b>A</b>daptive <b>L</b>anguage
  settings), an algorithm developed for pruning large language generative models under
  a continual model adaptation setting. While avoiding resource-heavy finetuning or
  retraining, our pruning process is guided by the proposed sensitivity analysis.
  The sensitivity effectively measures model’s ability to withstand perturbations
  introduced by the new dataset and finds model’s weights that are relevant for all
  encountered datasets. As a result, COPAL allows seamless model adaptation to new
  domains while enhancing the resource efficiency. Our empirical evaluation on a various
  size of LLMs show that COPAL outperforms baseline models, demonstrating its efficacy
  in efficiency and adaptability.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: malla24a
month: 0
tex_title: "{COPAL}: Continual Pruning in Large Language Generative Models"
firstpage: 34529
lastpage: 34542
page: 34529-34542
order: 34529
cycles: false
bibtex_author: Malla, Srikanth and Choi, Joon Hee and Choi, Chiho
author:
- given: Srikanth
  family: Malla
- given: Joon Hee
  family: Choi
- given: Chiho
  family: Choi
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://raw.githubusercontent.com/mlresearch/v235/main/assets/malla24a/malla24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
