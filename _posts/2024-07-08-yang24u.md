---
title: Reducing Fine-Tuning Memory Overhead by Approximate and Memory-Sharing Backpropagation
openreview: IpSKpOY2EH
abstract: Fine-tuning pretrained large models to downstream tasks is an important
  problem, which however suffers from huge memory overhead due to large-scale parameters.
  This work strives to reduce memory overhead in fine-tuning from perspectives of
  activation function and layer normalization. To this end, we propose the Approximate
  Backpropagation (Approx-BP) theory, which provides the theoretical feasibility of
  decoupling the forward and backward passes. We apply our Approx-BP theory to backpropagation
  training and derive memory-efficient alternatives of GELU and SiLU activation functions,
  which use derivative functions of ReLUs in the backward pass while keeping their
  forward pass unchanged. In addition, we introduce a Memory-Sharing Backpropagation
  strategy, which enables the activation memory to be shared by two adjacent layers,
  thereby removing activation memory usage redundancy. Our method neither induces
  extra computation nor reduces training efficiency. We conduct extensive experiments
  with pretrained vision and language models, and the results demonstrate that our
  proposal can reduce up to $\sim$$30%$ of the peak memory usage. Our code is released
  at github.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: yang24u
month: 0
tex_title: Reducing Fine-Tuning Memory Overhead by Approximate and Memory-Sharing
  Backpropagation
firstpage: 56357
lastpage: 56381
page: 56357-56381
order: 56357
cycles: false
bibtex_author: Yang, Yuchen and Shi, Yingdong and Wang, Cheems and Zhen, Xiantong
  and Shi, Yuxuan and Xu, Jun
author:
- given: Yuchen
  family: Yang
- given: Yingdong
  family: Shi
- given: Cheems
  family: Wang
- given: Xiantong
  family: Zhen
- given: Yuxuan
  family: Shi
- given: Jun
  family: Xu
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/yang24u/yang24u.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
