---
title: Getting the most out of your tokenizer for pre-training and domain adaptation
openreview: ZFYBnLljtT
abstract: Tokenization is an understudied and often neglected component of modern
  LLMs. Most published works use a single tokenizer for all experiments, often borrowed
  from another model, without performing ablations or analysis to optimize tokenization.
  Moreover, the tokenizer is generally kept unchanged when fine-tuning a base model.
  In this paper, we show that the size, pre-tokenization regular expression, and training
  data of a tokenizer can significantly impact the modelâ€™s generation speed, effective
  context size, memory usage, and downstream performance. We train specialized Byte-Pair
  Encoding code tokenizers, and conduct extensive ablations on the impact of tokenizer
  design on the performance of LLMs for code generation tasks such as HumanEval and
  MBPP, and provide recommendations for tokenizer hyper-parameters selection and switching
  the tokenizer in a pre-trained LLM. We perform our experiments on models trained
  from scratch and from pre-trained models, verifying their applicability to a wide
  range of use-cases. We find that when fine-tuning on more than 50 billion tokens,
  we can specialize the tokenizer of a pre-trained LLM to obtain large gains in generation
  speed and effective context size.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: dagan24a
month: 0
tex_title: Getting the most out of your tokenizer for pre-training and domain adaptation
firstpage: 9784
lastpage: 9805
page: 9784-9805
order: 9784
cycles: false
bibtex_author: Dagan, Gautier and Synnaeve, Gabriel and Roziere, Baptiste
author:
- given: Gautier
  family: Dagan
- given: Gabriel
  family: Synnaeve
- given: Baptiste
  family: Roziere
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://raw.githubusercontent.com/mlresearch/v235/main/assets/dagan24a/dagan24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
