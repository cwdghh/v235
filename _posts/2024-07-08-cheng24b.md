---
title: Layerwise Change of Knowledge in Neural Networks
openreview: 7zEoinErzQ
abstract: This paper aims to explain how a deep neural network (DNN) gradually extracts
  new knowledge and forgets noisy features through layers in forward propagation.
  Up to now, although how to define knowledge encoded by the DNN has not reached a
  consensus so far, previous studies have derived a series of mathematical evidences
  to take interactions as symbolic primitive inference patterns encoded by a DNN.
  We extend the definition of interactions and, for the first time, extract interactions
  encoded by intermediate layers. We quantify and track the newly emerged interactions
  and the forgotten interactions in each layer during the forward propagation, which
  shed new light on the learning behavior of DNNs. The layer-wise change of interactions
  also reveals the change of the generalization capacity and instability of feature
  representations of a DNN.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: cheng24b
month: 0
tex_title: Layerwise Change of Knowledge in Neural Networks
firstpage: 8038
lastpage: 8059
page: 8038-8059
order: 8038
cycles: false
bibtex_author: Cheng, Xu and Cheng, Lei and Peng, Zhaoran and Xu, Yang and Han, Tian
  and Zhang, Quanshi
author:
- given: Xu
  family: Cheng
- given: Lei
  family: Cheng
- given: Zhaoran
  family: Peng
- given: Yang
  family: Xu
- given: Tian
  family: Han
- given: Quanshi
  family: Zhang
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://raw.githubusercontent.com/mlresearch/v235/main/assets/cheng24b/cheng24b.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
