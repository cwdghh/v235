---
title: 'KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache'
openreview: L057s2Rq8O
abstract: Efficiently serving large language models (LLMs) requires batching many
  requests together to reduce the cost per request. Yet, the key-value (KV) cache,
  which stores attention keys and values to avoid re-computations, significantly increases
  memory demands and becomes the new bottleneck in speed and memory usage. This memory
  demand increases with larger batch sizes and longer context lengths. Additionally,
  the inference speed is limited by the size of KV cache, as the GPUâ€™s SRAM must load
  the entire KV cache from the main GPU memory for each token generated, causing the
  computational core to be idle during this process. A straightforward and effective
  solution to reduce KV cache size is quantization, which decreases the total bytes
  taken by KV cache. However, there is a lack of in-depth studies that explore the
  element distribution of KV cache to understand the hardness and limitation of KV
  cache quantization. To fill the gap, we conducted a comprehensive study on the element
  distribution in KV cache of popular LLMs. Our findings indicate that the key cache
  should be quantized per-channel, i.e., group elements along the channel dimension
  and quantize them together. In contrast, the value cache should be quantized per-token.
  From this analysis, we developed a tuning-free 2bit KV cache quantization algorithm,
  named KIVI. With the hardware-friendly implementation, KIVI can enable Llama (Llama-2),
  Falcon, and Mistral models to maintain almost the same quality while using 2.6$\times$
  less peak memory usage (including the model weight). This reduction in memory usage
  enables up to 4x larger batch size, bringing $2.35 \times \sim 3.47 \times$ throughput
  on real LLM inference workload.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: liu24bz
month: 0
tex_title: "{KIVI}: A Tuning-Free Asymmetric 2bit Quantization for {KV} Cache"
firstpage: 32332
lastpage: 32344
page: 32332-32344
order: 32332
cycles: false
bibtex_author: Liu, Zirui and Yuan, Jiayi and Jin, Hongye and Zhong, Shaochen and
  Xu, Zhaozhuo and Braverman, Vladimir and Chen, Beidi and Hu, Xia
author:
- given: Zirui
  family: Liu
- given: Jiayi
  family: Yuan
- given: Hongye
  family: Jin
- given: Shaochen
  family: Zhong
- given: Zhaozhuo
  family: Xu
- given: Vladimir
  family: Braverman
- given: Beidi
  family: Chen
- given: Xia
  family: Hu
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/assets/liu24bz/liu24bz.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
