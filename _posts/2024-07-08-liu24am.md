---
title: Characterizing ResNet’s Universal Approximation Capability
openreview: z7zHsNFXHc
abstract: 'Since its debut in 2016, ResNet has become arguably the most favorable
  architecture in deep neural network (DNN) design. It effectively addresses the gradient
  vanishing/exploding issue in DNN training, allowing engineers to fully unleash DNN’s
  potential in tackling challenging problems in various domains. Despite its practical
  success, an essential theoretical question remains largely open: how well/best can
  ResNet approximate functions? In this paper, we answer this question for several
  important function classes, including polynomials and smooth functions. In particular,
  we show that ResNet with constant width can approximate Lipschitz continuous function
  with a Lipschitz constant $\mu$ using $\mathcal{O}(c(d)(\varepsilon/\mu)^{-d/2})$
  tunable weights, where $c(d)$ is a constant depending on the input dimension $d$
  and $\epsilon>0$ is the target approximation error. Further, we extend such a result
  to Lebesgue-integrable functions with the upper bound characterized by the modulus
  of continuity. These results indicate a factor of $d$ reduction in the number of
  tunable weights compared with the classical results for ReLU networks. Our results
  are also order-optimal in $\varepsilon$, thus achieving optimal approximation rate,
  as they match a generalized lower bound derived in this paper. This work adds to
  the theoretical justifications for ResNet’s stellar practical performance.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: liu24am
month: 0
tex_title: Characterizing {R}es{N}et’s Universal Approximation Capability
firstpage: 31477
lastpage: 31515
page: 31477-31515
order: 31477
cycles: false
bibtex_author: Liu, Chenghao and Liang, Enming and Chen, Minghua
author:
- given: Chenghao
  family: Liu
- given: Enming
  family: Liang
- given: Minghua
  family: Chen
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/assets/liu24am/liu24am.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
