---
title: Pausing Policy Learning in Non-stationary Reinforcement Learning
openreview: qY622O6Ehg
abstract: 'Real-time inference is a challenge of real-world reinforcement learning
  due to temporal differences in time-varying environments: the system collects data
  from the past, updates the decision model in the present, and deploys it in the
  future. We tackle a common belief that continually updating the decision is optimal
  to minimize the temporal gap. We propose forecasting an online reinforcement learning
  framework and show that strategically pausing decision updates yields better overall
  performance by effectively managing aleatoric uncertainty. Theoretically, we compute
  an optimal ratio between policy update and hold duration, and show that a non-zero
  policy hold duration provides a sharper upper bound on the dynamic regret. Our experimental
  evaluations on three different environments also reveal that a non-zero policy hold
  duration yields higher rewards compared to continuous decision updates.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: lee24l
month: 0
tex_title: Pausing Policy Learning in Non-stationary Reinforcement Learning
firstpage: 26661
lastpage: 26685
page: 26661-26685
order: 26661
cycles: false
bibtex_author: Lee, Hyunin and Jin, Ming and Lavaei, Javad and Sojoudi, Somayeh
author:
- given: Hyunin
  family: Lee
- given: Ming
  family: Jin
- given: Javad
  family: Lavaei
- given: Somayeh
  family: Sojoudi
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/lee24l/lee24l.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
