---
title: How Smooth Is Attention?
openreview: aP0H8A1ywk
abstract: Self-attention and masked self-attention are at the heart of Transformers’
  outstanding success. Still, our mathematical understanding of attention, in particular
  of its Lipschitz properties — which are key when it comes to analyzing robustness
  and expressive power — is incomplete. We provide a detailed study of the Lipschitz
  constant of self-attention in several practical scenarios, discussing the impact
  of the sequence length $n$ and layer normalization on the local Lipschitz constant
  of both unmasked and masked self-attention. In particular, we show that for inputs
  of length $n$ in any compact set, the Lipschitz constant of self-attention is bounded
  by $\sqrt{n}$ up to a constant factor and that this bound is tight for reasonable
  sequence lengths. When the sequence length $n$ is too large for the previous bound
  to be tight, which we refer to as the mean-field regime, we provide an upper bound
  and a matching lower bound which are independent of $n$. Our mean-field framework
  for masked self-attention is novel and of independent interest. Our experiments
  on pretrained and randomly initialized BERT and GPT-2 support our theoretical findings.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: castin24a
month: 0
tex_title: How Smooth Is Attention?
firstpage: 5817
lastpage: 5840
page: 5817-5840
order: 5817
cycles: false
bibtex_author: Castin, Val\'{e}rie and Ablin, Pierre and Peyr\'{e}, Gabriel
author:
- given: Valérie
  family: Castin
- given: Pierre
  family: Ablin
- given: Gabriel
  family: Peyré
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/castin24a/castin24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
