---
title: Scalable High-Resolution Pixel-Space Image Synthesis with Hourglass Diffusion
  Transformers
openreview: WRIn2HmtBS
abstract: We present the Hourglass Diffusion Transformer (HDiT), an image-generative
  model that exhibits linear scaling with pixel count, supporting training at high
  resolution (e.g. $1024 \times 1024$) directly in pixel-space. Building on the Transformer
  architecture, which is known to scale to billions of parameters, it bridges the
  gap between the efficiency of convolutional U-Nets and the scalability of Transformers.
  HDiT trains successfully without typical high-resolution training techniques such
  as multiscale architectures, latent autoencoders or self-conditioning. We demonstrate
  that HDiT performs competitively with existing models on ImageNet $256^2$, and sets
  a new state-of-the-art for diffusion models on FFHQ-$1024^2$. Code is available
  at https://github.com/crowsonkb/k-diffusion.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: crowson24a
month: 0
tex_title: Scalable High-Resolution Pixel-Space Image Synthesis with Hourglass Diffusion
  Transformers
firstpage: 9550
lastpage: 9575
page: 9550-9575
order: 9550
cycles: false
bibtex_author: Crowson, Katherine and Baumann, Stefan Andreas and Birch, Alex and
  Abraham, Tanishq Mathew and Kaplan, Daniel Z and Shippole, Enrico
author:
- given: Katherine
  family: Crowson
- given: Stefan Andreas
  family: Baumann
- given: Alex
  family: Birch
- given: Tanishq Mathew
  family: Abraham
- given: Daniel Z
  family: Kaplan
- given: Enrico
  family: Shippole
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/assets/crowson24a/crowson24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
