---
title: Revisit the Essence of Distilling Knowledge through Calibration
openreview: NZgbwzaOIx
abstract: Knowledge Distillation (KD) has evolved into a practical technology for
  transferring knowledge from a well-performing model (teacher) to a weak model (student).
  A counter-intuitive phenomenon known as capacity mismatch has been identified, wherein
  KD performance may not be good when a better teacher instructs the student. Various
  preliminary methods have been proposed to alleviate capacity mismatch, but a unifying
  explanation for its cause remains lacking. In this paper, we propose <em>a unifying
  analytical framework to pinpoint the core of capacity mismatch based on calibration</em>.
  Through extensive analytical experiments, we observe a positive correlation between
  the calibration of the teacher model and the KD performance with original KD methods.
  As this correlation arises due to the sensitivity of metrics (e.g., KL divergence)
  to calibration, we recommend employing measurements insensitive to calibration such
  as ranking-based loss. Our experiments demonstrate that ranking-based loss can effectively
  replace KL divergence, aiding large models with poor calibration to teach better.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: fan24d
month: 0
tex_title: Revisit the Essence of Distilling Knowledge through Calibration
firstpage: 12882
lastpage: 12894
page: 12882-12894
order: 12882
cycles: false
bibtex_author: Fan, Wen-Shu and Lu, Su and Li, Xin-Chun and Zhan, De-Chuan and Gan,
  Le
author:
- given: Wen-Shu
  family: Fan
- given: Su
  family: Lu
- given: Xin-Chun
  family: Li
- given: De-Chuan
  family: Zhan
- given: Le
  family: Gan
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/fan24d/fan24d.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
