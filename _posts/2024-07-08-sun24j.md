---
title: 'FedBPT: Efficient Federated Black-box Prompt Tuning for Large Language Models'
openreview: AoYhtJ4A90
abstract: Pre-trained language models (PLM) have revolutionized the NLP landscape,
  achieving stellar performances across diverse tasks. These models, while benefiting
  from vast training data, often require fine-tuning on specific data to cater to
  distinct downstream tasks. However, this data adaptation process has inherent security
  and privacy concerns, primarily when leveraging user-generated, device-residing
  data. Federated learning (FL) provides a solution, allowing collaborative model
  fine-tuning without centralized data collection. However, applying FL to finetune
  PLMs is hampered by challenges, including restricted model parameter access due
  to the high encapsulation, high computational requirements, and communication overheads.
  This paper introduces Federated Black-box Prompt Tuning (FedBPT), a framework designed
  to address these challenges. FedBPT allows the clients to treat the model as a black-box
  inference API. By focusing on training optimal prompts and utilizing gradient-free
  optimization methods, FedBPT reduces the number of exchanged variables, boosts communication
  efficiency, and minimizes computational and storage costs. Experiments highlight
  the frameworkâ€™s ability to drastically cut communication and memory costs while
  maintaining competitive performance. Ultimately, FedBPT presents a promising solution
  for efficient, privacy-preserving fine-tuning of PLM in the age of large language
  models.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: sun24j
month: 0
tex_title: "{F}ed{BPT}: Efficient Federated Black-box Prompt Tuning for Large Language
  Models"
firstpage: 47159
lastpage: 47173
page: 47159-47173
order: 47159
cycles: false
bibtex_author: Sun, Jingwei and Xu, Ziyue and Yin, Hongxu and Yang, Dong and Xu, Daguang
  and Liu, Yudong and Du, Zhixu and Chen, Yiran and Roth, Holger R
author:
- given: Jingwei
  family: Sun
- given: Ziyue
  family: Xu
- given: Hongxu
  family: Yin
- given: Dong
  family: Yang
- given: Daguang
  family: Xu
- given: Yudong
  family: Liu
- given: Zhixu
  family: Du
- given: Yiran
  family: Chen
- given: Holger R
  family: Roth
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://raw.githubusercontent.com/mlresearch/v235/main/assets/sun24j/sun24j.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
