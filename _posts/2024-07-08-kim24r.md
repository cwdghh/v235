---
title: Demystifying SGD with Doubly Stochastic Gradients
openreview: z373OXJXWU
abstract: 'Optimization objectives in the form of a sum of intractable expectations
  are rising in importance (<em>e.g.,</em>, diffusion models, variational autoencoders,
  and many more), a setting also known as "finite sum with infinite data." For these
  problems, a popular strategy is to employ SGD with <em>doubly stochastic gradients</em>
  (doubly SGD): the expectations are estimated using the gradient estimator of each
  component, while the sum is estimated by subsampling over these estimators. Despite
  its popularity, little is known about the convergence properties of doubly SGD,
  except under strong assumptions such as bounded variance. In this work, we establish
  the convergence of doubly SGD with independent minibatching and random reshuffling
  under general conditions, which encompasses dependent component gradient estimators.
  In particular, for dependent estimators, our analysis allows fined-grained analysis
  of the effect correlations. As a result, under a per-iteration computational budget
  of $b \times m$, where $b$ is the minibatch size and $m$ is the number of Monte
  Carlo samples, our analysis suggests where one should invest most of the budget
  in general. Furthermore, we prove that random reshuffling (RR) improves the complexity
  dependence on the subsampling noise.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: kim24r
month: 0
tex_title: Demystifying {SGD} with Doubly Stochastic Gradients
firstpage: 24210
lastpage: 24247
page: 24210-24247
order: 24210
cycles: false
bibtex_author: Kim, Kyurae and Ko, Joohwan and Ma, Yian and Gardner, Jacob R.
author:
- given: Kyurae
  family: Kim
- given: Joohwan
  family: Ko
- given: Yian
  family: Ma
- given: Jacob R.
  family: Gardner
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/assets/kim24r/kim24r.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
