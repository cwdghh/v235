---
title: Converting Transformers to Polynomial Form for Secure Inference Over Homomorphic
  Encryption
openreview: 9HPoJ6ulgV
abstract: Designing privacy-preserving DL solutions is a major challenge within the
  AI community. Homomorphic Encryption (HE) has emerged as one of the most promising
  approaches in this realm, enabling the decoupling of knowledge between a model owner
  and a data owner. Despite extensive research and application of this technology,
  primarily in CNNs, applying HE on transformer models has been challenging because
  of the difficulties in converting these models into a polynomial form. We break
  new ground by introducing the first polynomial transformer, providing the first
  demonstration of secure inference over HE with full transformers. This includes
  a transformer architecture tailored for HE, alongside a novel method for converting
  operators to their polynomial equivalent. This innovation enables us to perform
  secure inference on LMs and ViTs with several datasts and tasks. Our techniques
  yield results comparable to traditional models, bridging the performance gap with
  transformers of similar scale and underscoring the viability of HE for state-of-the-art
  applications. Finally, we assess the stability of our models and conduct a series
  of ablations to quantify the contribution of each model component. Our code is publicly
  available.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: zimerman24a
month: 0
tex_title: Converting Transformers to Polynomial Form for Secure Inference Over Homomorphic
  Encryption
firstpage: 62803
lastpage: 62814
page: 62803-62814
order: 62803
cycles: false
bibtex_author: Zimerman, Itamar and Baruch, Moran and Drucker, Nir and Ezov, Gilad
  and Soceanu, Omri and Wolf, Lior
author:
- given: Itamar
  family: Zimerman
- given: Moran
  family: Baruch
- given: Nir
  family: Drucker
- given: Gilad
  family: Ezov
- given: Omri
  family: Soceanu
- given: Lior
  family: Wolf
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/assets/zimerman24a/zimerman24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
