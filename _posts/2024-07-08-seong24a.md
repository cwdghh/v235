---
title: Self-Supervised Interpretable End-to-End Learning via Latent Functional Modularity
openreview: dFEeI51O5j
abstract: We introduce MoNet, a novel functionally modular network for self-supervised
  and interpretable end-to-end learning. By leveraging its functional modularity with
  a latent-guided contrastive loss function, MoNet efficiently learns task-specific
  decision-making processes in latent space without requiring task-level supervision.
  Moreover, our method incorporates an online, post-hoc explainability approach that
  enhances the interpretability of end-to-end inferences without compromising sensorimotor
  control performance. In real-world indoor environments, MoNet demonstrates effective
  visual autonomous navigation, outperforming baseline models by 7% to 28% in task
  specificity analysis. We further explore the interpretability of our network through
  post-hoc analysis of perceptual saliency maps and latent decision vectors. This
  provides valuable insights into the incorporation of explainable artificial intelligence
  into robotic learning, encompassing both perceptual and behavioral perspectives.
  Supplementary materials are available at https://sites.google.com/view/monet-lgc.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: seong24a
month: 0
tex_title: Self-Supervised Interpretable End-to-End Learning via Latent Functional
  Modularity
firstpage: 44212
lastpage: 44223
page: 44212-44223
order: 44212
cycles: false
bibtex_author: Seong, Hyunki and Shim, Hyunchul
author:
- given: Hyunki
  family: Seong
- given: Hyunchul
  family: Shim
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/assets/seong24a/seong24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
