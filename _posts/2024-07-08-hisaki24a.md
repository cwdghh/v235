---
title: 'RVI-SAC: Average Reward Off-Policy Deep Reinforcement Learning'
openreview: xB6YJZOKyT
abstract: In this paper, we propose an off-policy deep reinforcement learning (DRL)
  method utilizing the average reward criterion. While most existing DRL methods employ
  the discounted reward criterion, this can potentially lead to a discrepancy between
  the training objective and performance metrics in continuing tasks, making the average
  reward criterion a recommended alternative. We introduce RVI-SAC, an extension of
  the state-of-the-art off-policy DRL method, Soft Actor-Critic (SAC), to the average
  reward criterion. Our proposal consists of (1) Critic updates based on RVI Q-learning,
  (2) Actor updates introduced by the average reward soft policy improvement theorem,
  and (3) automatic adjustment of Reset Cost enabling the average reward reinforcement
  learning to be applied to tasks with termination. We apply our method to the Gymnasiumâ€™s
  Mujoco tasks, a subset of locomotion tasks, and demonstrate that RVI-SAC shows competitive
  performance compared to existing methods.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: hisaki24a
month: 0
tex_title: "{RVI}-{SAC}: Average Reward Off-Policy Deep Reinforcement Learning"
firstpage: 18352
lastpage: 18373
page: 18352-18373
order: 18352
cycles: false
bibtex_author: Hisaki, Yukinari and Ono, Isao
author:
- given: Yukinari
  family: Hisaki
- given: Isao
  family: Ono
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/assets/hisaki24a/hisaki24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
