---
title: 'Promises and Pitfalls of Generative Masked Language Modeling: Theoretical
  Framework and Practical Guidelines'
openreview: BTkaKA74mS
abstract: Autoregressive language models are the currently dominant paradigm for text
  generation, however they have some fundamental limitations that cannot be remedied
  by scaleâ€”for example inherently sequential and unidirectional generation. While
  alternate classes of models have been explored, we have limited mathematical understanding
  of their fundamental power and limitations. In this paper we focus on Generative
  Masked Language Models (GMLMs), a non-autoregressive paradigm in which we train
  a model to fit conditional probabilities of the data distribution via masking, which
  are subsequently used as inputs to a Markov Chain to draw samples from the model.
  These models empirically strike a promising speed-quality trade-off as each step
  can be typically parallelized by decoding the entire sequence in parallel. We develop
  a mathematical framework for analyzing and improving such models which sheds light
  on questions of sample complexity and inference speed and quality. Empirically,
  we adapt the T5 model for iteratively-refined parallel decoding, achieving 2-3x
  speedup in machine translation with minimal sacrifice in quality compared with autoregressive
  models. We run careful ablation experiments to give recommendations on key design
  choices, and make fine-grained observations on the common error modes in connection
  with our theory. Our mathematical analyses and empirical observations characterize
  both potentials and limitations of this approach, and can be applied to future works
  on improving understanding and performance of GMLMs. We released codes for our experiments.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: li24af
month: 0
tex_title: 'Promises and Pitfalls of Generative Masked Language Modeling: Theoretical
  Framework and Practical Guidelines'
firstpage: 27969
lastpage: 28017
page: 27969-28017
order: 27969
cycles: false
bibtex_author: Li, Yuchen and Kirchmeyer, Alexandre and Mehta, Aashay and Qin, Yilong
  and Dadachev, Boris and Papineni, Kishore and Kumar, Sanjiv and Risteski, Andrej
author:
- given: Yuchen
  family: Li
- given: Alexandre
  family: Kirchmeyer
- given: Aashay
  family: Mehta
- given: Yilong
  family: Qin
- given: Boris
  family: Dadachev
- given: Kishore
  family: Papineni
- given: Sanjiv
  family: Kumar
- given: Andrej
  family: Risteski
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/li24af/li24af.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
