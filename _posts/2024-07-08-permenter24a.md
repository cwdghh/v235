---
title: Interpreting and Improving Diffusion Models from an Optimization Perspective
openreview: o2ND9v0CeK
abstract: Denoising is intuitively related to projection. Indeed, under the manifold
  hypothesis, adding random noise is approximately equivalent to orthogonal perturbation.
  Hence, learning to denoise is approximately learning to project. In this paper,
  we use this observation to interpret denoising diffusion models as approximate gradient
  descent applied to the Euclidean distance function. We then provide straight-forward
  convergence analysis of the DDIM sampler under simple assumptions on the projection
  error of the denoiser. Finally, we propose a new gradient-estimation sampler, generalizing
  DDIM using insights from our theoretical results. In as few as 5-10 function evaluations,
  our sampler achieves state-of-the-art FID scores on pretrained CIFAR-10 and CelebA
  models and can generate high quality samples on latent diffusion models.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: permenter24a
month: 0
tex_title: Interpreting and Improving Diffusion Models from an Optimization Perspective
firstpage: 40461
lastpage: 40483
page: 40461-40483
order: 40461
cycles: false
bibtex_author: Permenter, Frank and Yuan, Chenyang
author:
- given: Frank
  family: Permenter
- given: Chenyang
  family: Yuan
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/assets/permenter24a/permenter24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
