---
title: How Free is Parameter-Free Stochastic Optimization?
openreview: 6L4K5jmSJq
abstract: 'We study the problem of parameter-free stochastic optimization, inquiring
  whether, and under what conditions, do fully parameter-free methods exist: these
  are methods that achieve convergence rates competitive with optimally tuned methods,
  without requiring significant knowledge of the true problem parameters. Existing
  parameter-free methods can only be considered “partially” parameter-free, as they
  require some non-trivial knowledge of the true problem parameters, such as a bound
  on the stochastic gradient norms, a bound on the distance to a minimizer, etc. In
  the non-convex setting, we demonstrate that a simple hyperparameter search technique
  results in a fully parameter-free method that outperforms more sophisticated state-of-the-art
  algorithms. We also provide a similar result in the convex setting with access to
  noisy function values under mild noise assumptions. Finally, assuming only access
  to stochastic gradients, we establish a lower bound that renders fully parameter-free
  stochastic convex optimization infeasible, and provide a method which is (partially)
  parameter-free up to the limit indicated by our lower bound.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: attia24a
month: 0
tex_title: How Free is Parameter-Free Stochastic Optimization?
firstpage: 2009
lastpage: 2034
page: 2009-2034
order: 2009
cycles: false
bibtex_author: Attia, Amit and Koren, Tomer
author:
- given: Amit
  family: Attia
- given: Tomer
  family: Koren
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/assets/attia24a/attia24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
