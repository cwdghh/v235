---
title: 'CLLMs: Consistency Large Language Models'
openreview: 8uzBOVmh8H
abstract: Jacobi decoding shows promise for more efficient LLM inference as it breaks
  the sequential nature of the LLM decoding process and transforms it into more parallelizable
  computation. However, in practice, it achieves little speedup compared to traditional
  autoregressive (AR) decoding, primarily because Jacobi decoding seldom accurately
  predicts more than one token in a single fixed-point iteration step. To address
  this, we develop a new approach aimed at realizing fast convergence from any state
  to the fixed point in a Jacobi trajectory. This is accomplished by refining the
  target LLM to consistently predict the fixed point given any state as input. Extensive
  experiments demonstrate the effectiveness of our method, showing 2.4$\times$ to
  3.4$\times$ improvements in generation speed while preserving generation quality
  across both domain-specific and open-domain benchmarks.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: kou24a
month: 0
tex_title: "{CLLM}s: Consistency Large Language Models"
firstpage: 25426
lastpage: 25440
page: 25426-25440
order: 25426
cycles: false
bibtex_author: Kou, Siqi and Hu, Lanxiang and He, Zhezhi and Deng, Zhijie and Zhang,
  Hao
author:
- given: Siqi
  family: Kou
- given: Lanxiang
  family: Hu
- given: Zhezhi
  family: He
- given: Zhijie
  family: Deng
- given: Hao
  family: Zhang
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://raw.githubusercontent.com/mlresearch/v235/main/assets/kou24a/kou24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
