---
title: Stochastic Bandits with ReLU Neural Networks
openreview: 2hidpjUPvV
abstract: We study the stochastic bandit problem with ReLU neural network structure.
  We show that a $\tilde{O}(\sqrt{T})$ regret guarantee is achievable by considering
  bandits with one-layer ReLU neural networks; to the best of our knowledge, our work
  is the first to achieve such a guarantee. In this specific setting, we propose an
  OFU-ReLU algorithm that can achieve this upper bound. The algorithm first explores
  randomly until it reaches a <em>linear</em> regime, and then implements a UCB-type
  linear bandit algorithm to balance exploration and exploitation. Our key insight
  is that we can exploit the piecewise linear structure of ReLU activations and convert
  the problem into a linear bandit in a transformed feature space, once we learn the
  parameters of ReLU relatively accurately during the exploration stage. To remove
  dependence on model parameters, we design an OFU-ReLU+ algorithm based on a batching
  strategy, which can provide the same theoretical guarantee.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: xu24c
month: 0
tex_title: Stochastic Bandits with {R}e{LU} Neural Networks
firstpage: 54866
lastpage: 54887
page: 54866-54887
order: 54866
cycles: false
bibtex_author: Xu, Kan and Bastani, Hamsa and Goel, Surbhi and Bastani, Osbert
author:
- given: Kan
  family: Xu
- given: Hamsa
  family: Bastani
- given: Surbhi
  family: Goel
- given: Osbert
  family: Bastani
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/xu24c/xu24c.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
