---
title: Bayesian Regret Minimization in Offline Bandits
openreview: mz55Ox0Igz
abstract: We study how to make decisions that minimize Bayesian regret in offline
  linear bandits. Prior work suggests that one must take actions with maximum lower
  confidence bound (LCB) on their reward. We argue that reliance on LCB is inherently
  flawed in this setting and propose a new algorithm that directly minimizes upper-bounds
  on the Bayesian regret using efficient conic optimization solvers. Our bounds build
  heavily on new connections to monetary risk measures. Proving a matching lower-bound,
  we show that our upper-bounds are tight, and by minimizing them we are guaranteed
  to outperform the LCB approach. Our numerical results on synthetic domains confirm
  that our approach is superior to maximizing LCB.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: petrik24a
month: 0
tex_title: "{B}ayesian Regret Minimization in Offline Bandits"
firstpage: 40502
lastpage: 40522
page: 40502-40522
order: 40502
cycles: false
bibtex_author: Petrik, Marek and Tennenholtz, Guy and Ghavamzadeh, Mohammad
author:
- given: Marek
  family: Petrik
- given: Guy
  family: Tennenholtz
- given: Mohammad
  family: Ghavamzadeh
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/petrik24a/petrik24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
