---
title: Improving Sharpness-Aware Minimization by Lookahead
openreview: J9YKDvqr65
abstract: Sharpness-Aware Minimization (SAM), which performs gradient descent on adversarially
  perturbed weights, can improve generalization by identifying flatter minima. However,
  recent studies have shown that SAM may suffer from convergence instability and oscillate
  around saddle points, resulting in slow convergence and inferior performance. To
  address this problem, we propose the use of a lookahead mechanism to gather more
  information about the landscape by looking further ahead, and thus find a better
  trajectory to converge. By examining the nature of SAM, we simplify the extrapolation
  procedure, resulting in a more efficient algorithm. Theoretical results show that
  the proposed method converges to a stationary point and is less prone to saddle
  points. Experiments on standard benchmark datasets also verify that the proposed
  method outperforms the SOTAs, and converge more effectively to flat minima.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: yu24q
month: 0
tex_title: Improving Sharpness-Aware Minimization by Lookahead
firstpage: 57776
lastpage: 57802
page: 57776-57802
order: 57776
cycles: false
bibtex_author: Yu, Runsheng and Zhang, Youzhi and Kwok, James
author:
- given: Runsheng
  family: Yu
- given: Youzhi
  family: Zhang
- given: James
  family: Kwok
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/yu24q/yu24q.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
