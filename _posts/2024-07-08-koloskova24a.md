---
title: On Convergence of Incremental Gradient for Non-convex Smooth Functions
openreview: ZRMQX6aTUS
abstract: In machine learning and neural network optimization, algorithms like incremental
  gradient, single shuffle SGD, and random reshuffle SGD are popular due to their
  cache-mismatch efficiency and good practical convergence behavior. However, their
  optimization properties in theory, especially for non-convex smooth functions, remain
  incompletely explored. This paper delves into the convergence properties of SGD
  algorithms with arbitrary data ordering, within a broad framework for non-convex
  smooth functions. Our findings show enhanced convergence guarantees for incremental
  gradient and single shuffle SGD. Particularly if $n$ is the training set size, we
  improve $n$ times the optimization term of convergence guarantee to reach accuracy
  $\epsilon$ from $O \left( \frac{n}{\epsilon} \right)$ to $O \left( \frac{1}{\epsilon}\right)$.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: koloskova24a
month: 0
tex_title: On Convergence of Incremental Gradient for Non-convex Smooth Functions
firstpage: 25058
lastpage: 25086
page: 25058-25086
order: 25058
cycles: false
bibtex_author: Koloskova, Anastasia and Doikov, Nikita and Stich, Sebastian U and
  Jaggi, Martin
author:
- given: Anastasia
  family: Koloskova
- given: Nikita
  family: Doikov
- given: Sebastian U
  family: Stich
- given: Martin
  family: Jaggi
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://raw.githubusercontent.com/mlresearch/v235/main/assets/koloskova24a/koloskova24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
