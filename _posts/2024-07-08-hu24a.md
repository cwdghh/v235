---
title: Outlier-Efficient Hopfield Layers for Large Transformer-Based Models
openreview: kLiDMGJKx1
abstract: 'We introduce an Outlier-Efficient Modern Hopfield Model (termed OutEffHop)
  and use it to address the outlier inefficiency problem of training gigantic transformer-based
  models. Our main contribution is a novel associative memory model facilitating <em>outlier-efficient</em>
  associative memory retrievals. Interestingly, this memory model manifests a model-based
  interpretation of an outlier-efficient attention mechanism (Softmax_1): it is an
  approximation of the memory retrieval process of OutEffHop. Methodologically, this
  allows us to introduce novel outlier-efficient Hopfield layers as powerful alternatives
  to traditional attention mechanisms, with superior post-quantization performance.
  Theoretically, the Outlier-Efficient Modern Hopfield Model retains and improves
  the desirable properties of standard modern Hopfield models, including fixed point
  convergence and exponential storage capacity. Empirically, we demonstrate the efficacy
  of the proposed model across large-scale transformer-based and Hopfield-based models
  (including BERT, OPT, ViT, and STanHop-Net), benchmarking against state-of-the-art
  methods like Clipped_Softmax and Gated_Attention. Notably, OutEffHop achieves an
  average reduction of 22+% in average kurtosis and 26+% in the maximum infinity norm
  of model outputs across four models. Code is available at GitHub; future updates
  are on arXiv.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: hu24a
month: 0
tex_title: Outlier-Efficient Hopfield Layers for Large Transformer-Based Models
firstpage: 19123
lastpage: 19152
page: 19123-19152
order: 19123
cycles: false
bibtex_author: Hu, Jerry Yao-Chieh and Chang, Pei-Hsuan and Luo, Haozheng and Chen,
  Hong-Yu and Li, Weijian and Wang, Wei-Po and Liu, Han
author:
- given: Jerry Yao-Chieh
  family: Hu
- given: Pei-Hsuan
  family: Chang
- given: Haozheng
  family: Luo
- given: Hong-Yu
  family: Chen
- given: Weijian
  family: Li
- given: Wei-Po
  family: Wang
- given: Han
  family: Liu
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://raw.githubusercontent.com/mlresearch/v235/main/assets/hu24a/hu24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
