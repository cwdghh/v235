---
title: The Linear Representation Hypothesis and the Geometry of Large Language Models
openreview: UGpGkLzwpP
abstract: 'Informally, the "linear representation hypothesis" is the idea that high-level
  concepts are represented linearly as directions in some representation space. In
  this paper, we address two closely related questions: What does "linear representation"
  actually mean? And, how do we make sense of geometric notions (e.g., cosine similarity
  and projection) in the representation space? To answer these, we use the language
  of counterfactuals to give two formalizations of linear representation, one in the
  output (word) representation space, and one in the input (context) space. We then
  prove that these connect to linear probing and model steering, respectively. To
  make sense of geometric notions, we use the formalization to identify a particular
  (non-Euclidean) inner product that respects language structure in a sense we make
  precise. Using this <em>causal inner product</em>, we show how to unify all notions
  of linear representation. In particular, this allows the construction of probes
  and steering vectors using counterfactual pairs. Experiments with LLaMA-2 demonstrate
  the existence of linear representations of concepts, the connection to interpretation
  and control, and the fundamental role of the choice of inner product.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: park24c
month: 0
tex_title: The Linear Representation Hypothesis and the Geometry of Large Language
  Models
firstpage: 39643
lastpage: 39666
page: 39643-39666
order: 39643
cycles: false
bibtex_author: Park, Kiho and Choe, Yo Joong and Veitch, Victor
author:
- given: Kiho
  family: Park
- given: Yo Joong
  family: Choe
- given: Victor
  family: Veitch
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/park24c/park24c.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
