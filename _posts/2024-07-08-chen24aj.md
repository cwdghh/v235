---
title: 'GRATH: Gradual Self-Truthifying for Large Language Models'
openreview: d2f2sCXQuI
abstract: Truthfulness is paramount for large language models (LLMs) as they are increasingly
  deployed in real-world applications. However, existing LLMs still struggle with
  generating truthful content, as evidenced by their modest performance on benchmarks
  like TruthfulQA. To address this issue, we propose GRAdual self-truTHifying (GRATH),
  a novel post-processing method to enhance truthfulness of LLMs. GRATH utilizes out-of-domain
  question prompts to generate pairwise truthfulness training data with each pair
  containing a question and its correct and incorrect answers, and then optimizes
  the model via direct preference optimization (DPO) to learn from the truthfulness
  difference between answer pairs. GRATH iteratively refines truthfulness data and
  updates the model, leading to a gradual improvement in model truthfulness in a self-supervised
  manner. Empirically, we evaluate GRATH using different 7B-LLMs and compare with
  LLMs with similar or even larger sizes on benchmark datasets. Our results show that
  GRATH effectively improves LLMsâ€™ truthfulness without compromising other core capabilities.
  Notably, GRATH achieves state-of-the-art performance on TruthfulQA, with MC1 accuracy
  of 54.71% and MC2 accuracy of 69.10%, which even surpass those on 70B-LLMs. The
  code is available at https://github.com/chenweixin107/GRATH.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: chen24aj
month: 0
tex_title: "{GRATH}: Gradual Self-Truthifying for Large Language Models"
firstpage: 7260
lastpage: 7279
page: 7260-7279
order: 7260
cycles: false
bibtex_author: Chen, Weixin and Song, Dawn and Li, Bo
author:
- given: Weixin
  family: Chen
- given: Dawn
  family: Song
- given: Bo
  family: Li
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://raw.githubusercontent.com/mlresearch/v235/main/assets/chen24aj/chen24aj.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
