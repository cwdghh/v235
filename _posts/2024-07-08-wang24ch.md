---
title: Can Gaussian Sketching Converge Faster on a Preconditioned Landscape?
openreview: gB3E8IwQZy
abstract: This paper focuses on the large-scale optimization which is very popular
  in the big data era. The gradient sketching is an important technique in the large-scale
  optimization. Specifically, the random coordinate descent algorithm is a kind of
  gradient sketching method with the random sampling matrix as the sketching matrix.
  In this paper, we propose a novel gradient sketching called GSGD (Gaussian Sketched
  Gradient Descent). Compared with the classical gradient sketching methods such as
  the random coordinate descent and SEGA (Hanzely et al., 2018), our GSGD does not
  require the importance sampling but can achieve a fast convergence rate matching
  the ones of these methods with importance sampling. Furthermore, if the objective
  function has a non-smooth regularization term, our GSGD can also exploit the implicit
  structure information of the regularization term to achieve a fast convergence rate.
  Finally, our experimental results substantiate the effectiveness and efficiency
  of our algorithm.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: wang24ch
month: 0
tex_title: Can {G}aussian Sketching Converge Faster on a Preconditioned Landscape?
firstpage: 52064
lastpage: 52082
page: 52064-52082
order: 52064
cycles: false
bibtex_author: Wang, Yilong and Ye, Haishan and Dai, Guang and Tsang, Ivor
author:
- given: Yilong
  family: Wang
- given: Haishan
  family: Ye
- given: Guang
  family: Dai
- given: Ivor
  family: Tsang
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/wang24ch/wang24ch.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
