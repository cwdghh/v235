---
title: 'Position: Considerations for Differentially Private Learning with Large-Scale
  Public Pretraining'
openreview: ncjhi4qAPV
abstract: The performance of differentially private machine learning can be boosted
  significantly by leveraging the transfer learning capabilities of non-private models
  pretrained on large <em>public</em> datasets. We critically review this approach.
  We primarily question whether the use of large Web-scraped datasets <em>should</em>
  be viewed as differential-privacy-preserving. We further scrutinize whether existing
  machine learning benchmarks are appropriate for measuring the ability of pretrained
  models to generalize to sensitive domains. Finally, we observe that reliance on
  large pretrained models may lose <em>other</em> forms of privacy, requiring data
  to be outsourced to a more compute-powerful third party.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: tramer24a
month: 0
tex_title: 'Position: Considerations for Differentially Private Learning with Large-Scale
  Public Pretraining'
firstpage: 48453
lastpage: 48467
page: 48453-48467
order: 48453
cycles: false
bibtex_author: Tram\`{e}r, Florian and Kamath, Gautam and Carlini, Nicholas
author:
- given: Florian
  family: Tram√®r
- given: Gautam
  family: Kamath
- given: Nicholas
  family: Carlini
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/assets/tramer24a/tramer24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
