---
title: Controllable Prompt Tuning For Balancing Group Distributional Robustness
openreview: OYL91MHfuU
abstract: Models trained on data composed of different groups or domains can suffer
  from severe performance degradation under distribution shifts. While recent methods
  have largely focused on optimizing the worst-group objective, this often comes at
  the expense of good performance on other groups. To address this problem, we introduce
  an optimization scheme to achieve good performance across groups and find a good
  solution for all without severely sacrificing performance on any of them. However,
  directly applying such optimization involves updating the parameters of the entire
  network, making it both computationally expensive and challenging. Thus, we introduce
  Controllable Prompt Tuning (CPT), which couples our approach with prompt-tuning
  techniques. On spurious correlation benchmarks, our procedures achieve state-of-the-art
  results across both transformer and non-transformer architectures, as well as unimodal
  and multimodal data, while requiring only $0.4%$ tunable parameters.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: phan24b
month: 0
tex_title: Controllable Prompt Tuning For Balancing Group Distributional Robustness
firstpage: 40667
lastpage: 40687
page: 40667-40687
order: 40667
cycles: false
bibtex_author: Phan, Hoang and Wilson, Andrew Gordon and Lei, Qi
author:
- given: Hoang
  family: Phan
- given: Andrew Gordon
  family: Wilson
- given: Qi
  family: Lei
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/phan24b/phan24b.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
