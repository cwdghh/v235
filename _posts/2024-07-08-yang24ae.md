---
title: 'How Graph Neural Networks Learn: Lessons from Training Dynamics'
openreview: Dn4B53IcCW
abstract: A long-standing goal in deep learning has been to characterize the learning
  behavior of black-box models in a more interpretable manner. For graph neural networks
  (GNNs), considerable advances have been made in formalizing what functions they
  can represent, but whether GNNs will learn desired functions during the optimization
  process remains less clear. To fill this gap, we study their training dynamics in
  function space. In particular, we find that the optimization of GNNs through gradient
  descent implicitly leverages the graph structure to update the learned function.
  This phenomenon is dubbed as kernel-graph alignment, which has been empirically
  and theoretically corroborated. This new analytical framework from the optimization
  perspective enables interpretable explanations of when and why the learned GNN functions
  generalize, which are relevant to their limitations on heterophilic graphs. From
  a practical standpoint, it also provides high-level principles for designing new
  algorithms. We exemplify this by showing that a simple and efficient non-parametric
  algorithm, obtained by explicitly using graph structure to update the learned function,
  can consistently compete with nonlinear GNNs.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: yang24ae
month: 0
tex_title: 'How Graph Neural Networks Learn: Lessons from Training Dynamics'
firstpage: 56594
lastpage: 56623
page: 56594-56623
order: 56594
cycles: false
bibtex_author: Yang, Chenxiao and Wu, Qitian and Wipf, David and Sun, Ruoyu and Yan,
  Junchi
author:
- given: Chenxiao
  family: Yang
- given: Qitian
  family: Wu
- given: David
  family: Wipf
- given: Ruoyu
  family: Sun
- given: Junchi
  family: Yan
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/assets/yang24ae/yang24ae.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
