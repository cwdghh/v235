---
title: Information Flow in Self-Supervised Learning
openreview: vxDjeeBnTu
abstract: In this paper, we conduct a comprehensive analysis of two dual-branch (Siamese
  architecture) self-supervised learning approaches, namely Barlow Twins and spectral
  contrastive learning, through the lens of matrix mutual information. We prove that
  the loss functions of these methods implicitly optimize both matrix mutual information
  and matrix joint entropy. This insight prompts us to further explore the category
  of single-branch algorithms, specifically MAE and U-MAE, for which mutual information
  and joint entropy become the entropy. Building on this intuition, we introduce the
  Matrix Variational Masked Auto-Encoder (M-MAE), a novel method that leverages the
  matrix-based estimation of entropy as a regularizer and subsumes U-MAE as a special
  case. The empirical evaluations underscore the effectiveness of M-MAE compared with
  the state-of-the-art methods, including a 3.9% improvement in linear probing ViT-Base,
  and a 1% improvement in fine-tuning ViT-Large, both on ImageNet.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: tan24e
month: 0
tex_title: Information Flow in Self-Supervised Learning
firstpage: 47649
lastpage: 47666
page: 47649-47666
order: 47649
cycles: false
bibtex_author: Tan, Zhiquan and Yang, Jingqin and Huang, Weiran and Yuan, Yang and
  Zhang, Yifan
author:
- given: Zhiquan
  family: Tan
- given: Jingqin
  family: Yang
- given: Weiran
  family: Huang
- given: Yang
  family: Yuan
- given: Yifan
  family: Zhang
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://raw.githubusercontent.com/mlresearch/v235/main/assets/tan24e/tan24e.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
