---
title: Is DPO Superior to PPO for LLM Alignment? A Comprehensive Study
openreview: 6XH8R7YrSk
abstract: Reinforcement Learning from Human Feedback (RLHF) is currently the most
  widely used method to align large language models (LLMs) with human preferences.
  Existing RLHF methods can be roughly categorized as either reward-based or reward-free.
  Novel applications such as ChatGPT and Claude leverage reward-based methods that
  first learn a reward model and apply actor-critic algorithms, such as Proximal Policy
  Optimization (PPO). However, in academic benchmarks, state-of-the-art results are
  often achieved via reward-free methods, such as Direct Preference Optimization (DPO).
  Is DPO truly superior to PPO? Why does PPO perform poorly on these benchmarks? In
  this paper, we first conduct both theoretical and empirical studies on the algorithmic
  properties of DPO and show that DPO may have fundamental limitations. Moreover,
  we also comprehensively examine PPO and reveal the key factors for the best performances
  of PPO in fine-tuning LLMs. Finally, we benchmark DPO and PPO across a collection
  of RLHF testbeds, ranging from dialogue to code generation. Experiment results demonstrate
  that PPO is able to surpass other alignment methods in all cases and achieve state-of-the-art
  results in challenging code competitions.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: xu24h
month: 0
tex_title: Is {DPO} Superior to {PPO} for {LLM} Alignment? {A} Comprehensive Study
firstpage: 54983
lastpage: 54998
page: 54983-54998
order: 54983
cycles: false
bibtex_author: Xu, Shusheng and Fu, Wei and Gao, Jiaxuan and Ye, Wenjie and Liu, Weilin
  and Mei, Zhiyu and Wang, Guangju and Yu, Chao and Wu, Yi
author:
- given: Shusheng
  family: Xu
- given: Wei
  family: Fu
- given: Jiaxuan
  family: Gao
- given: Wenjie
  family: Ye
- given: Weilin
  family: Liu
- given: Zhiyu
  family: Mei
- given: Guangju
  family: Wang
- given: Chao
  family: Yu
- given: Yi
  family: Wu
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/xu24h/xu24h.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
