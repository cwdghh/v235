---
title: Which Frequencies do CNNs Need? Emergent Bottleneck Structure in Feature Learning
openreview: lGvIV4Bgsz
abstract: We describe the emergence of a Convolution Bottleneck (CBN) structure in
  CNNs, where the network uses its first few layers to transform the input representation
  into a representation that is supported only along a few frequencies and channels,
  before using the last few layers to map back to the outputs. We define the CBN rank,
  which describes the number and type of frequencies that are kept inside the bottleneck,
  and partially prove that the parameter norm required to represent a function $f$
  scales as depth times the CBN rank $f$. We also show that the parameter norm depends
  at next order on the regularity of $f$. We show that any network with almost optimal
  parameter norm will exhibit a CBN structure in both the weights and - under the
  assumption that the network is stable under large learning rate - the activations,
  which motivates the common practice of down-sampling; and we verify that the CBN
  results still hold with down-sampling. Finally we use the CBN structure to interpret
  the functions learned by CNNs on a number of tasks.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: wen24d
month: 0
tex_title: Which Frequencies do {CNN}s Need? {E}mergent Bottleneck Structure in Feature
  Learning
firstpage: 52779
lastpage: 52800
page: 52779-52800
order: 52779
cycles: false
bibtex_author: Wen, Yuxiao and Jacot, Arthur
author:
- given: Yuxiao
  family: Wen
- given: Arthur
  family: Jacot
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://raw.githubusercontent.com/mlresearch/v235/main/assets/wen24d/wen24d.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
