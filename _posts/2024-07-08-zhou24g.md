---
title: 'MultiMax: Sparse and Multi-Modal Attention Learning'
openreview: IC9UZ8lm25
abstract: SoftMax is a ubiquitous ingredient of modern machine learning algorithms.
  It maps an input vector onto a probability simplex and reweights the input by concentrating
  the probability mass at large entries. Yet, as a smooth approximation to the Argmax
  function, a significant amount of probability mass is distributed to other, residual
  entries, leading to poor interpretability and noise. Although sparsity can be achieved
  by a family of SoftMax variants, they often require an alternative loss function
  and do not preserve multimodality. We show that this trade-off between multi-modality
  and sparsity limits the expressivity of SoftMax as well as its variants. We provide
  a solution to this tension between objectives by proposing a piece-wise differentiable
  function, termed MultiMax, which adaptively modulates the output distribution according
  to input entry range. Through comprehensive analysis and evaluation, we show that
  MultiMax successfully produces a distribution that supresses irrelevant entries
  while preserving multi-modality, with benefits in image classification, language
  modeling and machine translation.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: zhou24g
month: 0
tex_title: "{M}ulti{M}ax: Sparse and Multi-Modal Attention Learning"
firstpage: 61897
lastpage: 61912
page: 61897-61912
order: 61897
cycles: false
bibtex_author: Zhou, Yuxuan and Fritz, Mario and Keuper, Margret
author:
- given: Yuxuan
  family: Zhou
- given: Mario
  family: Fritz
- given: Margret
  family: Keuper
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/assets/zhou24g/zhou24g.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
