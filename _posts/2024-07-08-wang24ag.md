---
title: 'StableSSM: Alleviating the Curse of Memory in State-space Models through Stable
  Reparameterization'
openreview: nMN5hNZMQK
abstract: 'In this paper, we investigate the long-term memory learning capabilities
  of state-space models (SSMs) from the perspective of parameterization. We prove
  that state-space models without any reparameterization exhibit a memory limitation
  similar to that of traditional RNNs: the target relationships that can be stably
  approximated by state-space models must have an exponential decaying memory. Our
  analysis identifies this “curse of memory” as a result of the recurrent weights
  converging to a stability boundary, suggesting that a reparameterization technique
  can be effective. To this end, we introduce a class of reparameterization techniques
  for SSMs that effectively lift its memory limitations. Besides improving approximation
  capabilities, we further illustrate that a principled choice of reparameterization
  scheme can also enhance optimization stability. We validate our findings using synthetic
  datasets, language models and image classifications.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: wang24ag
month: 0
tex_title: "{S}table{SSM}: Alleviating the Curse of Memory in State-space Models through
  Stable Reparameterization"
firstpage: 50766
lastpage: 50793
page: 50766-50793
order: 50766
cycles: false
bibtex_author: Wang, Shida and Li, Qianxiao
author:
- given: Shida
  family: Wang
- given: Qianxiao
  family: Li
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/assets/wang24ag/wang24ag.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
