---
title: 'Flextron: Many-in-One Flexible Large Language Model'
openreview: 9vKRhnflAs
abstract: Training modern LLMs is extremely resource intensive, and customizing them
  for various deployment scenarios characterized by limited compute and memory resources
  through repeated training is impractical. In this paper, we introduce Flextron,
  a network architecture and post-training model optimization framework supporting
  flexible model deployment. The Flextron architecture utilizes a nested elastic structure
  to rapidly adapt to specific user-defined latency and accuracy targets during inference
  with no additional fine-tuning required. It is also input-adaptive, and can automatically
  route tokens through its sub-networks for improved performance and efficiency. We
  present a sample-efficient training method and associated routing algorithms for
  systematically transforming an existing trained LLM into a Flextron model. We evaluate
  Flextron on the GPT-3 and LLama-2 family of LLMs, and demonstrate superior performance
  over multiple end-to-end trained variants and other state-of-the-art elastic networks,
  all with a single pretraining run that consumes a mere 7.63% tokens compared to
  original pretraining.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: cai24e
month: 0
tex_title: 'Flextron: Many-in-One Flexible Large Language Model'
firstpage: 5298
lastpage: 5311
page: 5298-5311
order: 5298
cycles: false
bibtex_author: Cai, Ruisi and Muralidharan, Saurav and Heinrich, Greg and Yin, Hongxu
  and Wang, Zhangyang and Kautz, Jan and Molchanov, Pavlo
author:
- given: Ruisi
  family: Cai
- given: Saurav
  family: Muralidharan
- given: Greg
  family: Heinrich
- given: Hongxu
  family: Yin
- given: Zhangyang
  family: Wang
- given: Jan
  family: Kautz
- given: Pavlo
  family: Molchanov
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://raw.githubusercontent.com/mlresearch/v235/main/assets/cai24e/cai24e.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
